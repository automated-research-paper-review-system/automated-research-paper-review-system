,Abstract,Title,Name,comments,is_meta_review,RECOMMENDATION,REPLICABILITY,PRESENTATION_FORMAT,CLARITY,MEANINGFUL_COMPARISON,SUBSTANCE,REVIEWER_CONFIDENCE,SOUNDNESS_CORRECTNESS,APPROPRIATENESS,IMPACT,ORIGINALITY,Year,Content,Result,Introduction,Related_work,Discussion,Conclusion
0,"The ability to capture time information is essential to many natural language processing and information retrieval applications. Therefore, a lexical resource associating word senses to their temporal orientation might be crucial for the computational tasks aiming at the interpretation of language of time in texts. In this paper, we propose a semi-supervised minimum cuts strategy that makes use of WordNet glosses and semantic relations to supplement WordNet entries with temporal information. Intrinsic and extrinsic evaluations show that our approach outperforms prior semi-supervised non-graph classifiers.",Identifying Temporal Orientation of Word Senses,25,"This paper presents an approach to tag word senses with temporal information
(past, present, future or atemporal). They model the problem using a
graph-based semi-supervised classification algorithm that allows to combine
item specific information - such as the presence of some temporal indicators in
the glosses - and the structure of Wordnet - that is semantic relations between
synsets â, and to take into account unlabeled data. They perform a full
annotation of Wordnet, based on a set of training data labeled in a previous
work and using the rest of Wordnet as unlabeled data. Specifically, they take
advantage of the structure of the label set by breaking the task into a binary
formulation (temporal vs atemporal), then using the data labeled as temporal to
perform a finer grained tagging (past, present or future). In order to
intrinsically evaluate their approach, they annotate a subset of synsets in
Wordnet using crowd-sourcing. They compare their system to the results obtained
by a state-of-the-art time tagger (Stanford's SUTime) using an heuristic as a
backup strategy, and to previous works. They obtain improvements around 11% in
accuracy, and show that their approach allows performance higher than previous
systems using only 400 labeled data. Finally, they perform an evaluation of
their resource on an existing task (TempEval-3) and show improvements of about
10% in F1 on 4 labels.

This paper is well-constructed and generally clear, the approach seems sound
and well justified. This work led to the development of a resource with fine
grained temporal information at the word sense level that would be made
available and could be used to improve various NLP tasks. I have a few remarks,
especially concerning the settings of the experiments.

I think that more information should be given on the task performed in the
extrinsic evaluation section. An example could be useful to understand what the
system is trying to predict (the features describe âentity pairsâ but it
has not been made clear before what are these pairs) and what are the features
(especially, what are the entity attributes? What is the POS for a pair, is it
one dimension or two? Are the lemmas obtained automatically?). The sentence
describing the labels used is confusing, I'm not sure to understand what
âevent to document creation timeâ and âevent to same sentence eventâ
means, are they the kind of pairs considered? Are they relations (as they are
described as relation at the beginning of p.8)? I find unclear the footnote
about the 14 relations: why the other relations have to be ignored, what makes
a mapping too âcomplexâ? Also, are the scores macro or micro averaged?
Finally, the ablation study seems to indicate a possible redundancy between
Lexica and Entity with quite close scores, any clue about this behavior?

I have also some questions about the use of the SVM.  For the extrinsic
evaluation, the authors say that they optimized the parameters of the
algorithm: what are these parameters?  And since a SVM is also used within the
MinCut framework, is it optimized and how? Finally, if it's the LibSVM library
that is used (Weka wrapper), I think a reference to LibSVM should be included. 

Other remarks:
- It would be interesting to have the number of examples per label in the gold
data, the figures are given for coarse grained labels (127 temporal vs 271
atemporal), but not for the finer grained.
- It would also be nice to have an idea of the number of words that are
ambiguous at the temporal level, words like âpresentâ.
- It is said in the caption of the table 3 that the results presented are
âsignificantly betterâ but no significancy test is indicated, neither any
p-value.

Minor remarks:
- Related work: what kind of task was performed in (Filannino and Nenadic,
2014)?
- Related work: ârequires a post-calibration procedureâ, needs a reference
(and p.4 in 3.3 footnote it would be clearer to explain calibration)
- Related work: âtheir model differ from oursâ, in what?
- Table 3 is really too small: maybe, remove the parenthesis, put the
â(p,r,f1)â in the caption and give only two scores, e.g. prec and f1. The
caption should also be reduced.
- Information in table 4 would be better represented using a graph.
- Beginning of p.7: 1064 â 1264
- TempEval-3: reference ?
- table 6: would be made clearer by ordering the scores for one column
- p.5, paragraph 3: atemporal) â atemporal",,4,4,Oral Presentation,4,5,4,2,4,5,4,3,2016,"Underlying idea behind classification with minimum cuts (Mincuts) in graph is that similar items should be grouped together in the same cut. Suppose we have n items x1, ......xn to divide into two classes C1 and C2 based on the two types of in-
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
formation at hand. The first one, individual score ind j(xi) is the non-negative estimate of each xi’s preference for being in class C j based on the features of xi alone. While the later one, association scores assoc(xi,xk) represent a non-negative estimates of how important it is that xi and xk be in the same class. Overall idea is to maximize each item’s net score i.e. individual score for the class it is assigned to minus its individual score for the other class and penalize putting tightly associated items into different classes. It can be seen as the following optimization problem: assign the xis to C1 and C2 so as to minimize the partition cost.
∑ x∈C1 ind2(x)+ ∑ x∈C2 ind1(x)+ ∑ xi∈C1,xk∈C2 assoc(xi,xk) (1)
We could represent the situation by building an undirected graph G with vertices {v1, .....,vn,s, t}; s and t are source and sink respectively. Add n edges (s,vi), each with weight ind1(xi), n edges (vi, t), each with weight ind2(xi). Finally, add (n 2
) edges (vi,vk), each with weight assoc(xi,xk). Finally, cuts in G are defined as follows:
Definition 1. A cut (S,T ) of G is a partition of its nodes into sets S= {s}∪ S′ and T = {t}∪ T ′ where s /∈ S′, t /∈ T ′. Its cost cost(S,T ) is the sum of the weights of all edges crossing from S to T . A minimum cut of G is one of minimum cost.
Figure 1 illustrates an example of the concepts for classifying three items. Brackets enclose example values; here, the individual scores happen to be probabilities. Based on individual scores alone, we would put Y (“Promise."") in s (Temporal class), N (“Chair"") in t (Atemporal class), and be undecided about M (""Oath""). But the association scores favour cuts that put Y and M in the same class, as shown in the table. Thus, the minimum cut, indicated by the dashed red line, places M together Y in s.Formulating the task of temporality detection problem on word senses in terms of graphs allows us to model item-specific and pair-wise information independently. Therefore, it is a very flexible paradigm where we have two different views on the data. For example, rule based approaches or machine learning algorithms employing linguistic and other features representing temporal indicators can be used to derive individual scores for a particular item in isolation. The
edges weighted by the individual scores of a vertex (=word sense) to the source/sink can be interpretative as the probability of a word sense being temporal or atemporal without taking similarity to other senses into account. And we could also simultaneously use conceptual-semantic and lexical relations from WordNet to derive the association scores. The edges between two items weighted by the association scores can indicate how similar/different two senses are. If two senses are connected via a temporality-preserving relation they are likely to be both temporal or in opposite atemporal. An example here is the hyponymy relation, a temporality-preserving relation (Dias et al., 2014), where two hyponymy such as present, nowadays—the period of time that is happening now and now—the momentary present are both temporal. To detect the temporal orientation of word senses, authors in (Dias et al., 2014) adopted a single view instead of two on the data. The ability to combine two views on the data is precisely one of the strengths of our approach.
Second, Mincuts can be easily expanded into a semi-supervised framework. This is essential as the existing labeled datasets for our problem are small. In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations. Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework.
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components. More importantly, the unlabeled data can be related to the labeled data (by some WordNet relation), it might help pull unlabeled data to the right cuts (categories).Formulation of our semi-supervised Mincut for synset temporality classification involves the following steps.
I We depute two vertices s (source) and t (sink) which corresponds to the “temporal” and “atemporal” category respectively. We call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices. Each example vertex corresponds to one WordNet synset and is connected to both s and t through an undirected weighted edge. This confirms that the graph is connected.
II The labeled training examples are connected to classification vertices they belong to via edges with high constant non-negative weight. Unlabeled examples are connected to the classification vertices via edges weighted with non-negative scores that indicate the degree of association with temporal/atemporal category. We use a classifier to assign these edges weights.
III Conceptual-semantic and lexical relations available in the WordNet are used to construct edges between two example vertices. Such edges can exist between any pair of example vertices, for instance between two unlabeled vertices.
IV After building graph, we then employ a maximum-flow algorithm to find the minimum s− t cuts of the graph. The cut in which the source vertex s lies is classified as “temporal” and the cut in which sink vertex t lies is labeled as “atemporal”.
V In order to fine tune the temporal part, we follow hierarchical strategy by organizing the classes (past, present, future) according to a hierarchy. The hierarchy of classes is decided based on the classes that are easier to discriminate to improve the overall classification accuracy. First, we define two vertices s (source) and t (sink) which correspond to the
“past” and “Not_Past” temporal categories respectively. Then we follow the above steps I through IV . This divides the temporal part into two disjoint subsets: past synsets and synsets belong to present and future temporal category. Finally, we repeat steps I through IV where vertices s (source) and t (sink) corresponds to “future” and “present” category respectively.
Labeled and unlabeled data selection: We use the same temporal (past, present, future) and atemporal sets of synsets considered as training data at the time of building TempoWordNet (TWnL) (Dias et al., 2014) as training /labeled data for our experiments. For test set, sample of synsets outside the labeled data is selected and annotated using crowdsourcing service. All other synsets outside labeled and test set are considered as unlabeled data. Weighting of edges to the classification vertices: The edge weight (non-negative) to the source s and the sink t denotes how likely it is that an example vertex is put in the cut in which s (temporal) or t (atemporal) lies. For the unlabeled and test examples, a supervised learning strategy (over the labeled data as training set) is used to assign the edge weights. Each synset is represented by its gloss encoded as a vector of word unigrams weighted by their frequency in the gloss. As for classifier, we used SVM from the Weka platform2. In order to ensure that Mincut does not reverse the labels of the labeled training data, we assign a high3 constant non-negative weight of 3 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.001 to the edge to the other classification vertex. Deriving weights for WordNet relations: While formulating the graph, we connect two vertices by an edge if they are linked by one of the ten (10) WordNet relations in Table 1. Main motivation towards using other relations in addition to the most frequently encoded relations (hypernym, hyponym) among synsets in WordNet is to achieve high graph connectivity. Moreover, we can assign different weights to different relations to reflect the degree to which they are temporality preserving. Therefore, we adopt two strategies to assign
2http://www.cs.waikato.ac.nz/ml/weka/ [Last access: 12/04/2015].
3w.r.t. the probability estimates (after calibration) of the classes from SVM.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
weights to different WordNet relations. The first method (ScWt), assigns the same constant weight of 1.0 to all WordNet relations.
The second method (DiffWt), considers several degrees of preserving temporality. In order to do this, we adopt a simple strategy to produce large noisy set of temporal 4 and atemporal synsets from WordNet. This method uses a list of 30 handcrafted temporal seeds (equally distributed over past, present, and future temporal categories) proposed in (Dias et al., 2014) along with their direct hyponym5 to classify each WordNet synset with at least one temporal word6 in its gloss as temporal and all other synsets as atemporal. We then simply count how often two synsets connected by a given relation (edge) have the same or different temporal dimension. Finally, weight is calculated by #same/(#same+#different). Results are reported in Table 1.
Wordnet Relation #same #different Weight Direct-Hypernym 61914 9600 0.76 Direct-Hyponym 73268 7246 0.91
Antonym 1905 3614 0.35 Similar-to 6587 1914 0.77 Derived-from 3630 1947 0.65 Also-see 1037 337 0.75 Attribute 350 109 0.76
Troponym 6917 2651 0.72 Domain 2380 2895 0.45 Domain-member 2380 2895 0.45
Table 1: WordNet relation weights (DiffWt Method)Labeled Data: We used a list that consists of 632 temporal synsets in WordNet and equal number of atemporal synsets provided by (Dias et al., 2014) as labeled data for our experiments. Temporal synsets are distributed as follows: 210 synsets marked as past, 291 as present, and 131 as future. Building of a Gold Standard: Since to our best knowledge, there is no gold standard resource with temporal association of words (except 30 handcrafted temporal seeds proposed by Dias et al., 2014), we designed our own annotation task using the crowdsourcing service of CrowdFlower platform7. For the annotation task, three hundred
4To fine tune the temporal part we used the same strategy for tagging past, present, and future synsets following hierarchical strategy
5Relation which preserves temporality according to (Dias et al., 2014)
6Most frequent sense of the temporal word from WordNet is selected
7http://www.crowdflower.com/
ninety eight (398) synsets equally distributed over nouns, verbs, adjectives and adverbs POS categories along with their lemmas and glosses are selected randomly from WordNet 8 as representative of the whole WordNet. Note that this number of synset is statistically significant representative sample of total WordNet synsets (117000 plus synsets) and derived using the formula described in (Israel, 1992). Afterwards, we designed two question that the annotators were expected to answer for a given synset (lemmas and gloss are also provided). While the first question is related to the decision which reflects a synset being temporal or atemporal, the motivation behind the second question is to collect a more fine-grained (past, present, future) gold-standard for synset-temporality association. Details of annotation guideline is out of scope of this paper.
The reliability of the annotators was evaluated on the basis of 60 control synsets 9 provided by (Dias et al., 2014) which were clearly associated either with a specific temporal or atemporal dimension and 10 temporally ambiguous synsets associated with more than one temporal dimensions. Similar to (Tekiroğlu et al., ), the raters who scored at least 70% accuracy on average on both sets of control synsets were considered to be reliable. Each unit was annotated by at least 10 reliable raters.
Table 2 demonstrates the observed agreement. Similar to (Mohammad, 2011; Özbal et al., 2011), annotations with a majority class greater than 5 is considered as reliable. Indeed for temporal vs atemporal) classification, 84.83 % of the synset annotations the absolute majority agreed on the same decision, while for past, present, and future, 72.36% of the annotations have majority class greater than 5. The high agreement confirms the quality of the resulting gold standard data.Temporal Vs Atemporal Classification: Using our formulation in Section 3.3, we construct a connected graph by importing 1264 training set (632 temporal and 632 atemporal synsets), 398 gold standard test set created using a crowdsourced service, and 115996 unlabeled synsets10. We con-
8WordNet version 3.0 used and selected outside from the labeled data set
930 temporal synsets (equally distributed over past, present, future) and 30 atemporal synsets
10All synset of WordNet−(training set+test set)
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Majority Class 3 4 5 6 7 8 9 10 Synset as temporal or atemporal 0 .20 1.21 4.32 10.69 14.56 29.34 19.23 11.01
Temporal synset into past, present, or future 1.23 3.01 10.45 20.22 16.56 12.34 14.23 9.01
Table 2: Percentage of synsets in each majority class.
struct edge weights to classification vertices, s (temporal) and t (atemporal) by using a SVM classifier discussed above. WordNet relations for links between example vertices are weighted either by non-negative constant value (ScWt) or by the method ‘DiffWt’ illustrated in Table 1. Temporally tagged synsets into past, present, and future: In order to fine tune the temporal part, we construct another connected graph by importing 632 labeled temporal synsets, temporal part of the gold standard test data (127 synsets out of 398 synsets), temporally tagged synsets as unlabeled data 11 and follow the same strategy presented in Section 3.3 where classification vertices, s and t correspond to ‘past’ and ‘Not_past’ temporal categories respectively. Finally, temporal synsets tagged as ‘Not_past’ classified either as present or future following the same analogy.","The underlying idea being that a reliable resource must evidence high quality time-tagging as well as improved performance for some external tasks.Baseline: In order to compare our semisupervised Mincut approach to a reasonable baseline, we use rule based approach to classify goldstandard data into past, present, future, or atemporal based on its lemmas and glosses. First, time expressions in the glosses of synsets are labeled and resolved via Standford’s SUTime tagger, which give accuracy in line with the stateof-the-art systems at identifying time expressions at TempEval. For each synset, Named-entity time tag provided by SUTime (e.g.“future_ref” or “present_ref” etc.) for the time expression present in its gloss is considered as the temporal class for that particular synset. In case of more than one temporal expression present12 in the gloss of a synset, majority class of the time tags is se-
11Total number of synsets classified as temporal − (Total number of temporal synsets in training data+Total number of gold standard temporal synsets tagged as temporal at the time of temporal vs atemporal classification process)
12Found very rarely < 1%
lected. Secondly, if no time expression is identified by the time tagger, a list composed of 30 handcrafted temporal seeds proposed in (Dias et al., 2014) along with their direct hyponym and standard temporal adverbials (since), prepositions (before/after), adjectives (former) etc. is used to classify synsets with at least one temporal word13 in its lemma(s) and gloss as temporal (past, present, future) and all other synsets as atemporal. Finally, performance of the rule based approach is measured for the gold standard data set and presented in Table 3. To figure out the contribution of word sense disambiguation, classic Lesk algorithm (Lesk, 1986) is used to choose right sense/synset for a word instead of most frequent sense. We found that contribution is negligible (< 0.4% improvement in overall accuracy).
To strengthen comparative evaluation of our semi-supervised Mincut approach, we propose to test our methodology with prior works (TempoWordNet: TWnL, TWnP, and TWnH). Comparative evaluation results are presented in Table 3. Results show the Mincut approach (CFG2) outperforms state-of-the-art approaches. It achieves highest accuracies for both temporal vs. atemporal and past, present, future classification with improvement of 11.3% and 10.3% respectively over the second best TempoWordNet versions (TWnH). Considering the above findings, we select our best Mincut configuration CFG2 for the remaining experiments. Distribution of time-tag synsets produced by this configuration: atemporal=110002, past=1733, present=4193, future=1730. Some examples are given below: • late–having died recently. (Past) • present, nowadays–the period of time that is
happening now. (Present) • promise–a verbal commitment by one person to
another. (Future) • field–a piece of land cleared of trees and usually
enclosed. (Atemporal) Performance with different size training data: We randomly generate subsets of labeled
13Most frequent sense of the temporal word from WordNet is selected
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Table 3: Accuracy (percentage classified correct) for temporal vs. atemporal and temporal into past, present, future classification using different methods namely TempoWordNet (TWnL, TWnP, TWnH) and Mincuts measured over created gold-standard data. CFG1 corresponds to the Mincut that uses SVM classifier to infer edges weights of unlabeled and test examples to the classification vertices s and t and predefined constant weights for WordNet relations (ScWt). CFG2 corresponds to the Mincut approach that uses the same SVM classifier to infer edges weights but uses a list of temporal synsets to infer weights of Wordnet relations (DiffWt). Both of our configurations perform significantly better than previous approaches. Results are also broken down by precision (p), recall (r), and f1 score for past, present, future, and atemporal categories.
Method Baseline TWnL TWnP TWnH CFG1 CFG2 Accuracy 48.8 65.6 62.0 68.4 74.4 79.7
temporal (p, r, f1) (52.0, 56.3, 54.0) (63.5, 82.1, 71.6) (55.8, 84.2, 67.1) (67.4, 81.9, 73.9) (84.5, 79.8, 82.0) (89.1, 79.3, 83.9) atemporal (p, r, f1) (58.2, 54.2, 56.1) (68.3, 79.2, 73.3) (58.9, 75.6, 66.2) (69.3, 82.6, 75.3) (81.3, 86.6, 83.8 ) (87.4, 90.8, 89.1)
Accuracy 45.6 62.0 59.6 65.7 72.7 76.0 past (p, r, f1) (49.3, 46.7, 47.9) (61.2, 73.0, 66.5) (59.3, 79.1, 67.7) (63.1, 75.0, 68.0) (71.1, 79.5, 75.0) (81.2, 78.5, 79.8) present (p, r, f1) (55.3, 48.2, 51.5) (63.0, 75.2, 68.5) (58.0, 78.2 66.0 ) (77.4, 69.2, 73.0) (73.0, 71.5, 72.2) (85.1, 74.7, 79.0) future (p, r, f1) (48.5, 49.0, 48.7) (62.1, 71.9, 66.6) (57.0, 83.1, 67.6) (60.0, 75.6, 66.8) (79.4, 69.5, 74.0) (86.1, 70.0, 77.2)
data/training data (1064 synsets: 632 temporal and 632 atemporal) L1,L2,L3......Ln, and ensures that L1 ⊂ L2 ⊂ L3...... ⊂ Ln. As proposed in (Dias et al., 2014), binary classification models based on the generated subsets of labeled data are learned. Using the same subsets of labeled data, we formulate our best performing minimum cut (CFG2). Accuracies of both approaches are presented in Table 4. As can be seen from the table, semi-supervised Mincut performs consistently better than the previous semi-supervised non-graph classification approach (SVM). Moreover, our proposed graph classification framework with only 400 labeled data/training data examples achieves even higher accuracies than SVM with 1264 training items (73.7% vs 68.4% ).
Number of labeled data SVM (TWnH) Minncut (CFG2) 100 59.8 64.3 200 62.6 67.5 400 65.5 73.7 600 67.4 77.6 800 67.9 79.2 1000 68.0 79.0
1264 (all) 68.4 79.7
Table 4: Accuracy with different sizes of label data for temporal vs. atemporal classification.For extrinsic evaluation, we focus on the problem of classifying temporal relations task of TempEval-3, assuming that identification of events, times are already performed. The underlying idea is that proposed method to yield a timeenhanced WordNet has a greater positive impact on more applied temporal information extraction tasks, whose output is useful for many information retrieval applications.
In order to produce comparative results with best performing system at TempEval-3 namely
UTTime (Laokulrat et al., 2013) for the above task, we follow the guidelines and use the same data sets provided by the evaluation campaign organizers. We restrict14 our experiment to a subset of relations namely BEFORE (CORR. Past), AFTER (CORR. Future), and INCLUDES (CORR. Present) with all other relation mapped to the ‘NA-RELATION’ for event to document creation time and event to same sentence event. For the task, we adopt a very simple strategy and implement the following features. • String features: The tokens and lemmas of each
entity pair. • Grammatical features: The part-of-speech
(PoS) tags of the entity pair (only for eventevent pairs). The grammatical information is obtained using the Standford CoreNLPtool15. • Entity attributes: Entity pair attributes as pro-
vided in the data set. • Dependency relation: We used the information
related to the dependency relation between two entities such as type of dependency, dependency order. • Textual context: The textual order of entity pair. • Lexica: The relative frequency of temporal cat-
egories, based on the resource developed in this research, in the text appearing between entity pair (event to same sentence event), text of all tokens in the time expression, 5 tokens following and preceding time expression/event. The features are encoded as the frequency with which a word from a temporal category (past, present, future) appeared in the text divided by the total number of tokens in the text.
14Because of the complexity to map 14 relations of TempEval-3 into three temporal classes (past, present, future) considered for this experiment
15http://stanfordnlp.github.io/CoreNLP/
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
We build our system namely T RelMincuts (composed of two classifiers) using Support Vector Machine (SVM) implementation of Weka over the features set and training data provided by the organizers. The best classifier for event to document creation time and event to same sentence event relation are selected via a grid search over parameter settings. The grid is evaluated with a 5- fold cross validation on the training data. We also measure the performance of UTTime for the above stated settings. Additionally, we build T RelTWnH by adopting same strategy and features set ‘Lexica’ is computed from prior time-tagging approach namely TWnH. Table 5 presents comparative evaluation results.
Approaches Precision(%) Recall(%) F1(%) UT Time 57.5 58.7 58.1 T RelMincuts 66.9 68.7 67.7 T RelTWnH 61.2 62.5 61.8
Table 5: Performance of different approaches on temporal relation classification based on TempEval-3 Evaluation strategy.
Results evidence that T RelMincuts significantly outperforms all other approaches and achieve highest performance in terms of precision (+5.7), recall (+6.2), and F1 score (+5.9). Results also demonstrate that our approach achieves 9.6% improvement in terms of F1 score over the best performing system in TempEval-3.
We perform feature ablation analyses as presented in Table 6 in order to measure features contribution. As can be seen, every features set produced improvement over baseline (mfc) and highest improvement is achieved with features set (Lexica) that comes from the proposed temporal lexical resource. The result also implies that while each feature type contains certain temporal information, there is also some redundancy across the feature types.
Features F1(%) Features F1(%) mfc baseline 33.55 all features 67.7 string alone 45.06 w/o string 65.70 grammatical alone 46.96 w/o grammatical 64.85 entity alone 52.23 w/o entity 62.08 dependency alone 48.65 w/o dependency 65.06 textual alone 46.82 w/o textual 64.96 lexica alone 51.62 w/o lexica 62.76
Table 6: Feature ablation analysis of F1 score. The most frequent class baseline (mfc) indicates accuracy if only predicting the present class.","There is considerable academic and commercial interest in processing time information in text, where that information is expressed either explicitly, or implicitly, or connotatively. Recognizing such information and exploiting it for Information Retrieval (IR) and Natural Language Processing (NLP) tasks are important features that can significantly improve the functionality of NLP/IR applications.
Automatic identification of temporal expressions in text is usually performed either via time taggers (Strötgen and Gertz, 2013), which contain pattern files, such as uni-grams and bi-grams used to express temporal expressions in a given language (e.g. names of months) or various grammatical rules. As a rule-based system, time taggers are limited by the coverage of the rules for the different types of temporal expressions that it recognizes. To exemplify, the word ‘present’ in the sentence “Apple’s iPhone is one of the most popular smartphones at present” when labeled by SUTime1 is tagged as:
<TIMEX3 tid=“t1” type=“DATE”
value=“PRESENT_REF”>present</TIMEX3> It rightly tags the word ‘present’ in the above example, and refers to it as the present time when reference date is considered as same as the tagging date. However, such word based indicators can be misleading. For example, below is the tag from SUTime for the word ‘present’ in the sentence “I was in Oxford Street getting the wife her birthday present”. The tag gives us a false impression by wrongly labeling the word as a temporal one.
<TIMEX3 tid=“t1” type=“DATE”
value=“PRESENT_REF”>present</TIMEX3> Reasons for this misleading information are i) time taggers usually do not use contextual indicators while deciding on temporality ii) different word senses of a single word can actually be either temporal or atemporal. A typical temporallyambiguous word, i.e. a word that has at least one temporal and at least one atemporal sense, is ‘present’, as shown by the two examples above.
Whereas most of the prior computational linguistics and text mining temporal studies have focused on temporal expressions and events, there has been a lack of work looking at the tempo-
1http://nlp.stanford.edu:8080/sutime/ process
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
ral orientation of word senses. Therefore, we focus our study on automatically time-tagging word senses into past, present, future, or atemporal using their WordNet (Miller, 1995) definition, instead of tagging temporal words.
In this paper, we put forward a semi-supervised graph-based classification paradigm build on an optimization theory namely the max-flow min-cut theorem (Papadimitriou and Steiglitz, 1998). In particular, we propose minimum cut in a connected graph to time-tag each synset of WordNet to one of the four dimensions: atemporal, past, present, and future. Our methodology was evaluated both intrinsically and extrinsically. It outperformed prior approaches to the temporality recognition of word senses/concepts. First, a gold standard is created using a crowdsourced annotation service to test our methodology. Second, temporal classification task is performed. Results show qualitative improvements when compared to previous state-of the-art approaches. Results also evidence that to achieve the performance of a standard supervised approach with our model, we need less than 10% of the training data.","Temporality in NLP and IR: Temporality has recently received increased attention in Natural Language Processing (NLP) and Information Retrieval (IR). Initial works proposed in NLP and IR are exhaustively summarized in (Mani et al., 2005). The introduction of the TempEval task (Verhagen et al., 2009) and subsequent challenges (TempEval-2 and -3) in the Semantic Evaluation workshop series have clearly established the importance of time to deal with different NLP tasks. In IR, the work of (Baeza-Yates, 2005) defines the foundations of Temporal-IR. Since, research have been tackling several topics such as query understanding (Metzler et al., 2009), temporal snippets generation (Alonso et al., 2007), temporal ranking (Kanhabua et al., 2011), temporal clustering (Alonso et al., 2009), or future retrieval (Radinsky and Horvitz, 2013).
In order to push forward further research in temporal NLP and IR, (Dias et al., 2014) developed TempoWordNet (TWn), an extension of WordNet (Miller, 1995), where each synset is augmented with its temporal connotation (past, present, future, or atemporal). It mainly relies on the quantitative analysis of the glosses associated to synsets,
and on the use of the resulting vectorial term representations for semi-supervised synset classification. While (Hasanuzzaman et al., 2014a) show that TWn can be useful to time-tag web snippets, less comprehensive results are shown in (Filannino and Nenadic, 2014), where TWn learning features did not lead to any classification improvements. In order to propose a more reliable resource, (Hasanuzzaman et al., 2014b) recently defined two new propagation strategies: probabilistic and hybrid respectively leading to TWnP and TWnH. Although some improvements was evidenced, no conclusive remarks could be reached.
There are several disadvantages of their approaches. First, it relies mostly on WordNet glosses and do not effectively exploit WordNet’s relation structure. Whereas we concentrate on the use of WordNet relations, glosses, and other attributes for the classification process. Second, strategies adopted to build TWnP and TWnH mainly depend on the probability estimates for the classes from Support Vector Machines (SVM) classifiers. However, probabilities are derived without using any post-calibration method. Converting scores to accurate probability estimates for multiclass problems requires a post-calibration procedure. In addition, there is no standard evaluation as to the accuracy of their approach. Graph-based Classification: To the best of our knowledge, we present the first work, which aims to recognize temporal dimension of word senses via graph-based classification algorithm. However, graph-based algorithms have been used to classify sentences and documents into subjective/objective or positive/negative level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005), instead of aiming at tagging at word sense level as we do. At the word level, a semi-supervised spin model is used for word polarity determination, where the graph is constructed using a variety of information such as gloss co-occurrences and WordNet links (Takamura et al., 2005). However, their model differs from ours.","One important remark can be made related to the difficulty of the task. This is particularly due to the fact that the temporal dimension of synsets is mainly judged upon their definition. For example, “dinosaur” can be classified as temporal or atemporal as its gloss “any of numerous extinct terrestrial reptiles of the Mesozoic era” allows both interpretations. Apart from it, while digging into the results we observed that classifying the temporal synsets into past, present, or future is more difficult than temporal vs. atemporal classification. It is due to the fact that past, present and future connotations are only indicative of the temporal orientation of the synset but cannot be taken as a strict class. Indeed, there are many temporal synsets, which are neither past, present nor future (e.g.monthly – a periodical that is published every month).","In this paper, we proposed a semi-supervised minimum cut framework to address the relatively unexplored problem of associating word senses with their underlying temporal dimensions. The basic idea is that instead of using single view, multiple views on the data would result in better temporal classification accuracy thus lead to a accurate and reliable temporal lexical resource. Thorough and comparative evaluations are performed to measure the quality of the resource. The results confirm the soundness of the proposed approach and the usefulness of the resource for temporal relation classification task. The resource is publicly available on https://www.anonymous.anonymous so that the community can benefit from it for relevant tasks and applications.
From a resource point of view, we would like to explore the effect of other graph construction methods, such as the use of freely available online dictionaries including thesaurus and distributional similarity measures. We would also like to use the resource for various applicative scenarios such as automatic analysis of time-oriented clinical narratives. As an example, one can imagine a system that automatically analyzes medical discharge summaries including previous diseases related to the current conditions, treatments, and the family history for medical decision making, data modeling and biomedical research.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
1,"We introduce positive-only projection (PoP), a novel technique for constructing semantic spaces and word embeddings. The PoP method is based on random projections. Hence, it is highly scalable and computationally efficient. In contrast to previous methods that use random projection matrices R with the expected value of 0 (i.e., E(R)=0), the proposed method uses R with E(R)>0. We use Kendall's tau_b distance to compute vector similarities in the resulting non-Gaussian spaces. Most importantly, since E(R), weighting methods such as positive pointwise mutual information (PPMI) can be applied to PoP-constructed spaces after their construction for efficiently transferring PoP embeddings onto spaces that are discriminative for semantic similarity assessments. Our PoP-constructed models, combined with PPMI, achieve an average score of 0.75 in the MEN relatedness test, which is comparable to results obtained by state-of-the-art top-performing algorithms.",Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction,7,"I am buying some of the motivation: the proposed method is much faster to train
than it is to train a neural network. Also, it keeps some properties of the
distribution when going to lower dimensionality. 

However, I am not convinced why it is so important for vectors to be
transformable with PPMI.

Most importantly, there is no direct comparison to related work.

Detailed comments:

- p.3: The definition of Kendall's tau that the authors use is strange. This is
NOT the original formula; I am not sure what it is and where it comes from.

- p.3: Why not use Spearman correlation as is standard in semantic tasks (and
as teh authors do at evaluation time)?

- The datasets chosen for evaluation are not the standard ones for measuring
semantic relatedness that the NLP community prefers. It is nice to try other
sets, but I would recommend to also include results on the standard ones.

- I can only see two lines on Figure 1. Where is the third line?

- There is no direct comparison to related work, just a statement that 

Some typos:

- large extend -- extent",,2,3,Poster,3,2,3,4,3,5,2,3,2016,"Any transformation from a count-based model to a predictive one can be expressed using a matrix notation such as:
Cp×n ×Tn×x = Pp×x. (1)
In Equation 1, C denotes the count-based model consisting of p vectors and n context elements (i.e., n dimensions). T is the transformation matrix that maps the p n-dimensional vectors in C to an x-dimensional space (often, but not necessarily, x 6= n and x n). Finally, P is the resulting x-dimensional predictive model. Note that T can be a composition of several transformations, e.g., a weighting transformation W followed by a projection onto a space of lower dimensionality R, i.e., Tn×x = Wn×n ×Rn×x.
In the proposed PoP technique, the transformation Tn×m (for m n, e.g., 100 ≤ m ≤ 5000) is simply a randomly generated matrix. The elements tij of Tn×m have the following distribution:
tij = { 0 with probability 1− s b 1Uα c with probability s , (2)
in which U is an independent uniform random variable in (0, 1], s is an extremely small number (e.g., s = 0.01) such that each row vector of T has at least one element that is not 0 (i.e.,∑m
i=1 tji 6= 0 for each row vector tj ∈ T). For α, we choose α = 0.5. Given Equations 1 and 2 and using the distributive property of multiplication over addition in matrices,5 the desired semantic space (i.e., P in Equation 1) can be constructed
5That is (A+B)×C = A×C+B×C.
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
using the two-step procedure of incremental word space construction known from RI:
Step 1. Each context element is mapped to one m-dimensional index vector ~r. ~r is randomly generated such that most elements in ~r are 0 and only a few are positive integers (i.e., the elements of ~r have the distribution given in Equation 2).
Step 2. Each target entity that is being analysed in the model is represented by a context vector ~v in which all the elements are initially set to 0. For each encountered occurrence of this target entity together with a context element (e.g., through a sequential scan of a corpus), we update ~v by adding the index vector ~r of the context element to it.
This process results in a model built directly at the reduced dimensionality m (i.e., P in Equation 1). The first step corresponds to the construction of the randomly generated transformation matrix T: Each index vector is a row of the transformation matrix T. The second step is an implementation of the matrix multiplication in Equation 1 which is distributed over addition: Each context vector is a row of P, which is computed in an iterative process.Once P is constructed, if desirable, similarities between entities can be computed by their Kendall’s τb (−1 ≤ τb ≤ 1) correlation (Kendall, 1938). In order to compute τb, we need to define a number of values. Given vectors ~x and ~y of the same dimension, we call a pair of observations (xj , yj) and (xj+1, yj+1) in ~x and ~y concordant if (xj < xj+1 ∧ yj < yj+1) ∨ (xj > xj+1 ∧ yj > yj+1). The pair is called discordant if (xj < xj+1∧ yj > yj+1) ∨ (xj > xj+1 ∧ yj < yj+1). Finally, the pair is called tied if xj = xj+1 ∨ yj = yj+1. Note that a tied pair is neither concordant nor discordant. We define n1 and n2 as the number of pairs with tied values in ~x and ~y, respectively. We use nc and nd to denote the number of concordant and discordant pairs, respectively. If m is the dimension of the two vectors, then n0 is defined as the total number of observation pairs: n0 = m(m−1) 2 . Given these definitions, Kendall’s τb is given by
τb = nc − nd√
(n0 − n1)(n0 − n2) .
To compute τb, we adopt an implementation of the algorithm proposed by Knight (1966), which has
a computational complexity of O(n log n).6 Since the vectors resulting from the PoP method have a very low dimensionality, this complexity does not harm the overall efficiency of the approach.
The choice of τb is motivated by generalising the role that cosine plays for computing similarities between vectors that are derived from a standard Gaussian random projection. In random projections with R of (asymptotic) N (0, 1) distribution, despite the common interpretation of the cosine similarity as the angle between two vectors, cosine can be seen as a measure of productmoment correlation coefficient between the two vectors. Since R and thus the obtained projected spaces have E = (0), Pearson’s correlation and the cosine measure have the same definition in these spaces (see also Jones and Furnas (1987) for a similar claim and on the relationships between correlation and the inner product and cosine). Subsequently, one can propose that in Gaussian random projections, Pearson’s correlation is used to compute similarities between vectors.
However, the use of projections proposed in this paper (i.e., T with a distribution set in Equation 2) will result in vectors that have a non-Gaussian distribution. In this case, τb becomes a reasonable candidate for measuring similarities (i.e., correlations between vectors) since it is a nonparametric correlation coefficient measure that does not assume a Gaussian distribution of projected spaces. However, we do not exclude the use of other similarity measures and may apply them in future work. In particular, we envisage additional transformations of PoP-constructed spaces to induce vectors with Gaussian distributions (see for instance the log-based PPMI transformation used in the next section). If a transformation to a Gaussian-like distribution is performed, then the use of Pearson’s correlation, which works under the assumption of Gaussian distribution, yields better results than Kendall’s correlation (as confirmed by our experiments).The PoP method is a randomized algorithm. In this class of algorithms, at the expense of a tolerable loss in accuracy of the outcome of the computations (of course, with a certain acceptable amount of probability) and by the help of ran-
6In our evaluation, we use the implementation of Knight’s algorithm in the Apache Commons Mathematics Library.
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
dom decisions, the computational complexity of algorithms for solving a problem is reduced (see, e.g., Karp (1991), for an introduction to randomized algorithms).7 For instance, using Gaussianbased sparse random projections in RI, the computation of eigenvectors (often of complexity of O(n2 logm)) is replaced by a much simpler process of random matrix construction (of an approximate complexity of O(n))—see (Bingham and Mannila, 2001). In return, a randomized algorithm such as the RI and PoP methods give different results even for the same input.
Assume the difference between the optimum result and the result from a randomized algorithm is given by δ (i.e., the error caused by replacing deterministic decisions with random ones). Much research in theoretical computer science and applied statistics focuses on specifying bounds for δ, which is often expressed as a function of the probability of encountered errors. For instance, δ and in Gaussian random projections are often derived from the lemma proposed by Johnson and Lindenstrauss (1984) and its variations. Similar studies for random projections in `1-normed spaces and deep neural networks are Indyk (2000) and Arora et al. (2014), respectively.
At this moment, unfortunately, we are not able to provide a detailed mathematical account for specifying δ and for the results obtained by the PoP method (nor are we able to pinpoint theoretical discussion about PoP’s underlying random projection). Instead, we rely on the outcome of our simulations and the performance of the method in an NLP task. Note that this is not an unusual situation. For instance, Kanerva et al. (2000) proposed RI with no mathematical justification. In fact, it was only a few years later that Li et al. (2006) proposed mathematical lemmas for justifying very sparse Gaussian random projections such as RI (QasemiZadeh, 2015). At any rate, projection onto manifolds is a vibrant research both in theoretical computer science and in mathematical statistics. Our research will benefit from this in the near future. Concerning δ, it can be shown that it and its variance σ2δ are functions of the dimension m of the projected space, that is: σ2δ ≈ 1 m , based on similar mathematical principles proposed by Kaski (1998) (and of Hecht-Nielsen (1994)) for random mapping methods.
7Such as many classic search algorithms that are proposed for solving NP-complete problems in artificial intelligence.
Our empirical research and observations on language data show that projections using the PoP method exhibit similar behavioural patterns as other sparse random projections in α-normed spaces. The dimensionm of random index vectors can be seen as the capacity of the method to memorize and distinguish entities. Form up to a certain number (100 ≤ m ≤ 4000) in our experiments, as was expected, a PoP-constructed model for a large m shows a better performance and smaller δ than a model for a small m. Since observations in semantic spaces have a very-long-tailed distribution, choosing different values of non-zero elements for index vectors does not effect the performance (as mentioned, in most cases 2 or 3 non-zero elements are sufficient). Furthermore, changes in the adopted distribution of tij only slightly effect the performance of the system, due to the use of τb as a similarity measure.
In the next section, using empirical investigations we show the advantages of the PoP model and support the claims from this section.For evaluation purposes, we use the MEN relatedness test set (Bruni et al., 2014) and the UKWaC corpus (Baroni et al., 2009). The dataset consists of 3000 pairs of words (from 751 distinct tagged lemmas). Similar to other ‘relatedness tests’, Spearman’s rank correlation ρ score from the comparison of human-based ranking and system-induced rankings is the figure of merit. We use these resources for evaluation since they are in public domain, both the dataset and corpus are large, and they have been used for evaluating several word space models—for example, see Levy et al. (2015), Tsvetkov et al. (2015), Baroni et al. (2014), Kiela and Clark (2014). In this section, unless otherwise stated, we use cosine for similarity measurements.
Figure 1 shows the performance of the simple count-based word space model for lemmatizedcontext-windows that extend symmetrically around lemmas from MEN.8 As expected, up to a certain context-window size, the performance using count-based methods increases with an
8We use the tokenized preprocessed UKWaC. However, except for using part-of-speech tags for locating lemmas listed in MEN, we do not use any additional information or processes (i.e., no frequency cut-off for context selection, no syntactic information, etc.).
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
extension of the window.9 For context-windows larger than 25+25 the performance gradually declines. More importantly, in all cases we have ρ < 0.50.
We performed the same experiments using the RI technique. For each context window size, we performed 10 runs of the RI model construction. Figure 1 reports for each context-window size the average of the observed performances for the 10 RI models. In this experiment, we used index vectors of dimensionality 1000 containing 4 nonzero elements. As shown in Figure 1, the average performance of the RI is almost identical to the performance of the count-based model. This is an expected result since RI’s objective is to retain Euclidean distances between vectors (thus cosine) but in spaces of lowered dimensionality. In this sense, RI is successful and achieves its goal of lowering the dimensionality while keeping distances between vectors. But it does not yield any improvements in the similarity assessment task.
We then performed similar experiments using PoP-constructed models, with the same context window sizes and the same dimensions as in the RI experiments, averaging again over 10 runs for each context window size. The performance is also reported in Figure 1. For the PoP method, however, instead of using the cosine measure we use τb for measuring similarity. The PoP-constructed models converge faster than RI and count-based method and for smaller context-windows they outperform the count-based and RI methods with a large margin. However, as the size of the windows grow, performances of these methods become more similar (but PoP still outperforms the others). One possible interpretation is that PoP is more sensitive to noise; if that is the case, this can be addressed by increasing the dimensionality of index vectors in order to reduce distortions in the projected spaces. In any case, the performance of PoP remains above 0.50 (i.e., ρ > 0.50).Although PoP outperforms RI and count-based models, its performance is still not satisfying since its index vectors are not sufficiently discriminative. In order to remedy this, transformations based on association measures such as positive pointwise mutual information (PPMI) have been
9After all, in models for relatedness tests, relationships of topical nature play a more important role than other relationships such as synonymy.
1+ 1 4+ 4
9+ 9
17 +1 7
0.65
0.7
0.75
Context Window Size
Sp ea
rm an
’s C
or re
la tio
n ρ
PMI-Dense+Cos PPMI-Dense+Cos
Our PoP+PPMI+Kendall Our PoP+PPMI+Pearson
Figure 2: Performances of (P)PMI-transformed models for various sizes of context-windows. From context size 4+4, the performance remains almost intact (0.72 for PMI and 0.75 for PPMI). We also report the average performance for PoP-constructed models constructed at the dimensionality m = 1000 and s = 0.002. PoP+PPMI+Pearson exhibits a performance similar as dense PPMI-weighted models, however, much faster and using far less amount of computational resources. Note that reported PoP+PMI performances can be enhanced by using m > 1000.
proposed for the adjustment of weights (which correspond to coordinates of vectors). This often enhances the discriminatory power of the models and thus increases their performance in semantic similarity assessment tasks (see Church and Hanks (1990), Turney (2001), Turney (2008), and Levy et al. (2015)). For a given set of vectors, PMI is interpreted as a measure of information overlap between vectors. As put by Bouma (2009), PMI is a mathematical tool for measuring how much the actual probability of a particular co-occurrence (e.g., two words in a word space) deviate from the expected probability of their individual occurrences (e.g., the probability of occurrences of each word in a words space) under the assumption of independence (i.e., the occurrence of one word does not affect the occurrences of other words).
In Figure 2, we show the performance of PMI-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
transformed spaces for small context-windows. One major problem with PMI is that it requires a lot of resources for its computation (as put by, PMI-induced spaces are dense). Even for a small number of entities in the model, due to the powerlaw distribution of word co-occurrences, the resulting spaces from large context-windows are high-dimensional so that the computation of PMI weights becomes intractable. But still, even for small context-windows, PMI+Cosine models outperform other techniques including the introduced count-based PoP method. The performance of PMI models can be further enhanced by its normalization, often discarding negative values10 and using Positive PMI values (PPMI). Also, SVD truncation of PPMI-weighted spaces can improve the performance slightly (see the above mentioned references) requiring, however, expensive computations of eigenvectors.11 For a p× n matrix with elements vxy, 1 ≤ x ≤ p and 1 ≤ y ≤ n, we compute the PPMI weight for a component vxy as follows:
ppmi(vxy) = max(0, log vxy×
∑p i=1 ∑n j=1 vij∑p
i=1 viy× ∑n j=1 vxj ). (3)
The most important benefit of the PoP method is that PoP-constructed models, in contrast to previously suggested random projection-based models, can be still weighted using PPMI (or any other weighting techniques applicable to the original count-based models). In an RI-constructed model, the sum of values of row and column vectors of the model are always 0 (i.e., ∑p i=1 viy and ∑n j=1 vxj in Equation 3 are always 0). As mentioned earlier, this is due to the fact that a random projection matrix in RI has an asymptotic standard Gaussian distribution (i.e., transformation matrix R has E(R) = 0). As a result, PPMI weights for the RIinduced vector elements are undefined. In contrast to RI, the sum of values of vector elements in the PoP-constructed models is always greater than 0 (since the transformation is carried out by a projection matrix R of E(R) > 0). Also, depending on the structure of data in the underlying countbased model, by choosing a suitably large value of s, it can be guaranteed that the sum of column vectors is always a non-zero value. Hence, vectors in PoP models can undergo the PPMI transformation defined in Equation 3. Moreover, the PPMI
10See Bouma (2009) for a mathematical delineation. Jurafsky and Martin (2015) also provide an intuitive description.
11In our experiments, applying SVD truncation to models results in negligible improvements between 0.01 and 0.001.
transformation is much faster, compared to the one performed on count-based models, due to the low dimensionality of vectors in the PoP-constructed model. Therefore, the PoP method makes it possible to benefit both from the high efficiency of randomized techniques as well as from the high accuracy of PPMI transformation in semantic similarity tasks.
If we put aside the information-theoretic interpretation of PPMI weighting (i.e., distilling statistical information that really matters), the logarithmic transformation of probabilities in the PPMI definition plays the role of a power transformation process for converting long-tailed distributions in count-based models to Gaussian-like distributions in predictive models. From a statistical perspective, any variation of PMI transformation can be seen as an attempt to stabilize variance of vector coordinates and therefore to make the observations more similar/fit to Gaussian distribution (a practice with long history in many research, particularly in biological and psychological sciences).
To exemplify this phenomenon, in Figure 3, we show histograms of the distributions of the assigned weights to the vector that represents the lemmatized form of the verb ‘abandon’ in various models. As shown, the raw collected frequencies in the count-based model have a long tail distribution (see Figure 3a). Applying the log transformation to this vector yields a vector of weights with a Gaussian distribution (Figure 3b). Weights in the RI-constructed vector (Figure 3c) have a perfect Gaussian distribution but with expected value of 0 (i.e., N (0, 1)). The PoP method, however, largely preserves the long tail distribution of coordinates from the original space (Figure 3d), which in turn can be weighted using PPMI and thereby transformed into a Gaussian-like distribution.
Given that models after PPMI transformation have bell-shaped Gaussian distributions, we expect that a correlation measure such as Pearson’s r, which takes advantage of the prior knowledge about the distribution of data, outperforms the non-parametric Kendall’s τb for computing similarities in PPMI-transformed spaces.12 This is indeed the case (see Figure 2).
12Note that using correlation measures such as Pearson’s r and Kendall’s τb in count-based model may excel measures such as cosine. However, their application is limited due to the high-dimensionality of count-based methods.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
100 10,000 1
10
100
1,000
log(Raw Frequencies)
log(Frequency)
(a) Count-based
0 50
100
200
300
400
PMI Weights
Frequency
(b) PMI
−1,700 0 1,700
20
40
RI Weights
Frequency
(c) RI
10 1,000 100,000
10
log(POP Weights)
log(Frequency)
(d) POP
−50 0 50 100
100
200
300
400
POP PMI Weights
Frequency
(e) POP+PMI
Figure 3: A histogram of the distribution of frequencies of weights in various models from 1+1 contextwindows for the lemmatized form of the verb ‘abandon’ in the UKWaC corpus.
10 0 50 0 10 00 15 00 20 00
30 00
40 00
50 00
0.5
0.55
0.6
0.7
0.75
Models’ Dimensionality (m)
Sp ea
rm an
’s C
or re
la tio
n ρ
PoP+PPMI+Pearson PoP+PPMI+ Kendall
PoP+Kendall
Figure 4: Changes in PoP’s performance when the dimensionality of models increases. The average performance in each set-up is shown by marked lines. The margins around these lines show the minimum and maximum performance observed in 10 independent executions.As discussed in § 2.3, PoP is a randomized algorithm and its performance is influenced by a number of parameters. In this section, we study the PoP method’s behaviour by reporting its performance in the MEN relatedness test under different parameter settings. To keep evaluations and reports in a manageable size, we focus on models built using context-windows of size 4+4.
Figure 4 shows the method’s performance when the dimensionm of the projected index vectors increases. In these experiments, index vectors are built using 4 non-zero elements; thus, as m increases, s in Equation 2 decreases. For each m, 100 ≤ m ≤ 5000, the models are built 10 times and the average as well as the maximum and the minimum observed performances in these experiments are reported. For PPMI transformed PoP spaces, with increasing dimensions, the performance boosts and, furthermore, the variance in performance (i.e., the shaded areas)13 gets smaller.
13Evidently, the probability of worst and best performances can be inferred from the reported average results.
1 2 4 8 16 32 0.5
0.55
0.6
0.7
0.75
1
Number of Non-Zero Elements in Index Vectors
Sp ea
rm an
’s C
or re
la tio
n ρ
PoP+PPMI+Pearson PoP+PPMI+ Kendall
PoP+Kendall
Figure 5: Changes in PoP’s performances when the dimensionality of models are fixed to m = 3000 and the number of non-zero elements in index vectors (i.e., s) increases. The average performances in each set-up are shown by marked lines. The margins around these lines show the minimum and maximum performance observed in 10 independent executions.
However, for the count-based PoP method without PPMI transformation (shown by dash-dotted lines) and with the number of non-zero elements fixed to 4, increasing m over 2000 decreases the performance. This is unexpected since an increase in dimensionality is usually assumed to entail an increase in performance. This behaviour, however, can be the result of using a very small s; simply put, the number of non-zero elements are not sufficient to build projected spaces with adequate distribution. To investigate this matter, we study the performance of the method with the dimension m fixed to 3000 but with index vectors built using different numbers of non-zero elements, i.e., different values of s.
Figure 5 shows the observed performances. For PPMI-weighted spaces, increasing the number of non-zero elements clearly deteriorates the performance. For unweighted PoP models, an increase in s up to the limit that does not result in nonorthogonal index vectors enhances performances. As shown in Figure 6, when the dimensionality
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
4 6 8 10 12 14 16 18 20 0
0.2
0.4
0.6
0.8
1
non-zero elements
P 6⊥
#index vectors n = 104
1 4
·104n
#non-zero elements = 8
m = 100 m = 1000 m = 2000
Figure 6: Proportion of non-orthogonal pairs of index vectors (i.e., P6⊥) obtained in a simulation for various dimensionality and number of non-zero elements. The left figure shows the changes of P 6⊥ For a fixed set of index vectors n = 104 when number of non-zero elements increases. The right figure shows P6⊥ when the number of non-zero elements is fixed to 8 but the number of index vectors n increases. As shown, P 6⊥ is determined by the number of non-zero elements and dimensionality of index vectors and independently of n.
of the index vectors is fixed and s increases, the chances of having non-orthogonal vectors in index vectors boosts. Hence, the chances of distortions in similarities increase. These distortions can enhance the result if they are controlled (e.g., using a training procedure such as the one used in neural net embedding). However, when left to chance, they can often lower the performance. Evidently, this is a simplified justification: in fact s plays the role of a switch that controls resemblance between distribution of data in the original space and the projected/transformed spaces. It seems that the sparsity of vectors in the original matrix plays a role for finding the optimal value for s. If PoP-constructed models are used directly (together with τb) for computing similarities, then we propose 0.002 < s. If PoP-constructed models are subject to an additional weighting process for stabilising vector distributions into Gaussianlike distributions such as PPMI, we propose using only 1 or 2 non-zero elements.
Last but not least, we confirm that by carefully selecting context elements (i.e., removing stop words and using lower and upper bound frequency cut-offs for context selection) and fine tuning PoP+PPMI+Pearson (i.e., increasing the dimension of models and scaling PMI weights as in Levy et al. (2015)) we achieve an even higher score in the MEN test (i.e., an average of 0.78 with the max of 0.787). Moreover, although improvements from applying SVD truncation are negligible, we can employ it for reducing the dimensionality of PoP vectors (e.g., from 6000 to 200).If the paper accepted, codes and resulting embeddings from experiments will be shared alongside the camera ready version.",,"The development of data-driven methods of natural language processing starts with an educated guess, a distributional hypothesis: We assume that some properties of linguistic entities can be modelled by ‘some statistical’ observations in language data. In the second step, this statistical information (which is determined by the hypothesis) is collected and represented in a mathematical framework. In the third step, tools provided by the chosen mathematical framework are used to implement a similarity-based logic to identify linguistic structures, and/or to verify the proposed
hypothesis. Harris’s distributional hypothesis is a well-known example of step one that states that meanings of words correlate with the context in which the words appear. Vector space models and η-normed-based similarity measures are renowned examples for steps two and three, respectively (i.e., word space models or word embeddings).
However, as pointed out for instance by Baroni et al. (2014), the count-based models resulting from the steps two and three are not discriminative enough to achieve satisfactory results; instead, predictive models are required. To this end, an additional transformation step is often added. Turney and Pantel (2010) describe this additional step as a combination of weighting and dimensionality reduction.1 This transformation from count-based to predictive models can be implemented simply via a collection of rules of thumb (i.e., heuristics), and/or it can involve more sophisticated mathematical transformations, such as converting raw counts to probabilities and using matrix factorization techniques. Likewise, by exploiting large amounts of computational power available nowadays, this transformation can be achieved via neural word embedding techniques (Mikolov et al., 2013; Levy and Goldberg, 2014).
To a large extend, the need for such transformations arises from the heavy-tailed distributions that we often find in statistical natural language models (such as the Zipfian distribution of words in contexts when building word spaces). Consequently, count-based models are sparse and highdimensional and therefore both computationally expensive to manipulate (due of the high dimensionality of models) and nondiscriminatory (due to the combination of the high-dimensionality of the models and the sparseness of observations, see Minsky and Papert (1969, chap. 12)).
1Similar to topics of feature weighting, selection, and engineering in statistical machine learning.
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
On the one hand, although neural networks are the top performers for addressing this problem, their usage is highly costly: they need to be trained that is often very time-consuming,2 and their performance can vary from one task to another depending on their objective function.3 On the other hand, although methods based on random projections efficiently address the problem of reducing the dimensionality of vectors—such as RI (Kanerva et al., 2000), reflective random indexing (RRI), (Cohen et al., 2010), ISA (Baroni et al., 2007) and random Manhattan indexing (RMI) (Zadeh and Handschuh, 2014)—in effect they retain distances between entities in the original space.4 In addition, since these methods use asymptotic Gaussian or Cauchy random projection matrices R with E(R) = 0, their resulting vectors cannot be adjusted and transformed using weighting techniques such as PPMI. Hence these methods do not outperform neural embeddings and combinations of PPMI weighting of count-baseds model followed by matrix factorization—such as truncation of weighted-vectors using singular value decomposition (SVD).
In order to overcome these problems, we propose a new method called positive-only projection (PoP). PoP is an incremental semantic space construction method that is similar to RI in the sense that it employs random projections. Hence, the construction of models using PoP does not require prior computation of embeddings but simply generating random vectors. However, in contrast to RI and similar methods, the PoP-constructed spaces can undergo weighting transformations such as PPMI. This is due to the fact that PoP uses random vectors that contain only positive integer values. Since the method is based on random projections, models can be built incrementally and efficiently. Since the vectors in PoP-constructed models are small (i.e., dimensionality of a few hundred), applying weighting methods such as PPMI to these models is incredibly faster than applying them to classical count-based models. Combined with a suitable weighting method such PPMI, the PoP method yields competitive results concern-
2Baroni et al. (2014) state that it took Ronan Collobert two months to train a set of embeddings from a Wikipedia dump. Even using GPU-accelerated computing, the required computation and training time for inducing neural word embeddings is high.
3Ibid, see results reported in supplemental materials. 4For η-normed space that they are designed for, i.e., η = 2
for RI, RRI, and ISA and η = 1 for RMI.
ing accuracy in semantic similarity assessment, compared for instance to neural net-based approaches and combinations of count-based models with weighting and matrix factorization. These results, however, are achieved without the need for heavy computations. Thus, instead of hours, days or months, models can be built in a matter of a few seconds or minutes. Note that even without weighting transformation, PoP-constructed models display a better performance than RI on tasks of semantic similarity assessments.
We describe the PoP method in § 2. In order to evaluate our models, in § 3, we report the performance of the PoP method in the MEN relatedness test. Finally, § 4 concludes with a discussion.",,"We introduced a new technique called PoP for incremental construction of semantic spaces. PoP can be seen as a dimensionality reduction method, which is based on a newly devised random projection matrix R of E(R). The major benefit of PoP is that it transfers vectors onto spaces of lower dimensionality without changing their distribution to a Gaussian shape with E = 0. Transformed spaces obtained using PoP can therefore be manipulated similarly to count-based models, only much faster and consequently requiring a considerably lower amount of computational resources.
PPMI weighting can be easily applied to POP-constructed models. In our experiments, we observe that PoP+PPMI+Pearson can be used to build models that achieve a high performance in semantic relatedness tests. More concretely, for index vector dimensions m ≥ 3000, PoP+PPMI+Pearson achieves an average score of 0.75 in the MEN relatedness test, which is comparable to many neural embedding techniques (see scores reported in Chen and de Melo (2015) and Tsvetkov et al. (2015)). However, in contrast to these approaches, PoP+PPMI+Pearson achieves this competitive performance without the need for time-consuming training of neural nets. Moreover, the involved processes are all done on vectors of low dimensionality. Hence, the PoP method can dramatically enhance the performance of a number of distributional natural language analyses.
This research can be extended in several ways. Firstly, theoretical accounts for PoP still need to be investigated. Secondly, many new methods can be designed by combining the proposed PoP and serialization of various weighting techniques (such as previously done for RI). Moreover, since weighting processes can be applied at a very low dimensionality, many computationallyintense techniques which cannot be applied to high-dimensional models for stabilising distributions of vectors (e.g., the use of power transforms such as BoxCox that maximizes the correlation coefficient of a Gaussian distribution of the weighted vectors) are now available to be used. On a similar basis, in the process of developing neural embeddings, PoP-constructed models can replace the high-dimensional count-based models as an input. Lastly, an important avenue of research is the use of so-called derandomization techniques to make PoP models linguistically more informed.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899",
2,"We introduce positive-only projection (PoP), a novel technique for constructing semantic spaces and word embeddings. The PoP method is based on random projections. Hence, it is highly scalable and computationally efficient. In contrast to previous methods that use random projection matrices R with the expected value of 0 (i.e., E(R)=0), the proposed method uses R with E(R)>0. We use Kendall's tau_b distance to compute vector similarities in the resulting non-Gaussian spaces. Most importantly, since E(R), weighting methods such as positive pointwise mutual information (PPMI) can be applied to PoP-constructed spaces after their construction for efficiently transferring PoP embeddings onto spaces that are discriminative for semantic similarity assessments. Our PoP-constructed models, combined with PPMI, achieve an average score of 0.75 in the MEN relatedness test, which is comparable to results obtained by state-of-the-art top-performing algorithms.",Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction,7,"The paper presents a positive-only projection (PoP) word embedding method. This
is a random projection method with a random projection matrix whose expected
value is positive. The authors argue that this enables the application of PPMI
which is not possible with an expected value of 0 and that being a random
projection method, their computation is efficient.

My main reservation about this paper has to do with its clarity. Particularly:

1. I could not understand the core difference between the method proposed in
the paper and previous random projection methods. Hence, I could not understand
how (and whether) the advantages the authors argue to achieve hold.

2. It was hard to follow the arguments of the paper starting from the
introduction. 

3. Some of the arguments of the paper are not supported: 

- Line 114: Sentence starts with ""in addition""

- Line 137: Sentence starts with ""Since""

- Line 154: Sentence starts with ""thus""

4. While I have worked on vector space modeling (who hasn't ?), I am not an
expert to random projections and have not used them in my research. It was hard
for me to understand the logic behind this research avenue from the paper. I
believe that a paper should be self contained and possible to follow by people
with some experience in the field.

5. The paper has lots of English mistakes (86: ""To a large extend"", 142: ""such
PPMI"").

In addition, I cannot see why the paper is evaluating only on MEN. There are a
couple of standard benchmarks (MEN, WordSeim, SimLex and a couple of others) -
if you present a new method, I feel that it is insufficient to evaluate only on
one dataset unless you provide a good justification.

I recommend that the authors will substantially improve the presentation in the
paper and will resubmit to another conference.",,2,3,Oral Presentation,2,4,4,3,3,5,3,4,2016,"Any transformation from a count-based model to a predictive one can be expressed using a matrix notation such as:
Cp×n ×Tn×x = Pp×x. (1)
In Equation 1, C denotes the count-based model consisting of p vectors and n context elements (i.e., n dimensions). T is the transformation matrix that maps the p n-dimensional vectors in C to an x-dimensional space (often, but not necessarily, x 6= n and x n). Finally, P is the resulting x-dimensional predictive model. Note that T can be a composition of several transformations, e.g., a weighting transformation W followed by a projection onto a space of lower dimensionality R, i.e., Tn×x = Wn×n ×Rn×x.
In the proposed PoP technique, the transformation Tn×m (for m n, e.g., 100 ≤ m ≤ 5000) is simply a randomly generated matrix. The elements tij of Tn×m have the following distribution:
tij = { 0 with probability 1− s b 1Uα c with probability s , (2)
in which U is an independent uniform random variable in (0, 1], s is an extremely small number (e.g., s = 0.01) such that each row vector of T has at least one element that is not 0 (i.e.,∑m
i=1 tji 6= 0 for each row vector tj ∈ T). For α, we choose α = 0.5. Given Equations 1 and 2 and using the distributive property of multiplication over addition in matrices,5 the desired semantic space (i.e., P in Equation 1) can be constructed
5That is (A+B)×C = A×C+B×C.
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
using the two-step procedure of incremental word space construction known from RI:
Step 1. Each context element is mapped to one m-dimensional index vector ~r. ~r is randomly generated such that most elements in ~r are 0 and only a few are positive integers (i.e., the elements of ~r have the distribution given in Equation 2).
Step 2. Each target entity that is being analysed in the model is represented by a context vector ~v in which all the elements are initially set to 0. For each encountered occurrence of this target entity together with a context element (e.g., through a sequential scan of a corpus), we update ~v by adding the index vector ~r of the context element to it.
This process results in a model built directly at the reduced dimensionality m (i.e., P in Equation 1). The first step corresponds to the construction of the randomly generated transformation matrix T: Each index vector is a row of the transformation matrix T. The second step is an implementation of the matrix multiplication in Equation 1 which is distributed over addition: Each context vector is a row of P, which is computed in an iterative process.Once P is constructed, if desirable, similarities between entities can be computed by their Kendall’s τb (−1 ≤ τb ≤ 1) correlation (Kendall, 1938). In order to compute τb, we need to define a number of values. Given vectors ~x and ~y of the same dimension, we call a pair of observations (xj , yj) and (xj+1, yj+1) in ~x and ~y concordant if (xj < xj+1 ∧ yj < yj+1) ∨ (xj > xj+1 ∧ yj > yj+1). The pair is called discordant if (xj < xj+1∧ yj > yj+1) ∨ (xj > xj+1 ∧ yj < yj+1). Finally, the pair is called tied if xj = xj+1 ∨ yj = yj+1. Note that a tied pair is neither concordant nor discordant. We define n1 and n2 as the number of pairs with tied values in ~x and ~y, respectively. We use nc and nd to denote the number of concordant and discordant pairs, respectively. If m is the dimension of the two vectors, then n0 is defined as the total number of observation pairs: n0 = m(m−1) 2 . Given these definitions, Kendall’s τb is given by
τb = nc − nd√
(n0 − n1)(n0 − n2) .
To compute τb, we adopt an implementation of the algorithm proposed by Knight (1966), which has
a computational complexity of O(n log n).6 Since the vectors resulting from the PoP method have a very low dimensionality, this complexity does not harm the overall efficiency of the approach.
The choice of τb is motivated by generalising the role that cosine plays for computing similarities between vectors that are derived from a standard Gaussian random projection. In random projections with R of (asymptotic) N (0, 1) distribution, despite the common interpretation of the cosine similarity as the angle between two vectors, cosine can be seen as a measure of productmoment correlation coefficient between the two vectors. Since R and thus the obtained projected spaces have E = (0), Pearson’s correlation and the cosine measure have the same definition in these spaces (see also Jones and Furnas (1987) for a similar claim and on the relationships between correlation and the inner product and cosine). Subsequently, one can propose that in Gaussian random projections, Pearson’s correlation is used to compute similarities between vectors.
However, the use of projections proposed in this paper (i.e., T with a distribution set in Equation 2) will result in vectors that have a non-Gaussian distribution. In this case, τb becomes a reasonable candidate for measuring similarities (i.e., correlations between vectors) since it is a nonparametric correlation coefficient measure that does not assume a Gaussian distribution of projected spaces. However, we do not exclude the use of other similarity measures and may apply them in future work. In particular, we envisage additional transformations of PoP-constructed spaces to induce vectors with Gaussian distributions (see for instance the log-based PPMI transformation used in the next section). If a transformation to a Gaussian-like distribution is performed, then the use of Pearson’s correlation, which works under the assumption of Gaussian distribution, yields better results than Kendall’s correlation (as confirmed by our experiments).The PoP method is a randomized algorithm. In this class of algorithms, at the expense of a tolerable loss in accuracy of the outcome of the computations (of course, with a certain acceptable amount of probability) and by the help of ran-
6In our evaluation, we use the implementation of Knight’s algorithm in the Apache Commons Mathematics Library.
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
dom decisions, the computational complexity of algorithms for solving a problem is reduced (see, e.g., Karp (1991), for an introduction to randomized algorithms).7 For instance, using Gaussianbased sparse random projections in RI, the computation of eigenvectors (often of complexity of O(n2 logm)) is replaced by a much simpler process of random matrix construction (of an approximate complexity of O(n))—see (Bingham and Mannila, 2001). In return, a randomized algorithm such as the RI and PoP methods give different results even for the same input.
Assume the difference between the optimum result and the result from a randomized algorithm is given by δ (i.e., the error caused by replacing deterministic decisions with random ones). Much research in theoretical computer science and applied statistics focuses on specifying bounds for δ, which is often expressed as a function of the probability of encountered errors. For instance, δ and in Gaussian random projections are often derived from the lemma proposed by Johnson and Lindenstrauss (1984) and its variations. Similar studies for random projections in `1-normed spaces and deep neural networks are Indyk (2000) and Arora et al. (2014), respectively.
At this moment, unfortunately, we are not able to provide a detailed mathematical account for specifying δ and for the results obtained by the PoP method (nor are we able to pinpoint theoretical discussion about PoP’s underlying random projection). Instead, we rely on the outcome of our simulations and the performance of the method in an NLP task. Note that this is not an unusual situation. For instance, Kanerva et al. (2000) proposed RI with no mathematical justification. In fact, it was only a few years later that Li et al. (2006) proposed mathematical lemmas for justifying very sparse Gaussian random projections such as RI (QasemiZadeh, 2015). At any rate, projection onto manifolds is a vibrant research both in theoretical computer science and in mathematical statistics. Our research will benefit from this in the near future. Concerning δ, it can be shown that it and its variance σ2δ are functions of the dimension m of the projected space, that is: σ2δ ≈ 1 m , based on similar mathematical principles proposed by Kaski (1998) (and of Hecht-Nielsen (1994)) for random mapping methods.
7Such as many classic search algorithms that are proposed for solving NP-complete problems in artificial intelligence.
Our empirical research and observations on language data show that projections using the PoP method exhibit similar behavioural patterns as other sparse random projections in α-normed spaces. The dimensionm of random index vectors can be seen as the capacity of the method to memorize and distinguish entities. Form up to a certain number (100 ≤ m ≤ 4000) in our experiments, as was expected, a PoP-constructed model for a large m shows a better performance and smaller δ than a model for a small m. Since observations in semantic spaces have a very-long-tailed distribution, choosing different values of non-zero elements for index vectors does not effect the performance (as mentioned, in most cases 2 or 3 non-zero elements are sufficient). Furthermore, changes in the adopted distribution of tij only slightly effect the performance of the system, due to the use of τb as a similarity measure.
In the next section, using empirical investigations we show the advantages of the PoP model and support the claims from this section.For evaluation purposes, we use the MEN relatedness test set (Bruni et al., 2014) and the UKWaC corpus (Baroni et al., 2009). The dataset consists of 3000 pairs of words (from 751 distinct tagged lemmas). Similar to other ‘relatedness tests’, Spearman’s rank correlation ρ score from the comparison of human-based ranking and system-induced rankings is the figure of merit. We use these resources for evaluation since they are in public domain, both the dataset and corpus are large, and they have been used for evaluating several word space models—for example, see Levy et al. (2015), Tsvetkov et al. (2015), Baroni et al. (2014), Kiela and Clark (2014). In this section, unless otherwise stated, we use cosine for similarity measurements.
Figure 1 shows the performance of the simple count-based word space model for lemmatizedcontext-windows that extend symmetrically around lemmas from MEN.8 As expected, up to a certain context-window size, the performance using count-based methods increases with an
8We use the tokenized preprocessed UKWaC. However, except for using part-of-speech tags for locating lemmas listed in MEN, we do not use any additional information or processes (i.e., no frequency cut-off for context selection, no syntactic information, etc.).
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
extension of the window.9 For context-windows larger than 25+25 the performance gradually declines. More importantly, in all cases we have ρ < 0.50.
We performed the same experiments using the RI technique. For each context window size, we performed 10 runs of the RI model construction. Figure 1 reports for each context-window size the average of the observed performances for the 10 RI models. In this experiment, we used index vectors of dimensionality 1000 containing 4 nonzero elements. As shown in Figure 1, the average performance of the RI is almost identical to the performance of the count-based model. This is an expected result since RI’s objective is to retain Euclidean distances between vectors (thus cosine) but in spaces of lowered dimensionality. In this sense, RI is successful and achieves its goal of lowering the dimensionality while keeping distances between vectors. But it does not yield any improvements in the similarity assessment task.
We then performed similar experiments using PoP-constructed models, with the same context window sizes and the same dimensions as in the RI experiments, averaging again over 10 runs for each context window size. The performance is also reported in Figure 1. For the PoP method, however, instead of using the cosine measure we use τb for measuring similarity. The PoP-constructed models converge faster than RI and count-based method and for smaller context-windows they outperform the count-based and RI methods with a large margin. However, as the size of the windows grow, performances of these methods become more similar (but PoP still outperforms the others). One possible interpretation is that PoP is more sensitive to noise; if that is the case, this can be addressed by increasing the dimensionality of index vectors in order to reduce distortions in the projected spaces. In any case, the performance of PoP remains above 0.50 (i.e., ρ > 0.50).Although PoP outperforms RI and count-based models, its performance is still not satisfying since its index vectors are not sufficiently discriminative. In order to remedy this, transformations based on association measures such as positive pointwise mutual information (PPMI) have been
9After all, in models for relatedness tests, relationships of topical nature play a more important role than other relationships such as synonymy.
1+ 1 4+ 4
9+ 9
17 +1 7
0.65
0.7
0.75
Context Window Size
Sp ea
rm an
’s C
or re
la tio
n ρ
PMI-Dense+Cos PPMI-Dense+Cos
Our PoP+PPMI+Kendall Our PoP+PPMI+Pearson
Figure 2: Performances of (P)PMI-transformed models for various sizes of context-windows. From context size 4+4, the performance remains almost intact (0.72 for PMI and 0.75 for PPMI). We also report the average performance for PoP-constructed models constructed at the dimensionality m = 1000 and s = 0.002. PoP+PPMI+Pearson exhibits a performance similar as dense PPMI-weighted models, however, much faster and using far less amount of computational resources. Note that reported PoP+PMI performances can be enhanced by using m > 1000.
proposed for the adjustment of weights (which correspond to coordinates of vectors). This often enhances the discriminatory power of the models and thus increases their performance in semantic similarity assessment tasks (see Church and Hanks (1990), Turney (2001), Turney (2008), and Levy et al. (2015)). For a given set of vectors, PMI is interpreted as a measure of information overlap between vectors. As put by Bouma (2009), PMI is a mathematical tool for measuring how much the actual probability of a particular co-occurrence (e.g., two words in a word space) deviate from the expected probability of their individual occurrences (e.g., the probability of occurrences of each word in a words space) under the assumption of independence (i.e., the occurrence of one word does not affect the occurrences of other words).
In Figure 2, we show the performance of PMI-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
transformed spaces for small context-windows. One major problem with PMI is that it requires a lot of resources for its computation (as put by, PMI-induced spaces are dense). Even for a small number of entities in the model, due to the powerlaw distribution of word co-occurrences, the resulting spaces from large context-windows are high-dimensional so that the computation of PMI weights becomes intractable. But still, even for small context-windows, PMI+Cosine models outperform other techniques including the introduced count-based PoP method. The performance of PMI models can be further enhanced by its normalization, often discarding negative values10 and using Positive PMI values (PPMI). Also, SVD truncation of PPMI-weighted spaces can improve the performance slightly (see the above mentioned references) requiring, however, expensive computations of eigenvectors.11 For a p× n matrix with elements vxy, 1 ≤ x ≤ p and 1 ≤ y ≤ n, we compute the PPMI weight for a component vxy as follows:
ppmi(vxy) = max(0, log vxy×
∑p i=1 ∑n j=1 vij∑p
i=1 viy× ∑n j=1 vxj ). (3)
The most important benefit of the PoP method is that PoP-constructed models, in contrast to previously suggested random projection-based models, can be still weighted using PPMI (or any other weighting techniques applicable to the original count-based models). In an RI-constructed model, the sum of values of row and column vectors of the model are always 0 (i.e., ∑p i=1 viy and ∑n j=1 vxj in Equation 3 are always 0). As mentioned earlier, this is due to the fact that a random projection matrix in RI has an asymptotic standard Gaussian distribution (i.e., transformation matrix R has E(R) = 0). As a result, PPMI weights for the RIinduced vector elements are undefined. In contrast to RI, the sum of values of vector elements in the PoP-constructed models is always greater than 0 (since the transformation is carried out by a projection matrix R of E(R) > 0). Also, depending on the structure of data in the underlying countbased model, by choosing a suitably large value of s, it can be guaranteed that the sum of column vectors is always a non-zero value. Hence, vectors in PoP models can undergo the PPMI transformation defined in Equation 3. Moreover, the PPMI
10See Bouma (2009) for a mathematical delineation. Jurafsky and Martin (2015) also provide an intuitive description.
11In our experiments, applying SVD truncation to models results in negligible improvements between 0.01 and 0.001.
transformation is much faster, compared to the one performed on count-based models, due to the low dimensionality of vectors in the PoP-constructed model. Therefore, the PoP method makes it possible to benefit both from the high efficiency of randomized techniques as well as from the high accuracy of PPMI transformation in semantic similarity tasks.
If we put aside the information-theoretic interpretation of PPMI weighting (i.e., distilling statistical information that really matters), the logarithmic transformation of probabilities in the PPMI definition plays the role of a power transformation process for converting long-tailed distributions in count-based models to Gaussian-like distributions in predictive models. From a statistical perspective, any variation of PMI transformation can be seen as an attempt to stabilize variance of vector coordinates and therefore to make the observations more similar/fit to Gaussian distribution (a practice with long history in many research, particularly in biological and psychological sciences).
To exemplify this phenomenon, in Figure 3, we show histograms of the distributions of the assigned weights to the vector that represents the lemmatized form of the verb ‘abandon’ in various models. As shown, the raw collected frequencies in the count-based model have a long tail distribution (see Figure 3a). Applying the log transformation to this vector yields a vector of weights with a Gaussian distribution (Figure 3b). Weights in the RI-constructed vector (Figure 3c) have a perfect Gaussian distribution but with expected value of 0 (i.e., N (0, 1)). The PoP method, however, largely preserves the long tail distribution of coordinates from the original space (Figure 3d), which in turn can be weighted using PPMI and thereby transformed into a Gaussian-like distribution.
Given that models after PPMI transformation have bell-shaped Gaussian distributions, we expect that a correlation measure such as Pearson’s r, which takes advantage of the prior knowledge about the distribution of data, outperforms the non-parametric Kendall’s τb for computing similarities in PPMI-transformed spaces.12 This is indeed the case (see Figure 2).
12Note that using correlation measures such as Pearson’s r and Kendall’s τb in count-based model may excel measures such as cosine. However, their application is limited due to the high-dimensionality of count-based methods.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
100 10,000 1
10
100
1,000
log(Raw Frequencies)
log(Frequency)
(a) Count-based
0 50
100
200
300
400
PMI Weights
Frequency
(b) PMI
−1,700 0 1,700
20
40
RI Weights
Frequency
(c) RI
10 1,000 100,000
10
log(POP Weights)
log(Frequency)
(d) POP
−50 0 50 100
100
200
300
400
POP PMI Weights
Frequency
(e) POP+PMI
Figure 3: A histogram of the distribution of frequencies of weights in various models from 1+1 contextwindows for the lemmatized form of the verb ‘abandon’ in the UKWaC corpus.
10 0 50 0 10 00 15 00 20 00
30 00
40 00
50 00
0.5
0.55
0.6
0.7
0.75
Models’ Dimensionality (m)
Sp ea
rm an
’s C
or re
la tio
n ρ
PoP+PPMI+Pearson PoP+PPMI+ Kendall
PoP+Kendall
Figure 4: Changes in PoP’s performance when the dimensionality of models increases. The average performance in each set-up is shown by marked lines. The margins around these lines show the minimum and maximum performance observed in 10 independent executions.As discussed in § 2.3, PoP is a randomized algorithm and its performance is influenced by a number of parameters. In this section, we study the PoP method’s behaviour by reporting its performance in the MEN relatedness test under different parameter settings. To keep evaluations and reports in a manageable size, we focus on models built using context-windows of size 4+4.
Figure 4 shows the method’s performance when the dimensionm of the projected index vectors increases. In these experiments, index vectors are built using 4 non-zero elements; thus, as m increases, s in Equation 2 decreases. For each m, 100 ≤ m ≤ 5000, the models are built 10 times and the average as well as the maximum and the minimum observed performances in these experiments are reported. For PPMI transformed PoP spaces, with increasing dimensions, the performance boosts and, furthermore, the variance in performance (i.e., the shaded areas)13 gets smaller.
13Evidently, the probability of worst and best performances can be inferred from the reported average results.
1 2 4 8 16 32 0.5
0.55
0.6
0.7
0.75
1
Number of Non-Zero Elements in Index Vectors
Sp ea
rm an
’s C
or re
la tio
n ρ
PoP+PPMI+Pearson PoP+PPMI+ Kendall
PoP+Kendall
Figure 5: Changes in PoP’s performances when the dimensionality of models are fixed to m = 3000 and the number of non-zero elements in index vectors (i.e., s) increases. The average performances in each set-up are shown by marked lines. The margins around these lines show the minimum and maximum performance observed in 10 independent executions.
However, for the count-based PoP method without PPMI transformation (shown by dash-dotted lines) and with the number of non-zero elements fixed to 4, increasing m over 2000 decreases the performance. This is unexpected since an increase in dimensionality is usually assumed to entail an increase in performance. This behaviour, however, can be the result of using a very small s; simply put, the number of non-zero elements are not sufficient to build projected spaces with adequate distribution. To investigate this matter, we study the performance of the method with the dimension m fixed to 3000 but with index vectors built using different numbers of non-zero elements, i.e., different values of s.
Figure 5 shows the observed performances. For PPMI-weighted spaces, increasing the number of non-zero elements clearly deteriorates the performance. For unweighted PoP models, an increase in s up to the limit that does not result in nonorthogonal index vectors enhances performances. As shown in Figure 6, when the dimensionality
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
4 6 8 10 12 14 16 18 20 0
0.2
0.4
0.6
0.8
1
non-zero elements
P 6⊥
#index vectors n = 104
1 4
·104n
#non-zero elements = 8
m = 100 m = 1000 m = 2000
Figure 6: Proportion of non-orthogonal pairs of index vectors (i.e., P6⊥) obtained in a simulation for various dimensionality and number of non-zero elements. The left figure shows the changes of P 6⊥ For a fixed set of index vectors n = 104 when number of non-zero elements increases. The right figure shows P6⊥ when the number of non-zero elements is fixed to 8 but the number of index vectors n increases. As shown, P 6⊥ is determined by the number of non-zero elements and dimensionality of index vectors and independently of n.
of the index vectors is fixed and s increases, the chances of having non-orthogonal vectors in index vectors boosts. Hence, the chances of distortions in similarities increase. These distortions can enhance the result if they are controlled (e.g., using a training procedure such as the one used in neural net embedding). However, when left to chance, they can often lower the performance. Evidently, this is a simplified justification: in fact s plays the role of a switch that controls resemblance between distribution of data in the original space and the projected/transformed spaces. It seems that the sparsity of vectors in the original matrix plays a role for finding the optimal value for s. If PoP-constructed models are used directly (together with τb) for computing similarities, then we propose 0.002 < s. If PoP-constructed models are subject to an additional weighting process for stabilising vector distributions into Gaussianlike distributions such as PPMI, we propose using only 1 or 2 non-zero elements.
Last but not least, we confirm that by carefully selecting context elements (i.e., removing stop words and using lower and upper bound frequency cut-offs for context selection) and fine tuning PoP+PPMI+Pearson (i.e., increasing the dimension of models and scaling PMI weights as in Levy et al. (2015)) we achieve an even higher score in the MEN test (i.e., an average of 0.78 with the max of 0.787). Moreover, although improvements from applying SVD truncation are negligible, we can employ it for reducing the dimensionality of PoP vectors (e.g., from 6000 to 200).If the paper accepted, codes and resulting embeddings from experiments will be shared alongside the camera ready version.",,"The development of data-driven methods of natural language processing starts with an educated guess, a distributional hypothesis: We assume that some properties of linguistic entities can be modelled by ‘some statistical’ observations in language data. In the second step, this statistical information (which is determined by the hypothesis) is collected and represented in a mathematical framework. In the third step, tools provided by the chosen mathematical framework are used to implement a similarity-based logic to identify linguistic structures, and/or to verify the proposed
hypothesis. Harris’s distributional hypothesis is a well-known example of step one that states that meanings of words correlate with the context in which the words appear. Vector space models and η-normed-based similarity measures are renowned examples for steps two and three, respectively (i.e., word space models or word embeddings).
However, as pointed out for instance by Baroni et al. (2014), the count-based models resulting from the steps two and three are not discriminative enough to achieve satisfactory results; instead, predictive models are required. To this end, an additional transformation step is often added. Turney and Pantel (2010) describe this additional step as a combination of weighting and dimensionality reduction.1 This transformation from count-based to predictive models can be implemented simply via a collection of rules of thumb (i.e., heuristics), and/or it can involve more sophisticated mathematical transformations, such as converting raw counts to probabilities and using matrix factorization techniques. Likewise, by exploiting large amounts of computational power available nowadays, this transformation can be achieved via neural word embedding techniques (Mikolov et al., 2013; Levy and Goldberg, 2014).
To a large extend, the need for such transformations arises from the heavy-tailed distributions that we often find in statistical natural language models (such as the Zipfian distribution of words in contexts when building word spaces). Consequently, count-based models are sparse and highdimensional and therefore both computationally expensive to manipulate (due of the high dimensionality of models) and nondiscriminatory (due to the combination of the high-dimensionality of the models and the sparseness of observations, see Minsky and Papert (1969, chap. 12)).
1Similar to topics of feature weighting, selection, and engineering in statistical machine learning.
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
On the one hand, although neural networks are the top performers for addressing this problem, their usage is highly costly: they need to be trained that is often very time-consuming,2 and their performance can vary from one task to another depending on their objective function.3 On the other hand, although methods based on random projections efficiently address the problem of reducing the dimensionality of vectors—such as RI (Kanerva et al., 2000), reflective random indexing (RRI), (Cohen et al., 2010), ISA (Baroni et al., 2007) and random Manhattan indexing (RMI) (Zadeh and Handschuh, 2014)—in effect they retain distances between entities in the original space.4 In addition, since these methods use asymptotic Gaussian or Cauchy random projection matrices R with E(R) = 0, their resulting vectors cannot be adjusted and transformed using weighting techniques such as PPMI. Hence these methods do not outperform neural embeddings and combinations of PPMI weighting of count-baseds model followed by matrix factorization—such as truncation of weighted-vectors using singular value decomposition (SVD).
In order to overcome these problems, we propose a new method called positive-only projection (PoP). PoP is an incremental semantic space construction method that is similar to RI in the sense that it employs random projections. Hence, the construction of models using PoP does not require prior computation of embeddings but simply generating random vectors. However, in contrast to RI and similar methods, the PoP-constructed spaces can undergo weighting transformations such as PPMI. This is due to the fact that PoP uses random vectors that contain only positive integer values. Since the method is based on random projections, models can be built incrementally and efficiently. Since the vectors in PoP-constructed models are small (i.e., dimensionality of a few hundred), applying weighting methods such as PPMI to these models is incredibly faster than applying them to classical count-based models. Combined with a suitable weighting method such PPMI, the PoP method yields competitive results concern-
2Baroni et al. (2014) state that it took Ronan Collobert two months to train a set of embeddings from a Wikipedia dump. Even using GPU-accelerated computing, the required computation and training time for inducing neural word embeddings is high.
3Ibid, see results reported in supplemental materials. 4For η-normed space that they are designed for, i.e., η = 2
for RI, RRI, and ISA and η = 1 for RMI.
ing accuracy in semantic similarity assessment, compared for instance to neural net-based approaches and combinations of count-based models with weighting and matrix factorization. These results, however, are achieved without the need for heavy computations. Thus, instead of hours, days or months, models can be built in a matter of a few seconds or minutes. Note that even without weighting transformation, PoP-constructed models display a better performance than RI on tasks of semantic similarity assessments.
We describe the PoP method in § 2. In order to evaluate our models, in § 3, we report the performance of the PoP method in the MEN relatedness test. Finally, § 4 concludes with a discussion.",,"We introduced a new technique called PoP for incremental construction of semantic spaces. PoP can be seen as a dimensionality reduction method, which is based on a newly devised random projection matrix R of E(R). The major benefit of PoP is that it transfers vectors onto spaces of lower dimensionality without changing their distribution to a Gaussian shape with E = 0. Transformed spaces obtained using PoP can therefore be manipulated similarly to count-based models, only much faster and consequently requiring a considerably lower amount of computational resources.
PPMI weighting can be easily applied to POP-constructed models. In our experiments, we observe that PoP+PPMI+Pearson can be used to build models that achieve a high performance in semantic relatedness tests. More concretely, for index vector dimensions m ≥ 3000, PoP+PPMI+Pearson achieves an average score of 0.75 in the MEN relatedness test, which is comparable to many neural embedding techniques (see scores reported in Chen and de Melo (2015) and Tsvetkov et al. (2015)). However, in contrast to these approaches, PoP+PPMI+Pearson achieves this competitive performance without the need for time-consuming training of neural nets. Moreover, the involved processes are all done on vectors of low dimensionality. Hence, the PoP method can dramatically enhance the performance of a number of distributional natural language analyses.
This research can be extended in several ways. Firstly, theoretical accounts for PoP still need to be investigated. Secondly, many new methods can be designed by combining the proposed PoP and serialization of various weighting techniques (such as previously done for RI). Moreover, since weighting processes can be applied at a very low dimensionality, many computationallyintense techniques which cannot be applied to high-dimensional models for stabilising distributions of vectors (e.g., the use of power transforms such as BoxCox that maximizes the correlation coefficient of a Gaussian distribution of the weighted vectors) are now available to be used. On a similar basis, in the process of developing neural embeddings, PoP-constructed models can replace the high-dimensional count-based models as an input. Lastly, an important avenue of research is the use of so-called derandomization techniques to make PoP models linguistically more informed.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899",
3,"Cross lingual projection of linguistic annotation suffers from many sources of bias and noise, leading to unreliable annotations that cannot be used directly. In this paper, we introduce a novel approach to sequence tagging that learns to correct the errors from cross-lingual projection using an explicit debiasing layer. This is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags. We evaluated with only 1,000 tokens tagged with gold standard tags, along with more plentiful parallel data. Our system equals or exceeds the state-of-the-art on eight simulated low-resource settings, as well as two real low-resource languages, Malagasy and Kinyarwanda.",Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection,12,"I reviewed this paper earlier, when it was an ACL 2016 short paper draft. At
that point, it had a flaw in the experiment setup, which is now corrected.

Since back then I suggested I'd be willing to accept the draft for another *ACL
event provided that the flaw is corrected, I now see no obstacles in doing so.

Another reviewer did point out that the setup of the paper is somewhat
artificial if we focus on real low-resource languages, relating to the costs of
*finding* vs. *paying* the annotators. I believe this should be exposed in the
writeup not to oversell the method.

There are relevant lines of work in annotation projection for extremely
low-resource languages, e.g., Johannsen et al. (2016, ACL) and Agic et al.
(2015, ACL). It would be nice to reflect on those in the related work
discussion for completeness.

In summary, I think this is a nice contribution, and I vote accept.

It should be indicated whether the data is made available. I evaluate those
parts in good faith now, presuming public availability of research.",,4,3,Poster,4,4,4,4,4,5,3,3,2016,"In this work, we consider the POS tagging problem for a low-resource language using both the gold annotated data and distant projected data. For a low-resource language, we assume two sets of data. First, there is a small conventional corpus for the low-resource language, annotated with gold tags. Second, there is also a parallel corpus between the language and English, where we can reliably tag the English side and project these annotations across the word alignments. Then based on the annotated and the projected data, we learn a deep neural model for the POS tagging. The goal of learning here is to improve the POS tagging accuracy on the low resource language.Parallel data is often available for low-resource languages. For example, for Malagasy we can obtain bilingual documents with English directly from the web. This provides ample opportunity for projecting annotations from English into the lowresource language. Although the POS tags can be projected, given sentence and word-alignments, direct projection has several issues and results in very noisy and unreliable annotations (Yarowsky et al., 2001; Duong et al., 2014b). One source of error are the word alignments. These errors arise from words in the source language that are not aligned any words in the other language, which might be due to their not being translated closely, errors in alignments, or translation phenomena that do not fit the assumptions underlying the word based alignment models (e.g., many to many translations cannot be captured).
An example of POS projection via word alignments between Malagasy and English is shown in Figure 1. A word in Malagasy is connected to a word in English or NULL word. Thus there exist words in the target language which are not aligned a word from source, for example ny in Figure 1. Previous works either used the majority projected POS tag for a token or used a default value to represent the token (Duong et al., 2014a; Täckström et al., 2013). Another problem is about noisy projected tags. For example, in this sentence, fanomezan-kevitra is labelled as VERB incorrectly, but should be NOUN, a consequence of a non-literal translation.
We now turn to the manner of labelling the projected data. For the parallel data, we consider each token in the low-resource language. Where this token is aligned to a single token in English, we assign the tag for that English token. For tokens that are aligned to many English words or none at all (NULL), we assign a distribution over tags according to the tag frequency distribution over the whole English sentence.
A natural question is whether this projected labelling might be suitable for use directly in supervised learning of a POS tagger. To test this, we compare training a bidirectional Long ShortTerm Memory (BiLSTM) tagger on this data, a small 1000 token dataset with gold-standard tags, and the union of the two.1 Evaluating the tag-
1See §3.2 for the model details, and §4.1 for a description
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
ging accuracy against gold standard tags, we observe in Tables 1 and 2 (top section, rows labelled BiLSTM) that the use of the gold-standard (Annotated) data is considerably superior to training on the directly Projected data, despite the smaller amount of Annotated data, while using the union of the two datasets does result in mild improvements in a few languages, but worsens performance for others.
These sobering results raise the question of how we might use the bilingual resources in a more effectively manner than direct projection. Clearly projections contain useful information, as the tagging accuracy is well above chance, however they are riddled with noise and biases, which needs to be accounted for for adequate performance.To address this problem, we propose a model based on jointly modelling the clean annotated data and the noisy projected data. For this we use a bidirectional LSTM tagger, as illustrated on the left in Figure 2, although other classifiers could be easily used in its place. The BiLSTM offers access to both the left and right lexical contexts around a given word (Graves et al., 2013), which are likely be of considerable use in POS tagging where context of central importance.
Let xt indicate a word in a sentence and yt indicate its corresponding POS tag, where K is the size of the tagset.2 The recurrent layer is designed to store contextual information, while the values in the hidden and output layers are computed as follows:
−→ h t = lstm( −→ h t−1, xt) ←− h t = lstm( ←− h t+1, xt)
ot = softmax(W→ −→ h t +W← ←− h t + b) (1) yt ∼ Multinomial(ot) .
This supervised model is trained on annotated gold data in the standard manner using a cross-entropy objective with stochastic gradient descent through the use of gradient backpropagation.
The projected data, however, needs to be treated differently to the annotated data: the tagging is often uncertain, as tokens may have been aligned to words with different parts of speech, be multiply
of the datasets and evaluation. 2We use the universal tagset from (Petrov et al., 2011), enabling comparison across languages.
aligned or left unaligned. These tags are not to be trusted in the same way as the gold annotated data. Our work allows for noise explicitly in the training objective, by attempting to model the noise generating process. The projected data consists of pairs, (xt, ỹ), where ỹ denotes the projected POS tag. In this setting, we assume that the true label, yt, is latent variable and both ỹ and y are K-dimensional binary random variables: ỹt is a vector representation of a projected tag, and yt is a one-hot representation of a gold tag.
We augment the deep neural network model to include a noise transformation such that its prediction matches the POS tag distribution of the noisy data, as follows:
p(Ỹt = j|xt, θ, A) = softmax (∑ i ai,jot,i ) ,
(2) where ot,i = p(Yt = i|xt, θ) is the probability of tag i in position t according to (1). This equation is parameterized by a K ×K matrix A.3 Each cell ai,j denotes the confusion score between classes i and j, with negative values quashing the correspondance, and positive values rewarding a pairing; in the situations where the projected tags closely match the supervised tagging, we expect that A ∝ I .
Joint modelling of the gold supervision and projected data gives rise to a training objective combining two cross-entropy terms,
L(θ,A) =− 1 |T p| ∑ t∈T p 〈ỹt, log softmax (Aot)〉
− 1 |T t| ∑ t∈T t 〈yt, log ot〉 ,
where T p indexes all the token positions in the projected dataset, and similarly for T t over the annotated training set.
We illustrate the combined model in Figure 2, showing on the left the gold supervised model and on the right the distant supervised components. The distant model builds on the base part, through feeding the output through a noising layer, which is finally used in a softmax to produce the noised output layer. The matrix A parameterizes the final layer, to adjust the tag probabilities from
3Our approach also supports mismatching tagsets, in which case A would be rectangular with dimensions based on the sizes of the two tag sets.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
raha ny marina
embedding, e
text, x
BiLSTM, h
h
output, o
label, y Conj Det Noun
tsara fa
...
noised output, õ
projected labels, ỹ
Annotated data
Projected data
Shared layers
misaotra
Adj
Figure 2: Illustration of the model architecture, using a bidirectional LSTM recurrent network, with a tag classification output. The left part illustrates the supervised training scenario and test setting, where each word x is assigned a tag y; the right part shows the projection training setting, with a noise layer, where the supervision is either a projected label or label distribution (used for NULL aligned words).
the supervised model into a distribution that better matches the noisy projected POS tags. However, the ultimate goal is to predict the POS tag yt. Consider the training effect of the projected POS tags: when performing error backpropagation, the cross-entropy error signal must pass through the tag transformation linking õ with o, which can be seen as a de-noising step, after which the cleaned error signal can be further backpropagated to the rest of the model. Provided there are consistent patterns of noise in the projection output, this technique can readily model these sources of variation, with a tiny handful of parameters, and thus greatly improve the utility of this form of distant supervision.
Directly training the whole deep neural network with random initialization is impractical, because without a good estimate for theAmatrix, the noise from the projected tags may misdirect training result in a poor local optima. For this reason the training process contains two stages. On the first stage, we use the clean annotated data to pretrain the network. On the second stage, we jointly use both projected and annotated data to continue training the model.We evaluate our algorithm on two kinds of experiment settings, simulation experiments and realworld experiments. For the simulation exper-
iments, we use the following 8 European languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv). These eight languages are obviously not low-resource languages, however we can use this data to simulate the low-resource setting by only using a small fraction of the gold annotations for training. This evaluation technique is widely used in previous work, and allows us to compare our results with prior stateof-the-art algorithms. For the real-world experiments, we use the following two low-resource languages: Malagasy, an Austronesian language spoken in Madagascar, and Kinyarwanda, a NigerCongo language spoken in Rwanda.For the simulation experiments, we use the Europarl v7 corpus, with English as the source language and each of eight languages as the target language. There are an average 1.85 million parallel sentences for each of the language pairs. For the real-world experiments, parallel data is smaller and generally of a lower quality. For Malagasy, we use a web-sourced collection of parallel texts.4 The parallel data of Malagasy has 100k sentences and 1,231k tokens. For Kinyarwanda, we obtained
4http://www.cs.cmu.edu/ ark/global-voices/
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
da nl de el it pt es sv Average BiLSTM Annotated 89.3 87.4 89.5 88.1 85.9 89.5 90.6 84.7 88.1 BiLSTM Projected 64.4 81.9 81.3 78.9 80.1 81.9 81.2 74.9 78.0 BiLSTM Ann+Proj 85.4 88.9 90.2 84.2 86.1 88.2 91.3 83.6 87.2 MaxEnt Supervised 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 86.9 Duong et al. 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3 BiLSTM+noise layer 92.3 91.7 92.5 92.8 90.2 92.9 92.4 89.1 91.7
Table 1: The POS tagging accuracy for various models in 8 languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) Swedish (sv). The top results of the second part are taken from (Duong et al., 2014a), evaluated on the same data split.
parallel texts from ARL MURI project.5 The parallel data of Kinyarwanda has 11k sentences and 52k tokens.We use Giza++ to induce word alignments on the parallel data (Och and Ney, 2003), using IBM model 3. Following prior work (Duong et al., 2014b), we retain only one-to-one alignments. Using all alignments, i.e., many-to-one and one-tomany, would result in many more POS-tagged tokens, but also bring considerable additional noise. For example, the English laws (NNS) aligned to French les (DT) lois (NNS) would end up incorrectly tagging the French determiner les as a noun (NNS). We use the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the label to the target side. As we show in the following section, and as confirmed in many studies (Täckström et al., 2013; Das and Petrov, 2011) the directly projected labels are very noisy and it is unwise to use the tags directly. We further filter the noise using the approach of Yarowsky et al. (2001) which selects sentences with the highest sentence alignment scores from IBM model 3. For 8 languages in Europarl corpus, we collect 200k sentences for each language. For the low-resource languages, we use the whole parallel data because of limited bilingual text.Gold annotated data is expensive and difficult to obtain, and thus we assume that only a small annotated dataset is available. For the simulation experiments, we use the CoNLL data (Buchholz and Marsi, 2006) as annotated data for eight languages. To simulate the low-resource setting, we
5The dataset was provided directly by Noah Smith.
take the first 1,000 tagged tokens for training and the remaining data is split equally between development and testing sets, following Duong et al. (2014a). For the real-world experiments, we use the Malagasy and Kinyarwanda data from Garrette and Baldridge (2013), who showed that a small annotated dataset could be collected very cheaply, requiring less than 2 hours of non-expert time to tag 1000 tokens. This constitutes a reasonable demand for cheap portability to other low-resource languages. We use the datasets from Garrette and Baldridge (2013), constituting training sets of 383 sentences and 5,294 tokens in Malagasy and 196 sentences and 4,882 tokens for Kinyarwanda. There are similar sized datasets used for testing.We compare our algorithm with several baselines, including the state-of-the-art algorithm from Duong et al. (2014a), a two-output maxent model, their reported baseline method of a supervised maximum entropy model trained on the annotated data, and our BiLSTM POS tagger trained directly from the annotated and/or projected data (denoted BiLSTM Annotated, Projected and Ann+Proj for the model trained on union of the two datasets). For the real low-resource languages, we also compare our algorithm with Garrette et al. (Garrette and Baldridge, 2013), which showed good results on the two low-resource languages. Our implementation is based on clab/cnn. 6 In all cases, the BiLSTM models use 128 dimensional word embeddings and 128 dimensional hidden layers. We set learning rate as 1.0 and use stochastic gradient descent model to learn the parameters.
We evaluate all algorithms on the gold testing sets, evaluating in terms of tagging accuracy. Following standard practice in POS tagging, we re-
6https://github.com/clab/cnn
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
da nl de el
it pt es sv
VERB
NOUN PRON
ADJ ADV ADP CONJ
DET
.
NUM
PRT
X
VERB
NOUN PRON
ADJ ADV ADP CONJ
DET
.
NUM
PRT
X
V E R B N O U N P R O N A D J A D V A D P C O N J D E T . N U M P R T X V E R B N O U N P R O N A D J A D V A D P C O N J D E T . N U M P R T X V E R B N O U N P R O N A D J A D V A D P C O N J D E T . N U M P R T X V E R B N O U N P R O N A D J A D V A D P C O N J D E T . N U M P R T X
1.0
-1.0
1.0
-1.0
Figure 3: Noise transformation matrix A between POS tags and noised (projection) outputs, shown as columns and rows, respectively, for the 8 languages.
port results using per-token accuracy, i.e., the fraction of predicted tags that exactly match the gold standard tags. Note that for all our experiments, we work with the universal POS tags and accordingly accuracy is measured against the gold tags after automatic mapping into the universal tagset.","First, we present the results in the eight simulation languages in Table 1. For most of languages, our method is better than Duong et al. (2014a) and the three naive BiLSTM baselines. Directly training on projected data hurts the performance, e.g., compare BiLSTM Projected and BiLSTM Ann+Proj. BiLSTM Annotated mostly outperforms MaxEnt Supervised, but both methods are worse than Duong et al. and our BiLSTM+noise layer, which both use the projected data more effectively. The result shows the noise layer provides a better use of the noisy projected data, improving the POS tagging accuracy.
We show the noise layer for the different languages in Figures 3. The blue (dark) cells in the grids denote values that are most highly weighted. Note the strong diagonal, showing that the tags are mostly trusted, although there is also evidence of considerable noise. The worst case is in Swedish (sv) with many weak values on the diagonal. In
this case, PRT and X appear to be confused for one another. The white grids are also important, showing tag combinations that the model learns to ignore, such as NUM vs DET in Italian (it) and NOUN vs PRON in Spanish (es) and Swedish (sv). It shows these types are not confused. The tokens that are NUM in Italian (it) are seldom projected as DET. Overall, the level of noise looks to be modest, which might not come as a surprise given the large clean parallel corpus for learning word alignments.
Now we present the results for two low-resource languages, Malagasy and Kinyarwanda which both have much smaller parallel corpora. The results in Table 2 show that our method works better than all others in both languages, with a similar pattern of results as for the eight simulation languages. Note that our method outperforms the state of the art on both languages (Duong et al., 2014a; Garrette and Baldridge, 2013).
To better understand the effect of the noise layer, we present the learned transformation matrices A in Figure 4. Note the strong diagonal for Malagasy in Figure 4, showing that each tag is most likely to map to itself, however there are also many high magnitude off diagonal elements. For instance nouns map to not just noun, but also adjective and number, but never pronoun (which
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Malagasy Kinyarwanda 1.0
-1.0
Figure 4: Noise transformation matrix A between POS tags and noised (projection) outputs, shown as columns and rows, respectively, for the two low-resource languages.
Model Accuracy Malagasy Kinyarwanda BiLSTM Annotated 81.5 76.9 BiLSTM Projected 67.2 61.9 BiLSTM Ann+Proj 78.6 73.2 MaxEnt Supervised 80.0 76.4 Duong et al. 85.3 78.3 BiLSTM+noise layer 86.3 82.5 Garrette et al. 81.2 81.9
Table 2: The POS tagging accuracy for various models in Malagasy and Kinyarwanda. The top results of the second part are taken from (Duong et al., 2014a), evaluated on the same data split.
are presumably well aligned.) Comparing results of Malagasy and Kinyarwanda in Figure 4, we can see the amount of noise is much greater in Kinyarwanda. This tallies with the performance results, in which we get stronger results and a greater improvement on Malagasy from using projection data, where we had more parallel data.","Part-of-speech tagging is a critical task for natural language processing (NLP) applications, providing lexical syntactic information. Automatic POS tagging has been wildly successful on many rich resource languages using supervised learning over large training corpora (McCallum et al., 2000; Lafferty et al., 2001; Ammar et al., 2016). However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (Garrette and Baldridge, 2013; Duong et al., 2014a). For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (Mintz et al., 2009; Takamatsu et al., 2012; Ritter et al., 2013; Plank et al., 2014). A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy “distant” data.
A popular method for distant supervision is to use parallel data between a low-resource language and a rich-resource language. Although annotated data in low-resource languages are difficult to obtain, bilingual resources are more plentiful. For example parallel translations into English are often available, in the form of news reports, novels or the Bible. Parallel data allows annotation from the high-resource language to be projected across alignments to the low-resource language, which has been shown to be effective for several language processing tasks including POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity recognition (Wang and Manning, 2013) and dependency parsing (McDonald et al., 2013).
Although cross-lingual POS projection is popular it has several problems, including noise from poor word alignments (Täckström et al., 2013; Das and Petrov, 2011) and cross-lingual syntactic divergence (Duong et al., 2013). Previous work has proposed heuristics or constraints to clean the projected tag before or during learning. In contrast, we consider compensating for these problems explicitly, by learning a noise transformation to encode the mapping between ‘clean’ tags and the kinds of noisy tags produced from projection.
We propose a new neural network model for sequence tagging in a low-resource language, suitable for training with both a tiny gold standard annotated corpus, as well as distant supervision using cross lingual tag projection. Our model uses a bidirectional Long Short-Term Memory (BiLSTM), which produces two types of output: gold tags generated directly from the hidden state of neural network, and uncertain projected tags generated after applying a further linear transformation. This transformation, which we refer to as output noising encodes the mapping between the projected high-resource tags and low-resource
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
tags, and learning when and how much to trust the projected data. For example, for languages without determiners, the model can learn to map projected determiner tags to nouns, or if verbs are often poorly aligned, the model can learn to effectively ignore the projected verb tag, through associating all tags with verbs. Our model is trained jointly on gold and distant projected annotations, and can be trained end-to-end with backpropagation.
Our approach captures the relations among tokens, noisy projected POS tags and ground truth POS tags. Our work differs in the use of projection, in that we explicitly model the transformation between tagsets as part of a more expressive deep learning neural network. Our contributions fall in three aspects. First, we study the noise of projected data in word alignments and describe it with an additional layer model. Second, we integrate the model into a deep neural network and jointly train the model on both annotated and projected data to make the model learn from better supervisions. Finally, evaluating on eight simulated and two real-world low-resource languages, experimental results demonstrate that our approach uniformly equals or exceeds existing methods on simulated languages, and achieves 86.3% accuracy for Malagasy and 82.5% on Kinyarwanda, exceeding the state-of-the-art results (Duong et al., 2014a).","For most natural language processing tasks, the conventional approach to developing a system is to use supervised learning algorithms trained on a set of annotated data. However, this approach is inappropriate to low-resource languages due to the lack of annotated data. An alternative approach is to harness different source of information aside from simple annotated text. Knowledgebases such as dictionaries are one possible source of information, which can be used to to inform or constrain models, such as limiting the search space for POS tagging (Banko and Moore, 2004; Goldberg et al., 2008; Li et al., 2012).
Parallel bilingual corpora provide another important source of information, and are often plentiful even for many low-resource languages in the form of multilingual government documents, book translations, multilingual websites, etc. Word alignments can provide a bridge to project in-
formation from a resource-rich source language to a resource-poor target language. For example, parallel data has been used for named entity recognition (Wang and Manning, 2013) based on the observation that named entities are most often preserved in translation; and also in syntactic tasks such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013) and dependency parsing (McDonald et al., 2013). Clues from related languages can also compensate for the lack of annotated data, as we expect there to be information shared between closely related languages in terms of the lexical items, morphology and syntactic structure. Some successful applications using language relatedness information are dependency parsing (McDonald et al., 2011), where a parser is estimated from a source, resource-rich language, but then applied to a target, low-resource language, and POS tagging (Hana et al., 2004) where parts of the tagger are estimated from the source language. However, these approaches are limited to closely related languages such as Czech and Russian, or Telugu and Kannada, and it is unclear whether these techniques will work well in situations where parallel data only exists for less-related languages, as is often the case in practice.
To summarize, for all these mentioned tasks, lexical resources are valuable sources of knowledge, but are also costly to build. Language relatedness information is applicable for closely related languages, but it is often the case that a given low-resource language does not have a closely-related, resource-rich language. Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems for low-resource languages (Yarowsky et al., 2001; Duong et al., 2014b; Guo et al., 2016; Guo et al., 2015), and here we primarily investigate methods to exploit parallel texts.
Yarowsky et al. (2001) pioneered the use of parallel data for projecting POS tag information from a resource-rich language to a resource-poor language. Duong et al. (2014b) proposed an approach using a maximum entropy classifier trained on 1000 tagged tokens, and used projected tags as auxiliary outputs. Das and Petrov (2011) used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens. Our work is closest to Duong et al. (2014a), and we share the same evaluation setting, which we
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
believe is well suited to the low-resource applications. Our approach differs from theirs in two ways: first we propose a deep learning model based on a long short term memory recurrent structure versus their maximum entropy classifier, and secondly we model the projection tag explicitly as a form of noise applied after classification, while they attempt to capture the correlations between tagsets only implicitly through a joint feature set over both tags. We believe that our work is the first to explicitly model the noise affecting cross-lingual projected annotations, and thereby allowing this rich data resource to be better exploited in learning NLP models in low-resource languages.",,"In this paper, we presented a technique for exploiting noisy cross-lingual projected annotations alongside a small amount of annotation data, in the context of POS tagging. To utilize both sources of data, we proposed a new model based on a bidirectional long short term memory recurrent neural network, with a layer for explicitly handling noisy projection labels. In two real low-resource languages, our methods outperform other algorithms. Our technique is general, and is likely to prove
useful for exploiting other noisy annotations such as distant supervision and crowd-sources annotations, and with other modelling approaches."
4,"Cross lingual projection of linguistic annotation suffers from many sources of bias and noise, leading to unreliable annotations that cannot be used directly. In this paper, we introduce a novel approach to sequence tagging that learns to correct the errors from cross-lingual projection using an explicit debiasing layer. This is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags. We evaluated with only 1,000 tokens tagged with gold standard tags, along with more plentiful parallel data. Our system equals or exceeds the state-of-the-art on eight simulated low-resource settings, as well as two real low-resource languages, Malagasy and Kinyarwanda.",Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection,12,"The paper describes a modification to the output layer of recurrent neural
network models which enables learning the model parameters from both gold and
projected annotations in a low-resource language. The traditional softmax
output layer which defines a distribution over possible labels is further
multiplied by a fully connected layer which models the noise generation
process, resulting in another output layer representing the distribution over
noisy labels. 

Overall, this is a strong submission. The proposed method is apt, simple and
elegant. The paper reports good results on POS tagging for eight simulated
low-resource languages and two truly low-resource languages, making use of a
small set of gold annotations and a large set of cross-lingually projected
annotations for training. The method is modular enough that researchers working
on different NLP problems in low-resource scenarios are likely to use it.

From a practical standpoint, the experimental setup is unusual. While I can
think of some circumstances where one needs to build a POS tagger with as
little as 1000 token annotations (e.g., evaluations in some DARPA-sponsored
research projects), it is fairly rare. A better empirical validation of the
proposed method would have been to plot the tagging accuracy of the proposed
method (and baselines) while varying the size of gold annotations. This plot
would help answer questions such as: Does it hurt the performance on a target
language if we use this method while having plenty of gold annotations? What is
the amount of gold annotations, approximately, below which this method is
beneficial? Does the answer depend on the target language?

Beyond cross-lingual projections, noisy labels could potentially be obtained
from other sources (e.g., crowd sourcing) and in different tag sets than gold
annotations. Although the additional potential impact is exciting, the paper
only shows results with cross-lingual projections with the same tag set. 

It is surprising that the proposed training objective gives equal weights to
gold vs. noisy labels. Since the setup assumes the availability of a small gold
annotated corpus, it would have been informative to report whether it is
beneficial to tune the contribution of the two terms in the objective function.


In line 357, the paper describes the projected data as pairs of word tokens
(x_t) and their vector representations \tilde{y}, but does not explicitly
mention what the vector representation looks like (e.g., a distribution over
cross-lingually projected POS tags for this word type). A natural question to
ask here is whether the approach still works if we construct \tilde{y} using
the projected POS tags at the token level (rather than aggregating all
predictions for the same word type). Also, since only one-to-one word
alignments are preserved, it is not clear how to construct \tilde{y} for words
which are never aligned.

Line 267, replace one of the two closing brackets with an opening bracket.",,4,3,Oral Presentation,4,4,4,4,5,5,4,4,2016,"In this work, we consider the POS tagging problem for a low-resource language using both the gold annotated data and distant projected data. For a low-resource language, we assume two sets of data. First, there is a small conventional corpus for the low-resource language, annotated with gold tags. Second, there is also a parallel corpus between the language and English, where we can reliably tag the English side and project these annotations across the word alignments. Then based on the annotated and the projected data, we learn a deep neural model for the POS tagging. The goal of learning here is to improve the POS tagging accuracy on the low resource language.Parallel data is often available for low-resource languages. For example, for Malagasy we can obtain bilingual documents with English directly from the web. This provides ample opportunity for projecting annotations from English into the lowresource language. Although the POS tags can be projected, given sentence and word-alignments, direct projection has several issues and results in very noisy and unreliable annotations (Yarowsky et al., 2001; Duong et al., 2014b). One source of error are the word alignments. These errors arise from words in the source language that are not aligned any words in the other language, which might be due to their not being translated closely, errors in alignments, or translation phenomena that do not fit the assumptions underlying the word based alignment models (e.g., many to many translations cannot be captured).
An example of POS projection via word alignments between Malagasy and English is shown in Figure 1. A word in Malagasy is connected to a word in English or NULL word. Thus there exist words in the target language which are not aligned a word from source, for example ny in Figure 1. Previous works either used the majority projected POS tag for a token or used a default value to represent the token (Duong et al., 2014a; Täckström et al., 2013). Another problem is about noisy projected tags. For example, in this sentence, fanomezan-kevitra is labelled as VERB incorrectly, but should be NOUN, a consequence of a non-literal translation.
We now turn to the manner of labelling the projected data. For the parallel data, we consider each token in the low-resource language. Where this token is aligned to a single token in English, we assign the tag for that English token. For tokens that are aligned to many English words or none at all (NULL), we assign a distribution over tags according to the tag frequency distribution over the whole English sentence.
A natural question is whether this projected labelling might be suitable for use directly in supervised learning of a POS tagger. To test this, we compare training a bidirectional Long ShortTerm Memory (BiLSTM) tagger on this data, a small 1000 token dataset with gold-standard tags, and the union of the two.1 Evaluating the tag-
1See §3.2 for the model details, and §4.1 for a description
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
ging accuracy against gold standard tags, we observe in Tables 1 and 2 (top section, rows labelled BiLSTM) that the use of the gold-standard (Annotated) data is considerably superior to training on the directly Projected data, despite the smaller amount of Annotated data, while using the union of the two datasets does result in mild improvements in a few languages, but worsens performance for others.
These sobering results raise the question of how we might use the bilingual resources in a more effectively manner than direct projection. Clearly projections contain useful information, as the tagging accuracy is well above chance, however they are riddled with noise and biases, which needs to be accounted for for adequate performance.To address this problem, we propose a model based on jointly modelling the clean annotated data and the noisy projected data. For this we use a bidirectional LSTM tagger, as illustrated on the left in Figure 2, although other classifiers could be easily used in its place. The BiLSTM offers access to both the left and right lexical contexts around a given word (Graves et al., 2013), which are likely be of considerable use in POS tagging where context of central importance.
Let xt indicate a word in a sentence and yt indicate its corresponding POS tag, where K is the size of the tagset.2 The recurrent layer is designed to store contextual information, while the values in the hidden and output layers are computed as follows:
−→ h t = lstm( −→ h t−1, xt) ←− h t = lstm( ←− h t+1, xt)
ot = softmax(W→ −→ h t +W← ←− h t + b) (1) yt ∼ Multinomial(ot) .
This supervised model is trained on annotated gold data in the standard manner using a cross-entropy objective with stochastic gradient descent through the use of gradient backpropagation.
The projected data, however, needs to be treated differently to the annotated data: the tagging is often uncertain, as tokens may have been aligned to words with different parts of speech, be multiply
of the datasets and evaluation. 2We use the universal tagset from (Petrov et al., 2011), enabling comparison across languages.
aligned or left unaligned. These tags are not to be trusted in the same way as the gold annotated data. Our work allows for noise explicitly in the training objective, by attempting to model the noise generating process. The projected data consists of pairs, (xt, ỹ), where ỹ denotes the projected POS tag. In this setting, we assume that the true label, yt, is latent variable and both ỹ and y are K-dimensional binary random variables: ỹt is a vector representation of a projected tag, and yt is a one-hot representation of a gold tag.
We augment the deep neural network model to include a noise transformation such that its prediction matches the POS tag distribution of the noisy data, as follows:
p(Ỹt = j|xt, θ, A) = softmax (∑ i ai,jot,i ) ,
(2) where ot,i = p(Yt = i|xt, θ) is the probability of tag i in position t according to (1). This equation is parameterized by a K ×K matrix A.3 Each cell ai,j denotes the confusion score between classes i and j, with negative values quashing the correspondance, and positive values rewarding a pairing; in the situations where the projected tags closely match the supervised tagging, we expect that A ∝ I .
Joint modelling of the gold supervision and projected data gives rise to a training objective combining two cross-entropy terms,
L(θ,A) =− 1 |T p| ∑ t∈T p 〈ỹt, log softmax (Aot)〉
− 1 |T t| ∑ t∈T t 〈yt, log ot〉 ,
where T p indexes all the token positions in the projected dataset, and similarly for T t over the annotated training set.
We illustrate the combined model in Figure 2, showing on the left the gold supervised model and on the right the distant supervised components. The distant model builds on the base part, through feeding the output through a noising layer, which is finally used in a softmax to produce the noised output layer. The matrix A parameterizes the final layer, to adjust the tag probabilities from
3Our approach also supports mismatching tagsets, in which case A would be rectangular with dimensions based on the sizes of the two tag sets.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
raha ny marina
embedding, e
text, x
BiLSTM, h
h
output, o
label, y Conj Det Noun
tsara fa
...
noised output, õ
projected labels, ỹ
Annotated data
Projected data
Shared layers
misaotra
Adj
Figure 2: Illustration of the model architecture, using a bidirectional LSTM recurrent network, with a tag classification output. The left part illustrates the supervised training scenario and test setting, where each word x is assigned a tag y; the right part shows the projection training setting, with a noise layer, where the supervision is either a projected label or label distribution (used for NULL aligned words).
the supervised model into a distribution that better matches the noisy projected POS tags. However, the ultimate goal is to predict the POS tag yt. Consider the training effect of the projected POS tags: when performing error backpropagation, the cross-entropy error signal must pass through the tag transformation linking õ with o, which can be seen as a de-noising step, after which the cleaned error signal can be further backpropagated to the rest of the model. Provided there are consistent patterns of noise in the projection output, this technique can readily model these sources of variation, with a tiny handful of parameters, and thus greatly improve the utility of this form of distant supervision.
Directly training the whole deep neural network with random initialization is impractical, because without a good estimate for theAmatrix, the noise from the projected tags may misdirect training result in a poor local optima. For this reason the training process contains two stages. On the first stage, we use the clean annotated data to pretrain the network. On the second stage, we jointly use both projected and annotated data to continue training the model.We evaluate our algorithm on two kinds of experiment settings, simulation experiments and realworld experiments. For the simulation exper-
iments, we use the following 8 European languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv). These eight languages are obviously not low-resource languages, however we can use this data to simulate the low-resource setting by only using a small fraction of the gold annotations for training. This evaluation technique is widely used in previous work, and allows us to compare our results with prior stateof-the-art algorithms. For the real-world experiments, we use the following two low-resource languages: Malagasy, an Austronesian language spoken in Madagascar, and Kinyarwanda, a NigerCongo language spoken in Rwanda.For the simulation experiments, we use the Europarl v7 corpus, with English as the source language and each of eight languages as the target language. There are an average 1.85 million parallel sentences for each of the language pairs. For the real-world experiments, parallel data is smaller and generally of a lower quality. For Malagasy, we use a web-sourced collection of parallel texts.4 The parallel data of Malagasy has 100k sentences and 1,231k tokens. For Kinyarwanda, we obtained
4http://www.cs.cmu.edu/ ark/global-voices/
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
da nl de el it pt es sv Average BiLSTM Annotated 89.3 87.4 89.5 88.1 85.9 89.5 90.6 84.7 88.1 BiLSTM Projected 64.4 81.9 81.3 78.9 80.1 81.9 81.2 74.9 78.0 BiLSTM Ann+Proj 85.4 88.9 90.2 84.2 86.1 88.2 91.3 83.6 87.2 MaxEnt Supervised 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 86.9 Duong et al. 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3 BiLSTM+noise layer 92.3 91.7 92.5 92.8 90.2 92.9 92.4 89.1 91.7
Table 1: The POS tagging accuracy for various models in 8 languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) Swedish (sv). The top results of the second part are taken from (Duong et al., 2014a), evaluated on the same data split.
parallel texts from ARL MURI project.5 The parallel data of Kinyarwanda has 11k sentences and 52k tokens.We use Giza++ to induce word alignments on the parallel data (Och and Ney, 2003), using IBM model 3. Following prior work (Duong et al., 2014b), we retain only one-to-one alignments. Using all alignments, i.e., many-to-one and one-tomany, would result in many more POS-tagged tokens, but also bring considerable additional noise. For example, the English laws (NNS) aligned to French les (DT) lois (NNS) would end up incorrectly tagging the French determiner les as a noun (NNS). We use the Stanford POS tagger (Toutanova et al., 2003) to tag the English side of the parallel data and then project the label to the target side. As we show in the following section, and as confirmed in many studies (Täckström et al., 2013; Das and Petrov, 2011) the directly projected labels are very noisy and it is unwise to use the tags directly. We further filter the noise using the approach of Yarowsky et al. (2001) which selects sentences with the highest sentence alignment scores from IBM model 3. For 8 languages in Europarl corpus, we collect 200k sentences for each language. For the low-resource languages, we use the whole parallel data because of limited bilingual text.Gold annotated data is expensive and difficult to obtain, and thus we assume that only a small annotated dataset is available. For the simulation experiments, we use the CoNLL data (Buchholz and Marsi, 2006) as annotated data for eight languages. To simulate the low-resource setting, we
5The dataset was provided directly by Noah Smith.
take the first 1,000 tagged tokens for training and the remaining data is split equally between development and testing sets, following Duong et al. (2014a). For the real-world experiments, we use the Malagasy and Kinyarwanda data from Garrette and Baldridge (2013), who showed that a small annotated dataset could be collected very cheaply, requiring less than 2 hours of non-expert time to tag 1000 tokens. This constitutes a reasonable demand for cheap portability to other low-resource languages. We use the datasets from Garrette and Baldridge (2013), constituting training sets of 383 sentences and 5,294 tokens in Malagasy and 196 sentences and 4,882 tokens for Kinyarwanda. There are similar sized datasets used for testing.We compare our algorithm with several baselines, including the state-of-the-art algorithm from Duong et al. (2014a), a two-output maxent model, their reported baseline method of a supervised maximum entropy model trained on the annotated data, and our BiLSTM POS tagger trained directly from the annotated and/or projected data (denoted BiLSTM Annotated, Projected and Ann+Proj for the model trained on union of the two datasets). For the real low-resource languages, we also compare our algorithm with Garrette et al. (Garrette and Baldridge, 2013), which showed good results on the two low-resource languages. Our implementation is based on clab/cnn. 6 In all cases, the BiLSTM models use 128 dimensional word embeddings and 128 dimensional hidden layers. We set learning rate as 1.0 and use stochastic gradient descent model to learn the parameters.
We evaluate all algorithms on the gold testing sets, evaluating in terms of tagging accuracy. Following standard practice in POS tagging, we re-
6https://github.com/clab/cnn
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
da nl de el
it pt es sv
VERB
NOUN PRON
ADJ ADV ADP CONJ
DET
.
NUM
PRT
X
VERB
NOUN PRON
ADJ ADV ADP CONJ
DET
.
NUM
PRT
X
V E R B N O U N P R O N A D J A D V A D P C O N J D E T . N U M P R T X V E R B N O U N P R O N A D J A D V A D P C O N J D E T . N U M P R T X V E R B N O U N P R O N A D J A D V A D P C O N J D E T . N U M P R T X V E R B N O U N P R O N A D J A D V A D P C O N J D E T . N U M P R T X
1.0
-1.0
1.0
-1.0
Figure 3: Noise transformation matrix A between POS tags and noised (projection) outputs, shown as columns and rows, respectively, for the 8 languages.
port results using per-token accuracy, i.e., the fraction of predicted tags that exactly match the gold standard tags. Note that for all our experiments, we work with the universal POS tags and accordingly accuracy is measured against the gold tags after automatic mapping into the universal tagset.","First, we present the results in the eight simulation languages in Table 1. For most of languages, our method is better than Duong et al. (2014a) and the three naive BiLSTM baselines. Directly training on projected data hurts the performance, e.g., compare BiLSTM Projected and BiLSTM Ann+Proj. BiLSTM Annotated mostly outperforms MaxEnt Supervised, but both methods are worse than Duong et al. and our BiLSTM+noise layer, which both use the projected data more effectively. The result shows the noise layer provides a better use of the noisy projected data, improving the POS tagging accuracy.
We show the noise layer for the different languages in Figures 3. The blue (dark) cells in the grids denote values that are most highly weighted. Note the strong diagonal, showing that the tags are mostly trusted, although there is also evidence of considerable noise. The worst case is in Swedish (sv) with many weak values on the diagonal. In
this case, PRT and X appear to be confused for one another. The white grids are also important, showing tag combinations that the model learns to ignore, such as NUM vs DET in Italian (it) and NOUN vs PRON in Spanish (es) and Swedish (sv). It shows these types are not confused. The tokens that are NUM in Italian (it) are seldom projected as DET. Overall, the level of noise looks to be modest, which might not come as a surprise given the large clean parallel corpus for learning word alignments.
Now we present the results for two low-resource languages, Malagasy and Kinyarwanda which both have much smaller parallel corpora. The results in Table 2 show that our method works better than all others in both languages, with a similar pattern of results as for the eight simulation languages. Note that our method outperforms the state of the art on both languages (Duong et al., 2014a; Garrette and Baldridge, 2013).
To better understand the effect of the noise layer, we present the learned transformation matrices A in Figure 4. Note the strong diagonal for Malagasy in Figure 4, showing that each tag is most likely to map to itself, however there are also many high magnitude off diagonal elements. For instance nouns map to not just noun, but also adjective and number, but never pronoun (which
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Malagasy Kinyarwanda 1.0
-1.0
Figure 4: Noise transformation matrix A between POS tags and noised (projection) outputs, shown as columns and rows, respectively, for the two low-resource languages.
Model Accuracy Malagasy Kinyarwanda BiLSTM Annotated 81.5 76.9 BiLSTM Projected 67.2 61.9 BiLSTM Ann+Proj 78.6 73.2 MaxEnt Supervised 80.0 76.4 Duong et al. 85.3 78.3 BiLSTM+noise layer 86.3 82.5 Garrette et al. 81.2 81.9
Table 2: The POS tagging accuracy for various models in Malagasy and Kinyarwanda. The top results of the second part are taken from (Duong et al., 2014a), evaluated on the same data split.
are presumably well aligned.) Comparing results of Malagasy and Kinyarwanda in Figure 4, we can see the amount of noise is much greater in Kinyarwanda. This tallies with the performance results, in which we get stronger results and a greater improvement on Malagasy from using projection data, where we had more parallel data.","Part-of-speech tagging is a critical task for natural language processing (NLP) applications, providing lexical syntactic information. Automatic POS tagging has been wildly successful on many rich resource languages using supervised learning over large training corpora (McCallum et al., 2000; Lafferty et al., 2001; Ammar et al., 2016). However, learning POS taggers for low-resource languages from small amounts of annotated data is very challenging (Garrette and Baldridge, 2013; Duong et al., 2014a). For such problems, distant supervision via heuristic methods can provide cheap but inaccurately labelled data (Mintz et al., 2009; Takamatsu et al., 2012; Ritter et al., 2013; Plank et al., 2014). A compromise, considered here, is to use a mixture of both resources: a small collection of clean annotated data and noisy “distant” data.
A popular method for distant supervision is to use parallel data between a low-resource language and a rich-resource language. Although annotated data in low-resource languages are difficult to obtain, bilingual resources are more plentiful. For example parallel translations into English are often available, in the form of news reports, novels or the Bible. Parallel data allows annotation from the high-resource language to be projected across alignments to the low-resource language, which has been shown to be effective for several language processing tasks including POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013), named entity recognition (Wang and Manning, 2013) and dependency parsing (McDonald et al., 2013).
Although cross-lingual POS projection is popular it has several problems, including noise from poor word alignments (Täckström et al., 2013; Das and Petrov, 2011) and cross-lingual syntactic divergence (Duong et al., 2013). Previous work has proposed heuristics or constraints to clean the projected tag before or during learning. In contrast, we consider compensating for these problems explicitly, by learning a noise transformation to encode the mapping between ‘clean’ tags and the kinds of noisy tags produced from projection.
We propose a new neural network model for sequence tagging in a low-resource language, suitable for training with both a tiny gold standard annotated corpus, as well as distant supervision using cross lingual tag projection. Our model uses a bidirectional Long Short-Term Memory (BiLSTM), which produces two types of output: gold tags generated directly from the hidden state of neural network, and uncertain projected tags generated after applying a further linear transformation. This transformation, which we refer to as output noising encodes the mapping between the projected high-resource tags and low-resource
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
tags, and learning when and how much to trust the projected data. For example, for languages without determiners, the model can learn to map projected determiner tags to nouns, or if verbs are often poorly aligned, the model can learn to effectively ignore the projected verb tag, through associating all tags with verbs. Our model is trained jointly on gold and distant projected annotations, and can be trained end-to-end with backpropagation.
Our approach captures the relations among tokens, noisy projected POS tags and ground truth POS tags. Our work differs in the use of projection, in that we explicitly model the transformation between tagsets as part of a more expressive deep learning neural network. Our contributions fall in three aspects. First, we study the noise of projected data in word alignments and describe it with an additional layer model. Second, we integrate the model into a deep neural network and jointly train the model on both annotated and projected data to make the model learn from better supervisions. Finally, evaluating on eight simulated and two real-world low-resource languages, experimental results demonstrate that our approach uniformly equals or exceeds existing methods on simulated languages, and achieves 86.3% accuracy for Malagasy and 82.5% on Kinyarwanda, exceeding the state-of-the-art results (Duong et al., 2014a).","For most natural language processing tasks, the conventional approach to developing a system is to use supervised learning algorithms trained on a set of annotated data. However, this approach is inappropriate to low-resource languages due to the lack of annotated data. An alternative approach is to harness different source of information aside from simple annotated text. Knowledgebases such as dictionaries are one possible source of information, which can be used to to inform or constrain models, such as limiting the search space for POS tagging (Banko and Moore, 2004; Goldberg et al., 2008; Li et al., 2012).
Parallel bilingual corpora provide another important source of information, and are often plentiful even for many low-resource languages in the form of multilingual government documents, book translations, multilingual websites, etc. Word alignments can provide a bridge to project in-
formation from a resource-rich source language to a resource-poor target language. For example, parallel data has been used for named entity recognition (Wang and Manning, 2013) based on the observation that named entities are most often preserved in translation; and also in syntactic tasks such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2013) and dependency parsing (McDonald et al., 2013). Clues from related languages can also compensate for the lack of annotated data, as we expect there to be information shared between closely related languages in terms of the lexical items, morphology and syntactic structure. Some successful applications using language relatedness information are dependency parsing (McDonald et al., 2011), where a parser is estimated from a source, resource-rich language, but then applied to a target, low-resource language, and POS tagging (Hana et al., 2004) where parts of the tagger are estimated from the source language. However, these approaches are limited to closely related languages such as Czech and Russian, or Telugu and Kannada, and it is unclear whether these techniques will work well in situations where parallel data only exists for less-related languages, as is often the case in practice.
To summarize, for all these mentioned tasks, lexical resources are valuable sources of knowledge, but are also costly to build. Language relatedness information is applicable for closely related languages, but it is often the case that a given low-resource language does not have a closely-related, resource-rich language. Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems for low-resource languages (Yarowsky et al., 2001; Duong et al., 2014b; Guo et al., 2016; Guo et al., 2015), and here we primarily investigate methods to exploit parallel texts.
Yarowsky et al. (2001) pioneered the use of parallel data for projecting POS tag information from a resource-rich language to a resource-poor language. Duong et al. (2014b) proposed an approach using a maximum entropy classifier trained on 1000 tagged tokens, and used projected tags as auxiliary outputs. Das and Petrov (2011) used parallel data and exploited graph-based label propagation to expand the coverage of labelled tokens. Our work is closest to Duong et al. (2014a), and we share the same evaluation setting, which we
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
believe is well suited to the low-resource applications. Our approach differs from theirs in two ways: first we propose a deep learning model based on a long short term memory recurrent structure versus their maximum entropy classifier, and secondly we model the projection tag explicitly as a form of noise applied after classification, while they attempt to capture the correlations between tagsets only implicitly through a joint feature set over both tags. We believe that our work is the first to explicitly model the noise affecting cross-lingual projected annotations, and thereby allowing this rich data resource to be better exploited in learning NLP models in low-resource languages.",,"In this paper, we presented a technique for exploiting noisy cross-lingual projected annotations alongside a small amount of annotation data, in the context of POS tagging. To utilize both sources of data, we proposed a new model based on a bidirectional long short term memory recurrent neural network, with a layer for explicitly handling noisy projection labels. In two real low-resource languages, our methods outperform other algorithms. Our technique is general, and is likely to prove
useful for exploiting other noisy annotations such as distant supervision and crowd-sources annotations, and with other modelling approaches."
5,"Wikipedia is a resource of choice exploited in many NLP applications, yet we are not aware of recent attempts to adapt coreference resolution to this resource. In this work, we revisit a seldom studied task which consists in identifying in a Wikipedia article all the mentions of the main concept being described.  We show that by exploiting the Wikipedia markup of a document, as well as links to external knowledge bases such as Freebase, we can  acquire useful information on entities that helps to classify mentions as coreferent or not. We designed a classifier which drastically outperforms fair baselines built on top of state-of-the-art coreference resolution systems. We also measure the benefits of this classifier in a full coreference resolution pipeline applied to Wikipedia texts.",Coreference in Wikipedia: Main Concept Resolution,11,"The authors present a new version of the coreference task tailored to
Wikipedia. The task is to identify the coreference chain specifically
corresponding to the entity that the Wikipedia article is about.  The authors
annotate 30 documents with all coreference chains, of which roughly 25% of the
mentions refer to the ""main concept"" of the article. They then describe some
simple baselines and a basic classifier which outperforms these. Moreover, they
integrate their classifier into the Stanford (rule-based) coreference system
and see substantial benefit over all state-of-the-art systems on Wikipedia.

I think this paper proposes an interesting twist on coreference that makes good
sense from an information extraction perspective, has the potential to somewhat
revitalize and shake up coreference research, and might bridge the gap in an
interesting way between coreference literature and entity linking literature. 
I am sometimes unimpressed by papers that dredge up a new task that standard
systems perform poorly on and then propose a tweak so that their system does
better. However, in this case, the actual task itself is quite motivating to me
and rather than the authors fishing for a new domain to run things in, it
really does feel like ""hey, wait, these standard systems perform poorly in a
setting that's actually pretty important.""

THE TASK: Main concept resolution is an intriguing task from an IE perspective.
 I can imagine many times where documents revolve primarily around a particular
entity (biographical documents, dossiers or briefings about a person or event,
clinical records, etc.) and where the information we care about extracting is
specific to that entity. The standard coreference task has always had the issue
of large numbers of mentions that would seemingly be pretty irrelevant for most
IE problems (like generic mentions), and this task is unquestionably composed
of mentions that actually do matter.

From a methodology standpoint, the notion of a ""main concept"" provides a bit of
a discourse anchor that is useful for coreference, but there appears to still
be substantial overhead to improve beyond the baselines, particularly on
non-pronominal mentions. Doing coreference directly on Wikipedia also opens the
doors for more interesting use of knowledge, which the authors illustrate here.
So I think this domain is likely to be an interesting testbed for ideas which
would improve coreference overall, but which in the general setting would be
more difficult to get robust improvements with and which would be dwarfed by
the amount of work dealing with other aspects of the problem.

Moreover, unlike past work which has carved off a slice of coreference (e.g.
the Winograd schema work), this paper makes a big impact on the metrics of the
*overall* coreference problem on a domain (Wikipedia) that many in the ACL
community are pretty interested in.

THE TECHNIQUES: Overall, the techniques are not the strong point of this paper,
though they do seem to be effective. The features seem pretty sensible, but it
seems like additional conjunctions of these may help (and it's unclear whether
the authors did any experimentation in this vein).  The authors should also
state earlier in the work that their primary MC resolution system is a binary
classifier; this is not explicitly stated early enough and the model is left
undefined throughout the description of featurization.

MINOR DETAILS:

Organization: I would perhaps introduce the dataset immediately after ""Related
Works"" (i.e. have it be the new Section 3) so that concrete results can be
given in ""Baselines"", further motivating ""Approach"".

When Section 4 refers to Dcoref and Scoref, you should cite the Stanford papers
or make it clear that it's the Stanford coreference system (many will be
unfamiliar with the Dcoref/Scoref names).

The use of the term ""candidate list"" was unclear, especially in the following:

""We leverage the hyperlink structure of the article in order to enrich the list
of mentions with shallow semantic attributes. For each link found within the
article under consideration, we look through the candidate list for all
mentions that match the surface string of the link.""

Please make it clear that the ""candidate list"" is the set of mentions in the
article that are possible candidates for being coreferent with the MC.        I think
most readers will understand that this module is supposed to import semantic
information from the link structure of Wikipedia (e.g. if a mention is
hyperlinked to an article that is female in Freebase, that mention is female),
so try to keep the terminology clear.

Section 6.1 says ""we consider the union of WCR mentions and all mentions
predicted by the method described in (Raghunathan et al., 2010)."" However,
Section 4.1 implies that these are the same? I'm missing where additional WCR
mentions would be extracted.",,5,4,Oral Presentation,3,4,4,4,5,5,4,4,2016,"Since there is no system readily available for our task, we devised four baselines on top of two available coreference resolution systems. Given the output of a CR system applied on a Wikipedia article, our goal here is to isolate the coreference chain that represents the main concept. We experimented with several heuristics, yielding the following baselines.
B1 picks the longest coreference chain identified and considers that its mentions are those that co-refer to the main concept. The underlying assumption is that the most mentioned concept in a Wikipedia article is the main concept itself.
B2 picks the longest coreference chain identified if it contains a mention that exactly matches the MC title, otherwise it checks in decreasing order (longest to shortest) for a chain containing the title. We expect this baseline to be more precise than the previous one overall.
It turns out that, for both CR systems, mentions of the MC often are spread over several coreference chains. Therefore we devised two more baselines that aggregate chains, with an expected increase in recall.
B3 conservatively aggregates chains containing a mention that exactly matches the MC title.
B4 more loosely aggregates all chains that contain at least one mention whose span is a substring of the title.1 For instance, given the main concept Barack Obama, we concatenate all chains containing either Obama or Barack
1Grammatical words are not considered for matching.
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
263
264
265
266
267
268
269
270
271
272
287
288
289
290
291
292
293
294
295
296
297
298
299
in their mentions. Obviously, this baseline should show a higher recall than the previous ones, but risks aggregating mentions that are not related to the MC. For instance, it will aggregate the coreference chain referring to University of Sydney concept with a chain containing the mention Sydney.
We observed that, for pronominal mentions, those baselines were not performing very well in terms of recall. With the aim of increasing recall, we added to the chain all the occurrences of pronouns found to refer to the MC (at least once) by the baseline. This heuristic was first proposed by Nguyen et al. (2007). For instance, if the pronoun he is found in the chain identified by the baseline, all pronouns he in the article are considered to be mentions of the MC Barack Obama. Obviously, there are cases where those pronouns do not corefer to the MC, but this step significantly improves the performance on pronouns.Our approach is composed of a preprocessor which computes a representation of each mention in an article as well as its main concept; and a feature extractor which compares both representations for inducing a set of features.We extract mentions using the same mention detection algorithm embedded in Dcoref and Scoref. This algorithm described in (Raghunathan et al., 2010) extracts all named-entities, noun phrases and pronouns, and then removes spurious mentions.
We leverage the hyperlink structure of the article in order to enrich the list of mentions with shallow semantic attributes. For each link found within the article under consideration, we look through the candidate list for all mentions that match the surface string of the link. We assign to those mentions the attributes (entity type, gender and number) extracted from the Freebase entry (if it exists) corresponding to the Wikipedia article the hyperlink points to. This module behaves as a substitute to the named-entity linking pipelines used in other works, such as (Ratinov and Roth, 2012; Hajishirzi et al., 2013). We expect it to be of high quality because it exploits human-made links.
We use the WikipediaMiner (Milne and Witten, 2008) API for easily accessing any piece
of structure (clean text, labels, internal links, redirects, etc) in Wikipedia, and Jena2 to index and query Freebase.
In the end, we represent a mention by three strings (actual mention span, head word, and span up to the head noun), as well as its coarse attributes (entity type, gender and number). Figure 1 shows the representation collected for the mention San Fernando Valley region of the city of Los Angeles found in the Los Angeles Pierce College article.
We represent the main concept of a Wikipedia article by its title, its inferred type (a common noun inferred from the first sentence of the article). Those attributes were used by Nguyen et al. (2007) to heuristically link a mention to the main concept of an article. We further extend this representation by the MC name variants extracted
2http://jena.apache.org
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
from the markup of Wikipedia (redirects, text anchored in links) as well as aliases from Freebase; the MC entity types we extracted from the Freebase notable types attribute, and its coarse attributes extracted from Freebase, such as its NER type, its gender and number. If the concept category is a person (PER), we import the profession attribute. Figure 2 illustrates the information we collect for the Wikipedia concept Los Angeles Pierce College.We experimented with a few hundred features for characterizing each mention, focusing on the most promising ones that we found simple enough to compute. In part, our features are inspired by coreference systems that use Wikipedia and Freebase as feature sources (see Section 2). These features, along with others related to the characteristics of Wikipedia texts, allow us to recognize mentions of the MC more accurately than current CR systems. We make a distinction between features computed for pronominal mentions and features computed from the other mentions.For each mention, we compute seven families of features we sketch below.
base Number of occurrences of the mention span and the mention head found in the list of candidate mentions. We also add a normalized version of those counts (frequency / total number of mentions).
title, inferred type, name variants, entity type Most often, a concept is referred to by its name, one of its variants, or its type which are encoded in the four first fields of our MC representation. We define four families of comparison features, each corresponding to one of the first four fields of a MC representation (see Figure 2). For instance, for the title family, we compare the title text span with each of the text spans of the mention representation (see Figure 1). A comparison between a field of the MC representation and a mention text span yields 10 boolean features. These features encode string similarities (exact match, partial match, one being the substring of another, sharing of a number of words, etc.). An eleventh feature is the semantic relatedness score of Wu and
Palmer (1994). For title, we therefore end up with 3 sets of 11 feature vectors.
tag Part-of-speech tags of the first and last words of the mention, as well as the tag of the words immediately before and after the mention in the article. We convert this into 34×4 binary features (presence/absence of a specific combination of tags).
main Boolean features encoding whether the MC and the mention coarse attributes matches; also we use conjunctions of all pairs of features in this family.We characterize pronominal mentions by five families of features, which, with the exception of the first one, all capture information extracted from Wikipedia.
base The pronoun span itself, number, gender and person attributes, to which we add the number of occurrences of the pronoun, as well as its normalized count. The most frequently occurring pronoun in an article is likely to co-refer to the main concept, and we expect these features to capture this to some extent.
main MC coarse attributes, such as NER type, gender, number (see Figure 2).
tag Part-of-speech of the previous and following tokens, as well as the previous and the next POS bigrams (this is converted into 2380 binary features).
position Often, pronouns at the beginning of a new section or paragraph refer to the main concept. Therefore, we compute 5 (binary) features encoding the relative position (first, first tier, second tier, last tier, last) of a mention in the sentence, paragraph, section and article.
distance Within a sentence, we search before and after the mention for an entity that is compatible (according to Freebase information) with the pronominal mention of interest. If a match is found, one feature encodes the distance between the match and the mention; another feature encodes the number of other compatible pronouns in the same sentence. We expect that this family of features will
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
help the model to capture the presence of local (within a sentence) co-references.As our approach is dedicated to Wikipedia articles, we used a dedicated resource we call WCR3, and whose details will be described in the nonanonymized version of this paper. It consists of 30 documents, comprising 60k tokens annotated with the OntoNotes project guidelines (Pradhan et al., 2007). Each mention is annotated with three attributes: the mention type (named-entity, noun phrase, or pronominal), the coreference type (identity, attributive or copular) and the equivalent Freebase entity if it exists. The resource contains roughly 7000 non singleton mentions, among which 1800 refer to the main concept, which is to say that 30 chains out of 1469 make up for 25% of the mentions annotated.
Since most coreference resolution systems for English are trained and tested on ACE (Doddington et al., 2004) or OntoNotes (Hovy et al., 2006) resources, it is interesting to measure how state-ofthe art systems perform on the WCR dataset. To this end, we ran a number of recent CR systems: the rule-based system of (Lee et al., 2013) we call it Dcoref; the Berkeley systems described in (Durrett and Klein, 2013; Durrett and Klein, 2014); the latent model of Martschat and Strube (2015) we call it Cort in Table 1; and the system described in (Clark and Manning, 2015) we call it Scoref which achieved the best results to date on the CoNLL 2012 test set.
System WCR OntoNotes Dcoref 51.77 55.59 Durrett and Klein (2013) 51.01 61.41 Durrett and Klein (2014) 49.52 61.79 Cort 49.94 62.47 Scoref 46.39 63.61
Table 1: CoNLL F1 score of recent state of the art systems on the WCR dataset, and the 2012 OntoNotes test data for predicted mentions.
We evaluate the systems on the whole dataset, using the v8.01 of the CoNLL scorer4 (Pradhan et al., 2014). The results are reported in Table 1 along with the performance of the systems on the
3http://www.anonymized.org 4http://conll.github.io/
reference-coreference-scorers
CoNLL 2012 test data (Pradhan et al., 2012). Expectedly, the performance of all systems dramatically decrease on WCR, which calls for further research on adapting the coreference resolution technology to new text genres. What is more surprising is that the rule-based system of (Lee et al., 2013) works better than the machine-learning based systems on the WCR dataset. Also, the ranking of the statistical systems on this dataset differs from the one obtained on the OntoNotes test set.
The WCR dataset is far smaller than the OntoNotes one; still, we paid attention to sample Wikipedia articles of various characteristics: size, topic (people, organizations, locations, events, etc.) and internal link density. Therefore, we believe our results to be representative. Those results further confirm the conclusions in (Hendrickx and Hoste, 2009), which show that a CR system trained on news-paper significantly underperforms on data coming from users comments and blogs. Nevertheless, statistical systems can be trained or adapted to the WCR dataset, a point we leave for future investigations.
We generated baselines for all the systems discussed in this section, but found results derived from statistical approaches to be close enough that we only include results of two systems in the sequel: Dcoref (Lee et al., 2013) and Scoref (Clark and Manning, 2015). We choose these two because they use the same pipeline (parser, mention detection, etc), while applying very different techniques (rules versus machine learning).In this section, we first describe the data preparation we conducted (section 6.1), and provide details on the classifier we trained (section 6.2). Then, we report experiments we carried out on the task of identifying the mentions co-referent (positive class) to the main concept of an article (section 6.3). We compare our approach to the baselines described in section 3, and analyze the impact of the families of features described in section 4. We also investigate a simple extension of Dcoref which takes advantage of our classifier for improving coreference resolution (section 6.4).Each article in WCR was part-of-speech tagged, syntactically parsed and the named-entities were
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
592
593
594
595
596
597
598
599
identified. This was done thanks to the Stanford CoreNLP toolkit (Manning et al., 2014). Since WCR does not contain singleton mentions (in conformance to the OntoNotes guidelines), we consider the union of WCR mentions and all mentions predicted by the method described in (Raghunathan et al., 2010). Overall, we added about 13 400 automatically extracted mentions (singletons) to the 7 000 coreferent mentions annotated in WCR. In the end, our training set consists of 20 362 mentions: 1 334 pronominal ones (627 of them referring to the MC), and 19 028 non-pronominal ones (16% of them referring to the MC).We trained two Support Vector Machine classifiers (Cortes and Vapnik, 1995), one for pronominal mentions and one for non-pronominal ones, making use of the LIBSVM library (Chang and Lin, 2011) and the features described in Section 4.2. For both models, we selected5 the Csupport vector classification and used a linear kernel. Since our dataset is unbalanced (at least for non-pronominal mentions), we penalized the negative class with a weight of 2.0.
During training, we do not use gold mention attributes, but we automatically enrich mentions with the information extracted from Wikipedia and Freebase, as described in Section 4.We focus on the task of identifying all the mentions referring to the main concept of an article. We measure the performance of the systems we devised by average precision, recall and F1 rates computed by a 10-fold cross-validation procedure.
The results of the baselines and our approach are reported in Table 4. Clearly, our approach outperforms all baselines for both pronominal and non-pronominal mentions, and across all metrics. On all mentions, our best classifier yields an absolute F1 increase of 13 points over Dcoref, and 15 points over Scoref.
In order to understand the impact of each family of features we considered in this study, we trained various classifiers in a greedy fashion. We started with the simplest feature set (base) and gradually added one family of features at a time, keeping
5We tried with less success other configurations on a heldout dataset.
at each iteration the one leading to the highest increase in F1. The outcome of this process for the pronominal mentions is reported in Table 2.
A baseline that always considers that a pronominal mention is co-referent to the main concept results in an F1 measure of 63.7%. This naive baseline is outperformed by the simplest of our model (base) by a large margin (over 10 absolute points). We observe that recall significantly improves when those features are augmented with the MC coarse attributes (+main). In fact, this variant already outperforms all the Dcoref-based baselines in terms of F1 score. Each feature family added further improves the performance overall, leading to better precision and recall than any of the baselines tested.
P R F1 always positive 46.70 100.00 63.70
base 70.34 78.31 74.11 +main 74.15 90.11 81.35 +position 80.43 89.15 84.57 +tag 82.12 90.11 85.93 +distance 85.46 92.82 88.99
Table 2: Performance of our approach on the pronominal mentions, as a function of the features.
Inspection shows that most of the errors on pronominal mentions are introduced by the lack of information on noun phrase mentions surrounding the pronouns. In example (f) shown in Figure 3, the classifier associates the mention it with the MC instead of the Johnston Atoll “ Safeguard C ” mission.
Table 3 reports the results obtained for the nonpronominal mentions classifier. The simplest classifier is outperformed by most baselines in terms of F1. Still, this model is able to correctly match mentions in example (a) and (b) of Figure 3 simply
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Pronominal Non Pronominal All P R F1 P R F1 P R F1
Dcoref B1 64.51 76.55 70.02 70.33 63.09 66.51 67.92 67.77 67.85 B2 76.45 50.23 60.63 83.52 49.57 62.21 80.90 49.80 61.65 B3 76.39 65.55 70.55 83.67 56.20 67.24 80.72 59.45 68.47 B4 71.74 83.41 77.13 74.39 75.59 74.98 73.30 78.31 75.77
Scoref B1 76.59 78.30 77.44 54.66 39.37 45.77 64.11 52.91 57.97 B2 89.59 74.16 81.15 69.90 31.20 43.15 79.69 46.14 58.44 B3 83.91 77.35 80.49 73.17 55.44 63.08 77.39 63.06 69.49 B4 78.48 90.74 84.17 67.51 67.85 67.68 71.68 75.81 73.69
this work 85.46 92.82 88.99 91.65 85.88 88.67 89.29 88.30 88.79
Table 4: Performance of the baselines on the task of identifying all MC coreferent mentions.
because those mentions are frequent within their respective article. Of course, such a simple model is often wrong as in example (c), where all mentions the United States are associated to the MC, simply because this is a frequent mention.
a MC= Anatole France France is also widely believed to be the model for narrator Marcel’s literary idol Bergotte in Marcel Proust’s In Search of Lost Time. b MC= Harry Potter and the Chamber of Secrets Although Rowling found it difficult to finish the book, it won . . . . c MC= Barack Obama On August 31, 2010, Obama announced that the United States* combat mission in Iraq was over. d MC= Houston Texans In 2002, the team wore a patch commemorating their inaugural season... e MC= Houston Texans The name Houston Oilers was unavailable to the expansion team... f MC= Johnston Atoll In 1993 , Congress appropriated no funds for the Johnston Atoll Safeguard C mission , bringing it* to an end. g MC= Houston Texans The Houston Texans are a professional American football team based in Houston* , Texas.
Figure 3: Examples of mentions (underlined) associated with the MC. An asterisk indicates wrong decisions.
The title feature family drastically increases precision, and the resulting classifier (+title) outperforms all the baselines in terms of F1 score. Adding the inferred type feature family gives a further boost in recall (7 absolute points) with no loss in precision (gain of almost 2 points). For instance, the resulting classifier can link the mention the team to the MC Houston Texans (see example (d)) because it correctly identifies the term team as a type. The family name variants also gives a nice boost in recall, in a slight expense of precision. This drop is due to some noisy redirects in Wikipedia, misleading our classifier. For instance, Johnston and Sand Islands is a redirect of the Johnston Atoll article. The entity type family further improves performance, mainly because it plays a role similar to the inferred type features extracted from Freebase. This indicates that the noun type induced directly from the first sentence of a Wikipedia article is pertinent and can complement the types extracted from Freebase when available or serve as proxy when they are missing. Finally, the main family significantly increases precision (over 4 absolute points) with no loss in recall. To illustrate a negative example, the resulting classifier wrongly recognizes mentions referring to the town Houston as coreferent to the football team in example (g). We handpicked a number of classification errors and found that most of these are difficult coreference cases. For instance, our best classifier fails to recognize that the mention the expansion team refers to the main concept Houston Texans in example (e).
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
System MUC B3 CEAFφ4 CoNLL P R F1 P R F1 P R F1 F1 Dcoref 61.59 60.42 61.00 53.55 43.33 47.90 42.68 50.86 46.41 51.77 D&K (2013) 68.52 55.96 61.61 59.08 39.72 47.51 48.06 40.44 43.92 51.01 D&K (2014) 63.79 57.07 60.24 52.55 40.75 45.90 45.44 39.80 42.43 49.52 M&S (2015) 70.39 53.63 60.88 60.81 37.58 46.45 47.88 38.18 42.48 49.94 C&M (2015) 69.45 49.53 57.83 57.99 34.42 43.20 46.61 33.09 38.70 46.58 Dcoref++ 66.06 62.93 64.46 57.73 48.58 52.76 46.76 49.54 48.11 55.11
Table 5: Performance of Dcoref++ on WCR compared to state of the art systems, including in order: Lee et al. (2013); Durrett and Klein (2013) - Final; Durrett and Klein (2014) - Joint; Martschat and Strube (2015) - Ranking:Latent; Clark and Manning (2015) - Statistical mode with clustering.Identifying all the mentions of the MC in a Wikipedia article is certainly useful in a number of NLP tasks (Nguyen et al., 2007; Nakayama, 2008). Finding all coreference chains in a Wikipedia article is worth studying. In the following, we describe an experiment where we introduced in Dcoref a new high-precision sieve which uses our classifier6. Sieves in Dcoref are ranked in decreasing order of precision, and we ranked this new sieve first. The aim of this sieve is to construct the coreference chain equivalent to the main concept. It merges two chains whenever they both contain mentions to the MC according to our classifier. We further prevent other sieves from appending new mentions to the MC coreference chain.
We ran this modified system (called Dcoref++) on the WCR dataset, where mentions were automatically predicted. The results of this system are reported in Table 5, measured in terms of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFφ4 (Luo, 2005) and the average F1 CoNLL score (Denis and Baldridge, 2009).
We observe an improvement for Dcoref++ over the other systems, for all the metrics. In particular, Dcoref++ increases by 4 absolute points the CoNLL F1 score. This shows that early decisions taken by our classifier benefit other sieves as well. It must be noted, however, that the overall gain in precision is larger than the one in recall.",,"Coreference Resolution (CR) is the task of identifying all mentions of entities in a document and grouping them into equivalence classes. CR is a prerequisite for many NLP tasks. For example, in Open Information Extraction (OIE) (Yates et al., 2007), one acquires subject-predicate-object relations, many of which (e.g., <the foundation stone, was laid by, the Queen s daughter>) are useless because the subject or the object contains material coreferring to other mentions in the text being mined.
Most CR systems, including state-of-the-art ones (Durrett and Klein, 2014; Martschat and Strube, 2015; Clark and Manning, 2015) are essentially adapted to news-like texts. This is basically imputable to the availability of large datasets where this text genre is dominant. This includes
resources developed within the Message Understanding Conferences (e.g., (Hirshman and Chinchor, 1998)) or the Automatic Content Extraction (ACE) program (e.g., (Doddington et al., 2004)), as well as resources developed within the collaborative annotation project OntoNotes (Pradhan et al., 2007).
It is now widely accepted that coreference resolution systems trained on newswire data performs poorly when tested on other text genres (Hendrickx and Hoste, 2009; Schäfer et al., 2012), including Wikipedia texts, as we shall see in our experiments.
Wikipedia is a large, multilingual, highly structured, multi-domain encyclopedia, providing an increasingly large wealth of knowledge. It is known to contain well-formed, grammatical and meaningful sentences, compared to say, ordinary internet documents. It is therefore a resource of choice in many NLP systems, see (Medelyan et al., 2009) for a review of some pioneering works.
While being a ubiquitous resource in the NLP community, we are not aware of much work conducted to adapt CR to this text genre. Two notable exceptions are (Nguyen et al., 2007) and (Nakayama, 2008), two studies dedicated to extract tuples from Wikipedia articles. Both studies demonstrate that the design of a dedicated rulebased CR system leads to improved extraction accuracy. The focus of those studies being information extraction, the authors did not spend much efforts in designing a fully-fledged CR designed for Wikipedia, neither did they evaluate it on a coreference resolution task.
Our main contribution in this work is to revisit the task initially discussed in (Nakayama, 2008) which consists in identifying in a Wikipedia article all the mentions of the concept being described by this article. We refer to this concept as the “main concept” (MC) henceforth. For instance, within
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
the article Chilly Gonzales, the task is to find all proper (e.g. Gonzales, Beck), nominal (e.g. the performer) and pronominal (e.g. he) mentions that refer to the MC “Chilly Gonzales”.
For us, revisiting this task means that we propose a testbed for evaluating systems designed for it, and we compare a number of state-of-the-art systems on this testbed. More specifically, we frame this task as a binary classification problem, where one has to decide whether a detected mention refers to the MC. Our classifier exploits carefully designed features extracted from Wikipedia markup and characteristics, as well as from Freebase; many of which we borrowed from the related literature.
We show that our approach outperforms stateof-the-art generic coreference resolution engines on this task. We further demonstrate that the integration of our classifier into the state-of-the-art rule-based coreference system of Lee et al. (2013) improves the detection of coreference chains in Wikipedia articles.
The paper is organized as follows. We discuss related works in Section 2. We describe in Section 3 the baselines we built on top of two state-ofthe-art coreference resolution systems, and present our approach in Section 4. We describe the dataset we exploited in Section 5. We explain experiments we conducted on a Wikipedia dataset in section 6, and conclude in Section 7.","Our approach is inspired by, and extends, previous works on coreference resolution which show that incorporating external knowledge into a CR system is beneficial. In particular, a variety of approaches (Ponzetto and Strube, 2006; Ng, 2007; Haghighi and Klein, 2009) have been shown to benefit from using external resources such as Wikipedia, WordNet (Miller, 1995), or YAGO (Suchanek et al., 2007). Ratinov and Roth (2012) and Hajishirzi et al. (2013) both investigate the integration of named-entity linking into machine learning and rule-based coreference resolution system respectively. They both use GLOW (Ratinov et al., 2011) a wikification system which associates detected mentions with their equivalent entity in Wikipedia. In addition, they assign to each mention a set of highly accurate knowledge attributes extracted from Wikipedia and Freebase (Bollacker et al., 2008), such as the
Wikipedia categories, gender, nationality, aliases, and NER type (ORG, PER, LOC, FAC, MISC).
One issue with all the aforementioned studies is that inaccuracies often cause cascading errors in the pipeline (Zheng et al., 2013). Consequently, most authors concentrate on high-precision linking at the cost of low recall.
Dealing specifically with Wikipedia articles, we can directly exploit the wealth of markup available (redirects, internal links, links to Freebase) without resorting to named-entity linking, thus benefiting from much less ambiguous information on mentions.",,"We developed a simple yet powerful approach that accurately identifies all the mentions that co-refer
6We use predicted results from 10-fold cross-validation.
to the concept being described in a Wikipedia article. We tackle the problem with two (pronominal and non-pronominal) models based on well designed features. The resulting system is compared to baselines built on top of state-of-the-art systems adapted to this task. Despite being relatively simple, our model reaches 89 % in F1 score, an absolute gain of 13 F1 points over the best baseline. We further show that incorporating our system into the Stanford deterministic rule-based system (Lee et al., 2013) leads to an improvement of 4% in F1 score on a fully fledged coreference task.
In order to allow other researchers to reproduce our results, and report on new ones, we share all the datasets we used in this study. We also provide a dump of all the mentions in English Wikipedia our classifier identified as referring to the main concept, along with information we extracted from Wikipedia and Freebase. This will be available at www.somewhere.country.
A natural extension of this work is to identify all coreference relations in a Wikipedia article, a task we are currently investigating."
6,"The representation of many common semantic phenomena requires structural properties beyond those commonly used for syntactic parsing.  We discuss a set of structural properties required for broad-coverage semantic representation, and note that existing parsers support some of these properties, but not all. We propose two transition-based techniques for parsing such semantic structures: (1) applying conversion procedures to map them into related formalisms, and using existing state-of-the-art parsers on the converted representations; and (2) constructing a parser that directly supports the full set of properties. We experiment with UCCA-annotated corpora, the only ones with all these structural semantic properties. Results demonstrate the effectiveness of transition-based methods for the task.",Broad-Coverage Semantic Parsing: A Transition-Based Approach,18,"This paper presents a transition-based graph parser able to cope with the rich
representations of a semantico-cognitive annotation scheme, instantiated in the
UCCA corpora. The authors start first by exposing what, according to them,
should cover a semantic-based annotation scheme: (i) being graph-based
(possibility for a token/node of having multiple governors) (2) having
non-terminal nodes (representing complex structures â syntactic -: coordinate
phrases, lexical: multiword expression) and (3) allowing discontinuous elements
(eg. Verbs+particules). Interestingly, none of these principles is tied to a
semantic framework, they could also work for syntax or other representation
layers. The authors quickly position their work by first introducing the larger
context of broad-coverage semantic parsing then their annotation scheme of
choice (UCCA).              They then present 3 sets of parsing experiments: (i) one
devoted to phrase-based parsing using the Stanford parser and an UCCA to
constituency conversion, (ii) one devoted to dependency parsing using an UCCA
to dependency conversion and finally (iii) the core of their proposal, a  set
of experiments showing that their transition-based graph parser is suitable for
direct parsing of UCCA graphs.

I found this work interesting but before considering a publication, I have
several concerns with regards to the methodology and the empirical
justifications:

The authors claimed that there are the first to propose a parser for a
semantically-oriented scheme such as theirs. Of course, they are. But with all
due respect to the work behind this scheme, it is made of graphs with a various
level of under-specified structural arguments and semantically oriented label
(Process, state) and nothing in their transition sets treats the specificities
of such a graph. Even the transitions related to the remote edges could have
been handled by the other ones assuming a difference in the label set itself
(like adding an affix for example). If we restrict the problem to graph
parsing, many works post the 2014-2015 semeval shared tasks (Almeda and
Martins, 2014,2015 ; Ribeyre et al, 2014-2015) proposed an extension to
transition-based graph parser or an adaptation of a higher-model one, and
nothing precludes their use on this data set.  Itâs mostly the use of a
specific feature template that anchors this model to this scheme (even though
itâs less influencial than the count features and the unigram one). Anyway,
because the above-mentioned graph-parsers are available [1,2] I donât
understand why they couldnât be used as a baseline or source of comparisons.
Regarding the phrase-based  experiments using uparse, it could have been also
validated by another parser from Fernandez-Gonzales and Martins (2015) which
can produce LCFRS-like parsing as good as Uparse (ref missing when you first
introduced uparse).  

Because this scheme supports a more abstract view of syntaxico-semantic
structures than most of the SDP treebanks, it would have been important to use
the same metrics as in the related shared task. At this point in the field,
many systems, models and data set are competing and I think that the lack of
comparison points with other models and parsers is detrimental to this work as
whole. Yet I found it interesting and because weâre at crossing time in term
of where to go next, I think that this paper should be discussed at a
conference such as ConLL.

Note in random order
-         please introduce the âgrounded semanticâ before page 2, you use
that phrase before
-         why havenât you try to stick to constituent-tree with rich node
labels and propagater traces and then train/parse with the Berkeley parser? It
could have been a good baseline. 
-         The conversion to surface dependency trees is in my mind useless: you
loose too many information, here a  richer conversion such as the one from
âSchluter et al, 2014, Semeval SDP) should have been used.
-         Can you expand on âUCCA graphs may contains implicit unit that have
no correspondent in the textâ  or provide a ref or an example.
-         You mentioned other representations such as MRS and DRT, this raises
the fact that your scheme doesnât seem to allow for a modelling of quantifier
scope information. Itâs thus fully comparable to other more syntax-oriented
scheme. Itâs indeed more abstract than DM for example and probably more
underspecified than the semantic level of the PCEDT but how much? How really
informative is this scheme and how really âparsableâ is it? According to
your scores, it seems âharderâ but an  error analysis would have been
useful.
- As I said before, the 3 principles you devised could apply to a lot of
things,  they look a bit ad-hoc to me and would probably need to take place in
a much wider (and a bit clearer) introduction. What are you trying to argue
for: a parser that can parse UCCA? a model suitable for semantic analysis ? or
a semantic oriented scheme that can actually be parsable?  you're trying to say
all of those in a very dense way and it's borderline to be be confusing.

[1] http://www.corentinribeyre.fr/projects/view/DAGParser
[2] https://github.com/andre-martins/TurboParser and
https://github.com/andre-martins/TurboParser/tree/master/semeval2014_data",,3,5,Oral Presentation,4,4,4,5,4,5,3,3,2016,"Broad-coverage Semantic Representation. While earlier work on grounded semantic parsing has mostly concentrated on shallow semantic analysis, focusing on semantic role labeling of verbal argument structures, the focus has recently shifted to parsing of more elaborate representations that account for a wider range of phenomena. Most closely related to this work is Broad-coverage Semantic Dependency Parsing (SDP), addressed in two SemEval tasks (Oepen et al., 2014; Oepen et al., 2015), experimenting with the Prague tectogrammatical layer (Sgall et al., 1986; Böhmová et al., 2003), and with dependencies derived from the Enju parser,1 and Lingo ERG (Flickinger, 2002). Like BSS parsing, SDP addresses a wide range of semantic phenomena, and supports discontinuous units and multiple parents. However, SDP uses bi-lexical dependencies, disallowing non-terminal nodes, and thus faces difficulties in supporting structures that have no clear head, such as coordination (Ivanova et al., 2012).
Another line of work addresses parsing into non-grounded2 semantic representation, notably Abstract Meaning Representation (AMR) (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). While sharing much of this work’s motivation, not grounding the representation in the text complicates the parsing task, as it requires that the alignment between words and logical symbols be automatically (and imprecisely) detected.3 Furthermore, grounding allows breaking down sentences into semantically meaningful sub-spans, which is useful for many applications (see discussion in Fernández-González and Martins (2015)). Wang et al. (2015) applied a transition-based approach to AMR parsing. Their method involved first syntactically parsing the input, and then converting the result into AMR. Other approaches for semantic representation, such as MRS (Copestake et al., 2005) and DRT (Kamp et al., 2011), involve considerably different representation and parsing approaches, and so fall beyond the scope of our discussion.
1See http://kmcs.nii.ac.jp/enju 2By grounded we mean the text tokens are directly annotated as part of the representation, as opposed to abstract formalisms approximating logical form, for example.
3Considerable technical effort has been invested in the AMR alignment task under various approaches (Flanigan et al., 2014; Pourdamghani et al., 2014; Pust et al., 2015).
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
The UCCA Annotation Scheme. Universal Cognitive Conceptual Annotation (UCCA) is a cross-linguistically applicable semantic representation scheme, that builds on the established “Basic Linguistic Theory” framework for typological description (Dixon, 2010a; Dixon, 2010b; Dixon, 2012), and on the Cognitive Linguistics literature. UCCA is a multi-layered representation, where each layer corresponds to a “module” of semantic distinctions (e.g., predicate-argument structures, adverbials, coreference etc.).
Formally, a UCCA structure over a sentence is a DAG, whose leaves correspond to the sentence’s words. The nodes of the graph, or its “units”, are either terminals or several sub-units (not necessarily contiguous) jointly viewed as a single entity according to some semantic or cognitive consideration. Edges bear a category, indicating the role of the sub-unit in the relation that the parent represents. UCCA structures support all three criteria of BSS: multiple parents, non-terminal nodes, and discontinuous units.
UCCA’s foundational layer, which we use here, covers the predicate-argument structures evoked by predicates of all grammatical categories (verbal, nominal, adjectival and others), the interrelations between them, as well as other major linguistic phenomena such as coordination and multi-word expressions. This set of categories has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation, and semantic stability in translation: UCCA annotations of translated text usually contain the same set of relationships (Sulem et al., 2015). This finding supports the claim that UCCA represents an abstract level of semantics, shared by different languages.
The layer’s basic notion is the Scene, which describes a movement, action or state. Each Scene contains one Main Relation, as well as one or more Participants. For example, the sentence “After graduation, John moved to Paris” contains two Scenes, whose main relations are “graduation” and “moved”. “John” is a Participant in both Scenes, while “Paris” only in the latter. UCCA marks one of the incoming edges for each non-root as “primary” and the others as “remote” edges. The two Scenes in this sentence are both arguments of the Linker “After”, which in this case expresses a temporal relation between them. Figure 1 presents the UCCA annotation of this and other examples.
Further categories account for relations between Scenes and the internal structures of complex arguments (e.g., coordination) and relations (e.g., complex adverbials, such a “very clearly”). UCCA graphs may contain implicit units that have no correspondent in the text, but the parsing of these units is deferred to future work, as it is likely to require different methods than those explored here (Roth and Frank, 2015).We begin by assessing the ability of existing technology to address the task, by taking a conversionbased approach. Training proceeds by converting BSS into a different representation, and training an existing parser on the converted structures. We evaluate the trained parsers by applying them to the test set, and converting the results back to BSS, where they are compared with the gold standard. The error resulting from this back and forth conversion is discussed in Section 6. Notation. Let L be the set of possible edge labels. A BSS is a directed acyclic graph G = (V,E, `) over a sequence of tokens w1, . . . , wn, where ` : E → L is a function of edge labels. For each token wi (i = 1, . . . , n), there exists a leaf (or a terminal) ti ∈ V . Conversion to Constituency Trees. We convert BSS to constituency trees by removing a subset of the edges.4 Specifically, when converting UCCA structures, we simply remove all remote edges, leaving only primary edges, which form a tree structure (see Figure 1a). The inverse conversion from trees to BSS is simply the identity function, as every constituency tree is a BSS. Conversion to Dependency Trees. In the conversion to dependency trees, we first convert BSS to constituency trees using the above procedure, and then convert the result to dependency trees. Assume Tc = (Vc, Ec, `c) is a constituency tree with labels `c : Ec → L, where L is the set of possible labels. The conversion from Tc to a dependency tree involves the removal of all nonterminals from Tc and the addition of edges between terminals. The nodes of the converted dependency tree are simply the terminals of Tc.
We define a linear order over possible edge labels L. For each node u ∈ V , denote with
4For trees, labeling nodes is equivalent to labeling edges. Thus, we do not distinguish between the two options. Note also that as the original structures may contain discontinuities, so may the resulting trees.
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
Data: constituency tree Tc = (Vc, Ec, `c) Result: dependency tree Td = (Vd, Ed, `d) foreach u ∈ Vc do
h(u)← argminv Priority(`c(u, v)); end Vd ← Terminals(Tc), Ed ← ∅; foreach t ∈ Vd do
u← t; while u = h(Parentc(u)) do
h∗(u)← t; u← Parentc(u);
end n(t)← h(u); end foreach t ∈ Vd do
u← Parentc(n(t)); t′ ← h∗(u); Ed ← Ed ∪ {(t′, t)}; `d(t
′, t)← `c(u, n(t))}; end
Algorithm 1: Constituency to dependency conversion pro-
cedure.
h(u) its child with the highest edge label. Denote with h∗(u) the terminal reached by recursively applying h(·) over u. For each terminal t, we define n(t) as the highest non-terminal such that t = h∗(n(t)), i.e., n(t) is the only node such that t = h∗(n(t)) and t 6= h∗(Parentc(n(t))). The head of t according to the dependency graph is the terminal h∗(Parentc(n(t))). The complete conversion procedure from constituency to dependency is given in Algorithm 1.
We note that this conversion procedure is simpler than the head percolation procedure used for converting syntactic constituency trees to dependency trees. This is because h(u) of a node u (similar to u’s head-containing child), depends only on the label of the edge (h(u), u), and not on the subtree spanned by u, because edge labels in UCCA directly express the role of the child in the parent unit, and are thus sufficient for determining which of u’s children contains the head node.
The inverse conversion introduces non-terminal nodes back into the tree. As the distinction between low- and high-attaching nodes is lost in the constituency to dependency conversion, we heuristically assume that attachments are always high-attaching. Assume Td = (Vd, Ed, `d) is a dependency tree. We begin by creating a root node r. Then, iterating over Vd in topological order, we add its members as terminals to the constituency tree and create a pre-terminal parent for each, with an edge labeled as Terminal between them. The parents of the pre-terminals are determined by the terminal’s parent in the dependency tree: if a de-
Data: dependency tree Td = (Vd, Ed, `d) Result: constituency tree Tc = (Vc, Ec, `c) r ← Node(); Vc ← {r}, Ec ← ∅; foreach t ∈ TopologicalSort(Vd) do
u← Node(); Vc ← Vc ∪ {u, t}; Ec ← Ec ∪ {(u, t)}; `c(u, t)← Terminal ; t′ ← Parentd(t); if t′ = ROOT then
Ec ← Ec ∪ {(r, u)}; `c(r, u)← Label(r);
else if ∃v ∈ Vd : (t, v) ∈ Ed then
u′ ← Node(); Ec ← Ec ∪ {(u′, u)}; `c(u
′, u)← Label(u′); else
u′ ← u; end p← Parentc(t′); Ec ← Ec ∪ {(p, u′)}; `c(p, u
′)← `d(t′, t); end
end Algorithm 2: Dependency to constituency conversion pro-
cedure.
pendency node t is a child of the root in Td, then t’s pre-terminal will also be a child of the root node. Otherwise, t’s pre-terminal is the child of the preterminal associated with t’s head in Td. We add an intermediate node in between if t has any dependents in Td, to allow adding their pre-terminals as children. Edge labels for the intermediate edges are determined by a rule-based function, denoted by Label(u).5 In practice, it mostly selects the UCCA label Center. This conversion procedure is given in Algorithm 2.We now turn to presenting BSP, a transition-based parser that supports the three criteria of broadcoverage semantic structures.
Transition-based parsing (Nivre, 2003) creates the parse as it scans the text from left to right. The parse is created incrementally by applying a transition at each step to the parser state, defined using three data structures: a buffer B of tokens and nodes to be processed, a stack S of nodes currently being processed, and a graph G = (V,E, `) of constructed nodes and labeled edges. Some of the states are marked as terminal, meaning that G is the final output. A classifier is used at each step
5See Supplementary Material for the definition of Label.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
to select the next transition based on features that encode the parser’s current state. During training, an oracle creates training instances for the classifier, based on the gold-standard annotation.
Despite being based on local decisions, transition-based methods have yielded excellent results in a variety of parsing tasks. Within syntactic dependency parsing, transition-based methods have been successfully applied to corpora in many languages and domains, yielding some of the best reported results (Dyer et al., 2015; Ballesteros et al., 2015). The approach has also yielded results comparable with the state-of-the-art in constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al., 2015) and AMR parsing (Wang et al., 2015).
BSP mostly builds on recent advances in discontinuous constituency and dependency DAG parsing techniques, and further introduces novel UCCA-oriented features for parsing BSS.
Transition Set. Given a sequence of tokens w1, . . . , wn, we predict a BSS G whose leaves correspond to the tokens. Parsing starts with a single node on the stack (the root node), and the input tokens w1, . . . , wn in the buffer. The set of transitions is given in Figure 2. In addition to the standard SHIFT and REDUCE operations, we follow previous work in transition-based constituency parsing (Sagae and Lavie, 2005), and include the NODE transition for creating new nonterminal nodes. Concretely, NODEX creates a new node on the buffer as a parent of the first element on the stack, with an X-labeled edge.
LEFT-EDGEX and RIGHT-EDGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. As a UCCA node may only have one incoming primary edge, EDGE transitions are disallowed where the child node already has an incoming primary edge. LEFTREMOTEX and RIGHT-REMOTEX do not have this restriction, and the created edge is marked as remote. We distinguish between these two pairs of transitions, for the parser to be able to determine whether an edge is a primary or a remote one. In order to support the prediction of multiple parents, node and edge transitions do not automatically ap-
ply REDUCE. This is in line with other work on transition-based DAG dependency parsing (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015). Once all edges for a particular node have been created, it is removed from the stack by applying REDUCE.
SWAP allows handling discontinuous nodes, by popping the second node on the stack and adding it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015). Finally, FINISH pops the root node and marks the state as terminal.
Features. Figure 3 presents the feature templates used by the parser. For some of the features, we used the notion of head word, defined by the h∗(·) function (Section 3). While head words are not explicitly represented in the UCCA scheme, these features proved useful as means of encoding word-to-word relations.
In addition to the binary features defined by the feature templates, we employ a real-valued feature, ratio, corresponding to the ratio between the number of terminals to number of nodes in the graph G. This novel feature serves as a regularizer for the creation of new nodes, and should be beneficial for other transition-based constituency parsers too. Features are generally adapted from the related parsers of (Zhang and Clark, 2009; Zhu et al., 2013; Tokgöz and Eryiğit, 2015; Maier, 2015), with a small additional set of features encoding relevant information for the novel LEFTREMOTEX and RIGHT-REMOTEX transitions.
Training. Following Maier (2015), we use a linear classifier, using the averaged structured perceptron algorithm for training it (Collins and Roark, 2004) with the MINUPDATE (Goldberg and Elhadad, 2011) procedure: a minimum number of updates to a feature has to occur in training for it to be included in the model. Inference is performed greedily (i.e., without beam search).
For training the local classifier, we use a dynamic oracle (Goldberg and Nivre, 2012), i.e., an oracle that outputs a set of optimal transitions: when applied to the current parser state, the gold standard graph is reachable from the resulting state. For example, the oracle would predict a NODE transition if the stack has on its top a parent in the gold graph that has not been created, but would predict a RIGHT-EDGE transition if the second stack element is a parent of the first element according to the gold graph and the edge between them has not been created. The transition
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Initial State Final State Stack Buffer Nodes Edges Terminal? Stack Buffer Nodes Edges Terminal? [root] w1:n {root} ∪
w1:n
∅ − ∅ ∅ V E +
Before Transition Transition After Transition Condition Stack Buffer Nodes Edges Stack Buffer Nodes Edges Terminal? S x | B V E SHIFT S | x B V E − S | x B V E REDUCE S B V E − S | x B V E NODEX S | x y | B V ∪ {y} E ∪ {(y, x)X} − x 6= root S | y, x B V E LEFT-EDGEX S | y, x B V E ∪ {(x, y)X} − 
x 6∈ w1:n, y 6= root, y 6;G x
S | x, y B V E RIGHT-EDGEX S | x, y B V E ∪ {(x, y)X} − S | y, x B V E LEFT-REMOTEX S | y, x B V E ∪ {(x, y)∗X} − S | x, y B V E RIGHT-REMOTEX S | x, y B V E ∪ {(x, y)∗X} − S | x, y B V E SWAP S | y x | B V E − i(x) < i(y) [root] ∅ V E FINISH ∅ ∅ V E +
Figure 2: The transition set of BSP. We write the stack with its top to the right and the buffer with its head to the left. (·, ·)X denotes a primary X-labeled edge, and (·, ·)∗X a remote X-labeled edge. i(x) is a running index for the created nodes. EDGE transitions have an additional condition: the prospective child may not already have a primary parent.
predicted by the classifier is deemed correct and is applied to the parser state to reach the subsequent state, if the transition is included in the set of optimal transitions. Otherwise, a random optimal transition is applied and the classifier’s weights are updated according to the perceptron update rule.Data. We conduct our main experiments on the UCCA Wikipedia corpus (henceforth, Wiki), and use the English part of the UCCA Twenty Thousand Leagues Under the Sea English-French parallel corpus (henceforth, 20K Leagues) as out-ofdomain data.6 Table 1 presents some statistics for the two corpora, demonstrating that while the Wiki corpus is over ten times larger, the overall statistics are similar. We use passages of indices up to 655 of the Wiki corpus as our training set, passages 656–700 as development set, and passages 701– 695 as in-domain test set. While UCCA edges can cross sentence boundaries, we adhere to the common practice in semantic parsing and train our parsers on individual sentences, discarding interrelations between them (0.18% of the edges). We also discard linkage nodes and edges, as they often express inter-sentence relations and are thus mostly redundant when applied at the sentence level, as well as implicit nodes (Section 2). In the out-of-domain experiments, we apply the same parser (trained on the Wiki corpus) to the 20K Leagues corpus without re-tuning the parameters. Evaluation. Since there are no standard evaluation measures for BSS, we define two simple
6Both are available at http://www.cs.huji.ac. il/˜oabend/ucca.html
measures for comparing such structures. Assume Gp = (Vp, Ep, `p) and Gg = (Vg, Eg, `g) are the predicted and gold-standard DAGs over the same sequence of terminals W = {w1, . . . , wn}, respectively. For an edge e = (u, v) in either graph, where u is the parent and v is the child, define its yield y(e) ⊆ W as the set of terminals in W that are descendants of v. We define the set of mutual edges between Gp and Gg:
M(Gp, Gg) =
{(e1, e2) ∈ Ep × Eg | y(e1) = y(e2) ∧ `p(e1) = `g(e2)}
Labeled precision and recall are defined by dividing |M(Gp, Gg)| by |Ep| and |Eg|, respectively. We report two variants of this measure, one where we consider only non-remote edges, and another where we consider remote edges. We note that the measure collapses to the standard PARSEVAL constituency evaluation measure if Gp are
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Features from (Zhang and Clark, 2009):
unigrams s0te, s0we, s1te, s1we, s2te, s2we, s3te, s3we,
b0wt, b1wt, b2wt, b3wt,
s0lwe, s0rwe, s0uwe, s1lwe, s1rwe, s1uwe bigrams s0ws1w, s0ws1e, s0es1w, s0es1e, s0wb0w, s0wb0t,
s0eb0w, s0eb0t, s1wb0w, s1wb0t, s1eb0w, s1eb0t,
b0wb1w, b0wb1t, b0tb1w, b0tb1t trigrams s0es1es2w, s0es1es2e, s0es1eb0w, s0es1eb0t,
s0es1wb0w, s0es1wb0t, s0ws1es2e, s0ws1eb0t separator s0wp, s0wep, s0wq, s0wcq, s0es1ep, s0es1eq,
s1wp, s1wep, s1wq, s1weq
extended (Zhu et al., 2013) s0llwe, s0lrwe, s0luwe, s0rlwe, s0rrwe,
s0ruwe, s0ulwe, s0urwe, s0uuwe, s1llwe,
s1lrwe, s1luwe, s1rlwe, s1rrwe, s1ruwe
disco (Maier, 2015) s0xwe, s1xwe, s2xwe, s3xwe,
s0xte, s1xte, s2xte, s3xte,
s0xy, s1xy, s2xy, s3xy
s0xs1e, s0xs1w, s0xs1x, s0ws1x, s0es1x,
s0xs2e, s0xs2w, s0xs2x, s0ws2x, s0es2x,
s0ys1y, s0ys2y, s0xb0t, s0xb0w
Features from (Tokgöz and Eryiğit, 2015):
counts s0P, s0C, s0wP, s0wC, b0P, b0C, b0wP, b0wC edges s0s1, s1s0, s0b0, b0s0, s0b0e, b0s0e history a0we, a1we
remote (Novel, UCCA-specific features) s0R, s0wR, b0R, b0wR
Figure 3: Feature templates for BSP. Notation: si, bi are the ith stack and buffer items, respectively. w and t are the word form and part-of-speech tag of the terminal returned by the h∗(·) function (Section 3). e is the edge label to the node returned by the h(·) function. l, r (ll, rr) are the leftmost and rightmost (grand)children, respectively. u (uu) is the unary (grand)child, when only one exists. p is a unique separator punctuation and q is the separator count between s0 and s1. x is the gap type (“none”, “pass” or “gap”) at the sub-graph under the current node, and y is the sum of gap lengths (Maier and Lichte, 2011). P and C are the number of parents and children, respectively, and R is the number of remote children. ai is the transition taken i steps back. All feature templates correspond to binary features.
Gg are trees. Punctuation marks are excluded from the evaluation, but not from the datasets. Conversions. We explore two conversion scenarios: one into (possibly discontinuous) constituency trees, and one into CoNLL-style dependencies. In the first setting we experiment with UPARSE, the only transition-based constituency parser, to our knowledge, able to parse trees with discontinuous constituents. In the second setting we use the MaltParser with arc-standard and arceager transition sets (Nivre et al., 2007),7 and the stack LSTM-based arc-standard parser (Dyer et al., 2015). For MaltParser, we try both SVM and perceptron classifiers, reporting results obtained with SVM (about 1% F-score higher). Default settings are used in all cases. We do not use existing dependency DAG parsers since we could not obtain their code. We note that UPARSE uses beam search by default, with a beam size of 4, where the other parsers use greedy search.
Upper bounds for the conversion-based methods are computed by applying the conversion and inverse conversion on the gold standard graphs and comparing them to the original gold standard. BSP. We train BSP for 16 iterations, using MINUPDATE = 5 and IMPORTANCE = 2, doubling weight updates for gold SWAP transitions to address the sparseness of discontinuous structures, as in Maier (2015). We train BSP both with and without remote edge transitions, to allow better comparison with conversion-based methods that only predict trees.","Table 2 presents the results of our main experiments, as well as upper bounds for the conversionbased methods. BSP obtains comparable F-scores to MaltParser and UPARSE in terms of primary edges, but unlike them, is able to predict some of the remote edges too. Removing remote edge transitions from BSP does not change results considerably on the primary edges, improving them by 0.9% F-score in the in-domain setting, but reduces them by the same amount when applied out-ofdomain. Out-of-domain results are largely comparable with the in-domain results, demonstrating robustness by BSP to domain variation.
The LSTM parser obtains the highest primary
7Preliminary experiments with non-projective variants of MaltParser yielded lower scores than projective ones, and were thus discarded from the final evaluation.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
F-score, with a considerable margin. Importantly, it obtains 9.1% F-score higher than the arcstandard MaltParser, differing from it only in its classifier. This suggests that applying a similar approach to BSP is likely to improve results, and further underscores the effectiveness of transitionbased methods for BSS parsing.
The conversion to constituency format only removes remote edges, and thus obtains a perfect primary edge score. The conversion to dependency format loses considerably more information, since all non-terminal nodes are lost and have to be reconstructed by a simple rule-based inverse conversion. Both conversions yield zero scores on remote edges, since these are invariably removed when converting to trees.
Primary Remote LP LR LF LP LR LF
Constituency Tree Conversion
UPARSE 64 67.3 65.4 − 0 0 Upper Bound 100 100 100 − 0 0
Dependency Tree Conversion
Maltarc-standard 63.4 57.3 60.1 − 0 0 Maltarc-eager 63.9 57.9 60.5 − 0 0 LSTM 73.2 66.2 69.2 − 0 0 Upper Bound 93.8 83.7 88.4 − 0 0
Direct Approach BSP 62.4 56 59 15.3 11.8 13.3 BSPTree 63.8 56.5 59.9 − 0 0
Out-of-domain BSP 60.6 53.9 57.1 20.2 10.3 13.6 BSPTree 60.2 52.8 56.2 − 0 0
Table 2: Main experimental results in percents (on the Wiki test set, except for the bottom part). Columns correspond to labeled precision, recall and F-score for the different parsers, for both primary (left-hand side) and remote (right-hand side) edges. Top: results for UPARSE after conversion to constituency tree annotation. Upper middle: results for the MaltParser arc-eager and arc-standard, and the LSTM parser, after conversion to dependency tree annotation. Lower middle: results for our BSP, when trained on the complete UCCA DAGs (BSP), and when trained on UCCA trees, obtained by removing remote edges (BSPTree). Bottom: results for BSP and BSPTree when tested on out-of-domain data (20K Leagues).
Feature Ablation. To evaluate the relative impact of the different feature sets on BSP, we remove a set of features at a time, and evaluate the resulting parser on the development set (Table 3). Almost all feature sets have a positive contribution to the primary edge F-score, or otherwise to the prediction of remote edges. unigrams and bigrams features are especially important, and the ratio feature greatly improves recall on primary edges. disco features have a positive contribution,
likely to be amplified in languages with a higher percentage of discontinuous units, e.g. German.","In order for a grounded semantic representation to cover the full range of semantic structures exhibited by natural language, there are three structural properties that should be supported. The first is multiple parents, representing arguments and relations (semantic units) that are shared between predicates. For instance, in the sentence “After graduation, John moved to Paris”, “John” is an argument of both “graduation” and “moved”, yielding a DAG structure (Figure 1a), rather than a tree.
The second is non-terminal nodes for representing units comprising more than one word. While bi-lexical dependencies partially circumvent this requirement, by representing complex units in terms of their headwords, they fall short when representing units that have no clear head.
Frequent examples of such constructions include coordination structures (e.g., “John and Mary went home”; Figure 1b), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases. In these cases, dependency schemes often apply some convention selecting one of the sub-units as the head, but as different head selections are needed for different purposes, standardization problems arise (Ivanova et al., 2012). For example, selecting the preposition to head prepositional phrases yields better parsing results (Schwartz et al., 2012), while the head noun is more useful for information extraction.
Third, semantic units may be discontinuous in the text. For instance, in “John gave everything up” (Figure 1c), the phrasal verb “gave ... up” forms a single semantic unit. Discontinuities are also pervasive with other multi-word expressions (Schneider et al., 2014). We call formal representations supporting all three properties Broadcoverage Semantic Structures (BSS).
However, to our knowledge, no existing parser for a grounded semantic annotation scheme supports the combination of these criteria. The only such scheme supporting them is UCCA (Abend and Rappoport, 2013), which has no parser. Several other models either support some of these properties (Oepen et al., 2015), or avoid grounding semantic units altogether (notably, AMR (Banarescu et al., 2013); see Section 2).
In this work we are first in proposing techniques for BSS parsing. We adopt a transition-based approach, which has recently produced some of the best results in syntactic dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015), and has also demonstrated strong performance in a variety of other semantic and syntactic settings (Maier, 2015; Wang et al., 2015, among others). Transition-based methods are a natural starting point for UCCA parsing, as the set of distinctions
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
(a) After
L
graduation P
H
, U
John
A
moved
P
to R
Paris
C
A
H
A
(b)
John
C
and
N
Mary
C
A
went
P
home
A
(c) John
A
gave
C
everything up
C
P
A
Figure 1: Semantic representation of the three structural properties required for BSS, according to the UCCA scheme. (a) includes a remote edge (dashed), resulting in “John” having two parents. (b) includes a coordination construction (“John and Mary”). (c) includes a discontinuous unit (“gave ... up”). Legend: P – a Scene’s main relation, A – participant, L – inter-scene linker, H – linked Scene, C – center, R – relator, N – connector, U – punctuation, F – function unit. Pre-terminal nodes are omitted for brevity.
it represents, centered around predicate-argument structures and their inter-relations, is similar to distinctions conveyed by dependency schemes.
We pursue two complementary parsing strategies. First, we assess the ability of existing technology to tackle the task, by developing conversion protocols between UCCA structures and two related formalisms: dependency trees and discontinuous constituency trees. As these formalisms are more restrictive than BSS, the conversion is necessarily lossy. Nonetheless, we find that it is effective in practice (Section 3). Second, we present a novel transition-based broadcoverage parser, Broad-coverage Semantic Parser (BSP; Section 4) supporting multiple parents, nonterminal nodes and discontinuous units, based on extending existing transition-based parsers with new transitions and features.
We experiment with the English UCCAannotated corpora (Abend and Rappoport, 2013) as a test case, in both in-domain and out-ofdomain scenarios, reaching nearly 70% labeled Fscore for the highest scoring parser. The results suggest concrete paths for further improvement. All converters and parsers will be made publicly available upon publication.",,,"We have introduced the first parser that supports multiple parents, non-terminal nodes and discontinuous units. We further explored a conversionbased parsing approach to assess the ability of existing technology to address the task. The work makes a further contribution by first experimenting on UCCA parsing. Results show that UCCA can be parsed with 69.2% primary F-score, and suggest means for improvement by taking an LSTM-based approach to the local classifier of BSP. The quality of the results is underscored by UCCA’s inter-annotator agreement (often taken as an upper bound) of 80–85% F-score on primary edges (Abend and Rappoport, 2013).
While much recent work focused on semantic parsing of different types, the relations between the different representations have not been clarified. We intend to further explore conversionbased parsing approaches, including different target representations and more sophisticated conversion procedures (Kong et al., 2015), to shed light on the commonalities and differences between representations, suggesting ways to design better semantic representations. We believe that UCCA’s merits in providing a cross-linguistically applicable, broad-coverage annotation will support ongoing efforts to incorporate deeper semantic structures into a variety of applications, such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015).
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
7,"The representation of many common semantic phenomena requires structural properties beyond those commonly used for syntactic parsing.  We discuss a set of structural properties required for broad-coverage semantic representation, and note that existing parsers support some of these properties, but not all. We propose two transition-based techniques for parsing such semantic structures: (1) applying conversion procedures to map them into related formalisms, and using existing state-of-the-art parsers on the converted representations; and (2) constructing a parser that directly supports the full set of properties. We experiment with UCCA-annotated corpora, the only ones with all these structural semantic properties. Results demonstrate the effectiveness of transition-based methods for the task.",Broad-Coverage Semantic Parsing: A Transition-Based Approach,18,"The paper presents the first broad-coverage semantic parsers for UCCA, one
specific approach to graph-based semantic representations. Unlike CoNLL
semantic dependency graphs, UCCA graphs can contain ""nonterminal"" nodes which
do not represent words in the string. Unlike AMRs, UCCA graphs are ""grounded"",
which the authors take to mean that the text tokens appear as nodes in the
semantic representation. The authors present a number of parsing methods,
including a transition-based parser that directly constructs UCCA parses, and
evaluate them.

Given that UCCA and UCCA-annotated data exist, it seems reasonable to develop a
semantic parser for UCCA. However, the introduction and background section hit
a wrong note to my ear, in that they seem to argue that UCCA is the _only_
graph-based semantic representation (SR) formalism that makes sense to be
studied. This doesn't work for me, and also seems unnecessary -- a good UCCA
parser could be a nice contribution by itself.

I do not entirely agree with the three criteria for semantic representation
formalisms the authors lay out in the introduction. For instance, it is not
clear to me that ""nonterminal nodes"" contribute any expressive capacity. Sure,
it can be inconvenient to have to decide which word is the head of a
coordinated structure, but exactly what information is it that could only be
represented with a nonterminal and not e.g. with more informative edge labels?
Also, the question of discontinuity does not even arise in SRs that are not
""grounded"". The advantages of ""grounded"" representations over AMR-style ones
did not become clear to me. I also think that the word ""grounded"" has been used
for enough different concepts in semantics in the past ten years, and would
encourage the authors to find a different one (""anchored""? ""lexicalized""?).
Thus I feel that the entire introductory part of the paper should be phrased
and argued much more carefully.

The parser itself seems fine, although I did not check the details. However, I
did not find the evaluation results very impressive. On the ""primary"" edges,
even a straightforward MaltParser outperforms the BSP parser presented here,
and the f-scores on the ""remote"" edges (which a dependency-tree parser like
Malt cannot compute directly) are not very high either. Furthermore, the
conversion of dependency graphs to dependency trees has been studied quite a
bit under the name ""tree approximations"" in the context of the CoNLL 2014 and
2015 shared tasks on semantic dependency parsing (albeit without ""nonterminal""
nodes). Several authors have proposed methods for reconstructing the edges that
were deleted in the graph-to-tree conversion; for instance, Agic et al. (2015),
""Semantic dependency graph parsing using tree approximations"" discuss the
issues involved in this reconstruction in detail. By incorporating such
methods, it is likely that the f-score of the MaltParser (and the LSTM-based
MaltParser!) could be improved further, and the strength of the BSP parser
becomes even less clear to me.",,2,4,Oral Presentation,3,2,2,4,3,5,3,3,2016,"Broad-coverage Semantic Representation. While earlier work on grounded semantic parsing has mostly concentrated on shallow semantic analysis, focusing on semantic role labeling of verbal argument structures, the focus has recently shifted to parsing of more elaborate representations that account for a wider range of phenomena. Most closely related to this work is Broad-coverage Semantic Dependency Parsing (SDP), addressed in two SemEval tasks (Oepen et al., 2014; Oepen et al., 2015), experimenting with the Prague tectogrammatical layer (Sgall et al., 1986; Böhmová et al., 2003), and with dependencies derived from the Enju parser,1 and Lingo ERG (Flickinger, 2002). Like BSS parsing, SDP addresses a wide range of semantic phenomena, and supports discontinuous units and multiple parents. However, SDP uses bi-lexical dependencies, disallowing non-terminal nodes, and thus faces difficulties in supporting structures that have no clear head, such as coordination (Ivanova et al., 2012).
Another line of work addresses parsing into non-grounded2 semantic representation, notably Abstract Meaning Representation (AMR) (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). While sharing much of this work’s motivation, not grounding the representation in the text complicates the parsing task, as it requires that the alignment between words and logical symbols be automatically (and imprecisely) detected.3 Furthermore, grounding allows breaking down sentences into semantically meaningful sub-spans, which is useful for many applications (see discussion in Fernández-González and Martins (2015)). Wang et al. (2015) applied a transition-based approach to AMR parsing. Their method involved first syntactically parsing the input, and then converting the result into AMR. Other approaches for semantic representation, such as MRS (Copestake et al., 2005) and DRT (Kamp et al., 2011), involve considerably different representation and parsing approaches, and so fall beyond the scope of our discussion.
1See http://kmcs.nii.ac.jp/enju 2By grounded we mean the text tokens are directly annotated as part of the representation, as opposed to abstract formalisms approximating logical form, for example.
3Considerable technical effort has been invested in the AMR alignment task under various approaches (Flanigan et al., 2014; Pourdamghani et al., 2014; Pust et al., 2015).
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
The UCCA Annotation Scheme. Universal Cognitive Conceptual Annotation (UCCA) is a cross-linguistically applicable semantic representation scheme, that builds on the established “Basic Linguistic Theory” framework for typological description (Dixon, 2010a; Dixon, 2010b; Dixon, 2012), and on the Cognitive Linguistics literature. UCCA is a multi-layered representation, where each layer corresponds to a “module” of semantic distinctions (e.g., predicate-argument structures, adverbials, coreference etc.).
Formally, a UCCA structure over a sentence is a DAG, whose leaves correspond to the sentence’s words. The nodes of the graph, or its “units”, are either terminals or several sub-units (not necessarily contiguous) jointly viewed as a single entity according to some semantic or cognitive consideration. Edges bear a category, indicating the role of the sub-unit in the relation that the parent represents. UCCA structures support all three criteria of BSS: multiple parents, non-terminal nodes, and discontinuous units.
UCCA’s foundational layer, which we use here, covers the predicate-argument structures evoked by predicates of all grammatical categories (verbal, nominal, adjectival and others), the interrelations between them, as well as other major linguistic phenomena such as coordination and multi-word expressions. This set of categories has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation, and semantic stability in translation: UCCA annotations of translated text usually contain the same set of relationships (Sulem et al., 2015). This finding supports the claim that UCCA represents an abstract level of semantics, shared by different languages.
The layer’s basic notion is the Scene, which describes a movement, action or state. Each Scene contains one Main Relation, as well as one or more Participants. For example, the sentence “After graduation, John moved to Paris” contains two Scenes, whose main relations are “graduation” and “moved”. “John” is a Participant in both Scenes, while “Paris” only in the latter. UCCA marks one of the incoming edges for each non-root as “primary” and the others as “remote” edges. The two Scenes in this sentence are both arguments of the Linker “After”, which in this case expresses a temporal relation between them. Figure 1 presents the UCCA annotation of this and other examples.
Further categories account for relations between Scenes and the internal structures of complex arguments (e.g., coordination) and relations (e.g., complex adverbials, such a “very clearly”). UCCA graphs may contain implicit units that have no correspondent in the text, but the parsing of these units is deferred to future work, as it is likely to require different methods than those explored here (Roth and Frank, 2015).We begin by assessing the ability of existing technology to address the task, by taking a conversionbased approach. Training proceeds by converting BSS into a different representation, and training an existing parser on the converted structures. We evaluate the trained parsers by applying them to the test set, and converting the results back to BSS, where they are compared with the gold standard. The error resulting from this back and forth conversion is discussed in Section 6. Notation. Let L be the set of possible edge labels. A BSS is a directed acyclic graph G = (V,E, `) over a sequence of tokens w1, . . . , wn, where ` : E → L is a function of edge labels. For each token wi (i = 1, . . . , n), there exists a leaf (or a terminal) ti ∈ V . Conversion to Constituency Trees. We convert BSS to constituency trees by removing a subset of the edges.4 Specifically, when converting UCCA structures, we simply remove all remote edges, leaving only primary edges, which form a tree structure (see Figure 1a). The inverse conversion from trees to BSS is simply the identity function, as every constituency tree is a BSS. Conversion to Dependency Trees. In the conversion to dependency trees, we first convert BSS to constituency trees using the above procedure, and then convert the result to dependency trees. Assume Tc = (Vc, Ec, `c) is a constituency tree with labels `c : Ec → L, where L is the set of possible labels. The conversion from Tc to a dependency tree involves the removal of all nonterminals from Tc and the addition of edges between terminals. The nodes of the converted dependency tree are simply the terminals of Tc.
We define a linear order over possible edge labels L. For each node u ∈ V , denote with
4For trees, labeling nodes is equivalent to labeling edges. Thus, we do not distinguish between the two options. Note also that as the original structures may contain discontinuities, so may the resulting trees.
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
Data: constituency tree Tc = (Vc, Ec, `c) Result: dependency tree Td = (Vd, Ed, `d) foreach u ∈ Vc do
h(u)← argminv Priority(`c(u, v)); end Vd ← Terminals(Tc), Ed ← ∅; foreach t ∈ Vd do
u← t; while u = h(Parentc(u)) do
h∗(u)← t; u← Parentc(u);
end n(t)← h(u); end foreach t ∈ Vd do
u← Parentc(n(t)); t′ ← h∗(u); Ed ← Ed ∪ {(t′, t)}; `d(t
′, t)← `c(u, n(t))}; end
Algorithm 1: Constituency to dependency conversion pro-
cedure.
h(u) its child with the highest edge label. Denote with h∗(u) the terminal reached by recursively applying h(·) over u. For each terminal t, we define n(t) as the highest non-terminal such that t = h∗(n(t)), i.e., n(t) is the only node such that t = h∗(n(t)) and t 6= h∗(Parentc(n(t))). The head of t according to the dependency graph is the terminal h∗(Parentc(n(t))). The complete conversion procedure from constituency to dependency is given in Algorithm 1.
We note that this conversion procedure is simpler than the head percolation procedure used for converting syntactic constituency trees to dependency trees. This is because h(u) of a node u (similar to u’s head-containing child), depends only on the label of the edge (h(u), u), and not on the subtree spanned by u, because edge labels in UCCA directly express the role of the child in the parent unit, and are thus sufficient for determining which of u’s children contains the head node.
The inverse conversion introduces non-terminal nodes back into the tree. As the distinction between low- and high-attaching nodes is lost in the constituency to dependency conversion, we heuristically assume that attachments are always high-attaching. Assume Td = (Vd, Ed, `d) is a dependency tree. We begin by creating a root node r. Then, iterating over Vd in topological order, we add its members as terminals to the constituency tree and create a pre-terminal parent for each, with an edge labeled as Terminal between them. The parents of the pre-terminals are determined by the terminal’s parent in the dependency tree: if a de-
Data: dependency tree Td = (Vd, Ed, `d) Result: constituency tree Tc = (Vc, Ec, `c) r ← Node(); Vc ← {r}, Ec ← ∅; foreach t ∈ TopologicalSort(Vd) do
u← Node(); Vc ← Vc ∪ {u, t}; Ec ← Ec ∪ {(u, t)}; `c(u, t)← Terminal ; t′ ← Parentd(t); if t′ = ROOT then
Ec ← Ec ∪ {(r, u)}; `c(r, u)← Label(r);
else if ∃v ∈ Vd : (t, v) ∈ Ed then
u′ ← Node(); Ec ← Ec ∪ {(u′, u)}; `c(u
′, u)← Label(u′); else
u′ ← u; end p← Parentc(t′); Ec ← Ec ∪ {(p, u′)}; `c(p, u
′)← `d(t′, t); end
end Algorithm 2: Dependency to constituency conversion pro-
cedure.
pendency node t is a child of the root in Td, then t’s pre-terminal will also be a child of the root node. Otherwise, t’s pre-terminal is the child of the preterminal associated with t’s head in Td. We add an intermediate node in between if t has any dependents in Td, to allow adding their pre-terminals as children. Edge labels for the intermediate edges are determined by a rule-based function, denoted by Label(u).5 In practice, it mostly selects the UCCA label Center. This conversion procedure is given in Algorithm 2.We now turn to presenting BSP, a transition-based parser that supports the three criteria of broadcoverage semantic structures.
Transition-based parsing (Nivre, 2003) creates the parse as it scans the text from left to right. The parse is created incrementally by applying a transition at each step to the parser state, defined using three data structures: a buffer B of tokens and nodes to be processed, a stack S of nodes currently being processed, and a graph G = (V,E, `) of constructed nodes and labeled edges. Some of the states are marked as terminal, meaning that G is the final output. A classifier is used at each step
5See Supplementary Material for the definition of Label.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
to select the next transition based on features that encode the parser’s current state. During training, an oracle creates training instances for the classifier, based on the gold-standard annotation.
Despite being based on local decisions, transition-based methods have yielded excellent results in a variety of parsing tasks. Within syntactic dependency parsing, transition-based methods have been successfully applied to corpora in many languages and domains, yielding some of the best reported results (Dyer et al., 2015; Ballesteros et al., 2015). The approach has also yielded results comparable with the state-of-the-art in constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al., 2015) and AMR parsing (Wang et al., 2015).
BSP mostly builds on recent advances in discontinuous constituency and dependency DAG parsing techniques, and further introduces novel UCCA-oriented features for parsing BSS.
Transition Set. Given a sequence of tokens w1, . . . , wn, we predict a BSS G whose leaves correspond to the tokens. Parsing starts with a single node on the stack (the root node), and the input tokens w1, . . . , wn in the buffer. The set of transitions is given in Figure 2. In addition to the standard SHIFT and REDUCE operations, we follow previous work in transition-based constituency parsing (Sagae and Lavie, 2005), and include the NODE transition for creating new nonterminal nodes. Concretely, NODEX creates a new node on the buffer as a parent of the first element on the stack, with an X-labeled edge.
LEFT-EDGEX and RIGHT-EDGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. As a UCCA node may only have one incoming primary edge, EDGE transitions are disallowed where the child node already has an incoming primary edge. LEFTREMOTEX and RIGHT-REMOTEX do not have this restriction, and the created edge is marked as remote. We distinguish between these two pairs of transitions, for the parser to be able to determine whether an edge is a primary or a remote one. In order to support the prediction of multiple parents, node and edge transitions do not automatically ap-
ply REDUCE. This is in line with other work on transition-based DAG dependency parsing (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015). Once all edges for a particular node have been created, it is removed from the stack by applying REDUCE.
SWAP allows handling discontinuous nodes, by popping the second node on the stack and adding it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015). Finally, FINISH pops the root node and marks the state as terminal.
Features. Figure 3 presents the feature templates used by the parser. For some of the features, we used the notion of head word, defined by the h∗(·) function (Section 3). While head words are not explicitly represented in the UCCA scheme, these features proved useful as means of encoding word-to-word relations.
In addition to the binary features defined by the feature templates, we employ a real-valued feature, ratio, corresponding to the ratio between the number of terminals to number of nodes in the graph G. This novel feature serves as a regularizer for the creation of new nodes, and should be beneficial for other transition-based constituency parsers too. Features are generally adapted from the related parsers of (Zhang and Clark, 2009; Zhu et al., 2013; Tokgöz and Eryiğit, 2015; Maier, 2015), with a small additional set of features encoding relevant information for the novel LEFTREMOTEX and RIGHT-REMOTEX transitions.
Training. Following Maier (2015), we use a linear classifier, using the averaged structured perceptron algorithm for training it (Collins and Roark, 2004) with the MINUPDATE (Goldberg and Elhadad, 2011) procedure: a minimum number of updates to a feature has to occur in training for it to be included in the model. Inference is performed greedily (i.e., without beam search).
For training the local classifier, we use a dynamic oracle (Goldberg and Nivre, 2012), i.e., an oracle that outputs a set of optimal transitions: when applied to the current parser state, the gold standard graph is reachable from the resulting state. For example, the oracle would predict a NODE transition if the stack has on its top a parent in the gold graph that has not been created, but would predict a RIGHT-EDGE transition if the second stack element is a parent of the first element according to the gold graph and the edge between them has not been created. The transition
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Initial State Final State Stack Buffer Nodes Edges Terminal? Stack Buffer Nodes Edges Terminal? [root] w1:n {root} ∪
w1:n
∅ − ∅ ∅ V E +
Before Transition Transition After Transition Condition Stack Buffer Nodes Edges Stack Buffer Nodes Edges Terminal? S x | B V E SHIFT S | x B V E − S | x B V E REDUCE S B V E − S | x B V E NODEX S | x y | B V ∪ {y} E ∪ {(y, x)X} − x 6= root S | y, x B V E LEFT-EDGEX S | y, x B V E ∪ {(x, y)X} − 
x 6∈ w1:n, y 6= root, y 6;G x
S | x, y B V E RIGHT-EDGEX S | x, y B V E ∪ {(x, y)X} − S | y, x B V E LEFT-REMOTEX S | y, x B V E ∪ {(x, y)∗X} − S | x, y B V E RIGHT-REMOTEX S | x, y B V E ∪ {(x, y)∗X} − S | x, y B V E SWAP S | y x | B V E − i(x) < i(y) [root] ∅ V E FINISH ∅ ∅ V E +
Figure 2: The transition set of BSP. We write the stack with its top to the right and the buffer with its head to the left. (·, ·)X denotes a primary X-labeled edge, and (·, ·)∗X a remote X-labeled edge. i(x) is a running index for the created nodes. EDGE transitions have an additional condition: the prospective child may not already have a primary parent.
predicted by the classifier is deemed correct and is applied to the parser state to reach the subsequent state, if the transition is included in the set of optimal transitions. Otherwise, a random optimal transition is applied and the classifier’s weights are updated according to the perceptron update rule.Data. We conduct our main experiments on the UCCA Wikipedia corpus (henceforth, Wiki), and use the English part of the UCCA Twenty Thousand Leagues Under the Sea English-French parallel corpus (henceforth, 20K Leagues) as out-ofdomain data.6 Table 1 presents some statistics for the two corpora, demonstrating that while the Wiki corpus is over ten times larger, the overall statistics are similar. We use passages of indices up to 655 of the Wiki corpus as our training set, passages 656–700 as development set, and passages 701– 695 as in-domain test set. While UCCA edges can cross sentence boundaries, we adhere to the common practice in semantic parsing and train our parsers on individual sentences, discarding interrelations between them (0.18% of the edges). We also discard linkage nodes and edges, as they often express inter-sentence relations and are thus mostly redundant when applied at the sentence level, as well as implicit nodes (Section 2). In the out-of-domain experiments, we apply the same parser (trained on the Wiki corpus) to the 20K Leagues corpus without re-tuning the parameters. Evaluation. Since there are no standard evaluation measures for BSS, we define two simple
6Both are available at http://www.cs.huji.ac. il/˜oabend/ucca.html
measures for comparing such structures. Assume Gp = (Vp, Ep, `p) and Gg = (Vg, Eg, `g) are the predicted and gold-standard DAGs over the same sequence of terminals W = {w1, . . . , wn}, respectively. For an edge e = (u, v) in either graph, where u is the parent and v is the child, define its yield y(e) ⊆ W as the set of terminals in W that are descendants of v. We define the set of mutual edges between Gp and Gg:
M(Gp, Gg) =
{(e1, e2) ∈ Ep × Eg | y(e1) = y(e2) ∧ `p(e1) = `g(e2)}
Labeled precision and recall are defined by dividing |M(Gp, Gg)| by |Ep| and |Eg|, respectively. We report two variants of this measure, one where we consider only non-remote edges, and another where we consider remote edges. We note that the measure collapses to the standard PARSEVAL constituency evaluation measure if Gp are
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Features from (Zhang and Clark, 2009):
unigrams s0te, s0we, s1te, s1we, s2te, s2we, s3te, s3we,
b0wt, b1wt, b2wt, b3wt,
s0lwe, s0rwe, s0uwe, s1lwe, s1rwe, s1uwe bigrams s0ws1w, s0ws1e, s0es1w, s0es1e, s0wb0w, s0wb0t,
s0eb0w, s0eb0t, s1wb0w, s1wb0t, s1eb0w, s1eb0t,
b0wb1w, b0wb1t, b0tb1w, b0tb1t trigrams s0es1es2w, s0es1es2e, s0es1eb0w, s0es1eb0t,
s0es1wb0w, s0es1wb0t, s0ws1es2e, s0ws1eb0t separator s0wp, s0wep, s0wq, s0wcq, s0es1ep, s0es1eq,
s1wp, s1wep, s1wq, s1weq
extended (Zhu et al., 2013) s0llwe, s0lrwe, s0luwe, s0rlwe, s0rrwe,
s0ruwe, s0ulwe, s0urwe, s0uuwe, s1llwe,
s1lrwe, s1luwe, s1rlwe, s1rrwe, s1ruwe
disco (Maier, 2015) s0xwe, s1xwe, s2xwe, s3xwe,
s0xte, s1xte, s2xte, s3xte,
s0xy, s1xy, s2xy, s3xy
s0xs1e, s0xs1w, s0xs1x, s0ws1x, s0es1x,
s0xs2e, s0xs2w, s0xs2x, s0ws2x, s0es2x,
s0ys1y, s0ys2y, s0xb0t, s0xb0w
Features from (Tokgöz and Eryiğit, 2015):
counts s0P, s0C, s0wP, s0wC, b0P, b0C, b0wP, b0wC edges s0s1, s1s0, s0b0, b0s0, s0b0e, b0s0e history a0we, a1we
remote (Novel, UCCA-specific features) s0R, s0wR, b0R, b0wR
Figure 3: Feature templates for BSP. Notation: si, bi are the ith stack and buffer items, respectively. w and t are the word form and part-of-speech tag of the terminal returned by the h∗(·) function (Section 3). e is the edge label to the node returned by the h(·) function. l, r (ll, rr) are the leftmost and rightmost (grand)children, respectively. u (uu) is the unary (grand)child, when only one exists. p is a unique separator punctuation and q is the separator count between s0 and s1. x is the gap type (“none”, “pass” or “gap”) at the sub-graph under the current node, and y is the sum of gap lengths (Maier and Lichte, 2011). P and C are the number of parents and children, respectively, and R is the number of remote children. ai is the transition taken i steps back. All feature templates correspond to binary features.
Gg are trees. Punctuation marks are excluded from the evaluation, but not from the datasets. Conversions. We explore two conversion scenarios: one into (possibly discontinuous) constituency trees, and one into CoNLL-style dependencies. In the first setting we experiment with UPARSE, the only transition-based constituency parser, to our knowledge, able to parse trees with discontinuous constituents. In the second setting we use the MaltParser with arc-standard and arceager transition sets (Nivre et al., 2007),7 and the stack LSTM-based arc-standard parser (Dyer et al., 2015). For MaltParser, we try both SVM and perceptron classifiers, reporting results obtained with SVM (about 1% F-score higher). Default settings are used in all cases. We do not use existing dependency DAG parsers since we could not obtain their code. We note that UPARSE uses beam search by default, with a beam size of 4, where the other parsers use greedy search.
Upper bounds for the conversion-based methods are computed by applying the conversion and inverse conversion on the gold standard graphs and comparing them to the original gold standard. BSP. We train BSP for 16 iterations, using MINUPDATE = 5 and IMPORTANCE = 2, doubling weight updates for gold SWAP transitions to address the sparseness of discontinuous structures, as in Maier (2015). We train BSP both with and without remote edge transitions, to allow better comparison with conversion-based methods that only predict trees.","Table 2 presents the results of our main experiments, as well as upper bounds for the conversionbased methods. BSP obtains comparable F-scores to MaltParser and UPARSE in terms of primary edges, but unlike them, is able to predict some of the remote edges too. Removing remote edge transitions from BSP does not change results considerably on the primary edges, improving them by 0.9% F-score in the in-domain setting, but reduces them by the same amount when applied out-ofdomain. Out-of-domain results are largely comparable with the in-domain results, demonstrating robustness by BSP to domain variation.
The LSTM parser obtains the highest primary
7Preliminary experiments with non-projective variants of MaltParser yielded lower scores than projective ones, and were thus discarded from the final evaluation.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
F-score, with a considerable margin. Importantly, it obtains 9.1% F-score higher than the arcstandard MaltParser, differing from it only in its classifier. This suggests that applying a similar approach to BSP is likely to improve results, and further underscores the effectiveness of transitionbased methods for BSS parsing.
The conversion to constituency format only removes remote edges, and thus obtains a perfect primary edge score. The conversion to dependency format loses considerably more information, since all non-terminal nodes are lost and have to be reconstructed by a simple rule-based inverse conversion. Both conversions yield zero scores on remote edges, since these are invariably removed when converting to trees.
Primary Remote LP LR LF LP LR LF
Constituency Tree Conversion
UPARSE 64 67.3 65.4 − 0 0 Upper Bound 100 100 100 − 0 0
Dependency Tree Conversion
Maltarc-standard 63.4 57.3 60.1 − 0 0 Maltarc-eager 63.9 57.9 60.5 − 0 0 LSTM 73.2 66.2 69.2 − 0 0 Upper Bound 93.8 83.7 88.4 − 0 0
Direct Approach BSP 62.4 56 59 15.3 11.8 13.3 BSPTree 63.8 56.5 59.9 − 0 0
Out-of-domain BSP 60.6 53.9 57.1 20.2 10.3 13.6 BSPTree 60.2 52.8 56.2 − 0 0
Table 2: Main experimental results in percents (on the Wiki test set, except for the bottom part). Columns correspond to labeled precision, recall and F-score for the different parsers, for both primary (left-hand side) and remote (right-hand side) edges. Top: results for UPARSE after conversion to constituency tree annotation. Upper middle: results for the MaltParser arc-eager and arc-standard, and the LSTM parser, after conversion to dependency tree annotation. Lower middle: results for our BSP, when trained on the complete UCCA DAGs (BSP), and when trained on UCCA trees, obtained by removing remote edges (BSPTree). Bottom: results for BSP and BSPTree when tested on out-of-domain data (20K Leagues).
Feature Ablation. To evaluate the relative impact of the different feature sets on BSP, we remove a set of features at a time, and evaluate the resulting parser on the development set (Table 3). Almost all feature sets have a positive contribution to the primary edge F-score, or otherwise to the prediction of remote edges. unigrams and bigrams features are especially important, and the ratio feature greatly improves recall on primary edges. disco features have a positive contribution,
likely to be amplified in languages with a higher percentage of discontinuous units, e.g. German.","In order for a grounded semantic representation to cover the full range of semantic structures exhibited by natural language, there are three structural properties that should be supported. The first is multiple parents, representing arguments and relations (semantic units) that are shared between predicates. For instance, in the sentence “After graduation, John moved to Paris”, “John” is an argument of both “graduation” and “moved”, yielding a DAG structure (Figure 1a), rather than a tree.
The second is non-terminal nodes for representing units comprising more than one word. While bi-lexical dependencies partially circumvent this requirement, by representing complex units in terms of their headwords, they fall short when representing units that have no clear head.
Frequent examples of such constructions include coordination structures (e.g., “John and Mary went home”; Figure 1b), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases. In these cases, dependency schemes often apply some convention selecting one of the sub-units as the head, but as different head selections are needed for different purposes, standardization problems arise (Ivanova et al., 2012). For example, selecting the preposition to head prepositional phrases yields better parsing results (Schwartz et al., 2012), while the head noun is more useful for information extraction.
Third, semantic units may be discontinuous in the text. For instance, in “John gave everything up” (Figure 1c), the phrasal verb “gave ... up” forms a single semantic unit. Discontinuities are also pervasive with other multi-word expressions (Schneider et al., 2014). We call formal representations supporting all three properties Broadcoverage Semantic Structures (BSS).
However, to our knowledge, no existing parser for a grounded semantic annotation scheme supports the combination of these criteria. The only such scheme supporting them is UCCA (Abend and Rappoport, 2013), which has no parser. Several other models either support some of these properties (Oepen et al., 2015), or avoid grounding semantic units altogether (notably, AMR (Banarescu et al., 2013); see Section 2).
In this work we are first in proposing techniques for BSS parsing. We adopt a transition-based approach, which has recently produced some of the best results in syntactic dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015), and has also demonstrated strong performance in a variety of other semantic and syntactic settings (Maier, 2015; Wang et al., 2015, among others). Transition-based methods are a natural starting point for UCCA parsing, as the set of distinctions
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
(a) After
L
graduation P
H
, U
John
A
moved
P
to R
Paris
C
A
H
A
(b)
John
C
and
N
Mary
C
A
went
P
home
A
(c) John
A
gave
C
everything up
C
P
A
Figure 1: Semantic representation of the three structural properties required for BSS, according to the UCCA scheme. (a) includes a remote edge (dashed), resulting in “John” having two parents. (b) includes a coordination construction (“John and Mary”). (c) includes a discontinuous unit (“gave ... up”). Legend: P – a Scene’s main relation, A – participant, L – inter-scene linker, H – linked Scene, C – center, R – relator, N – connector, U – punctuation, F – function unit. Pre-terminal nodes are omitted for brevity.
it represents, centered around predicate-argument structures and their inter-relations, is similar to distinctions conveyed by dependency schemes.
We pursue two complementary parsing strategies. First, we assess the ability of existing technology to tackle the task, by developing conversion protocols between UCCA structures and two related formalisms: dependency trees and discontinuous constituency trees. As these formalisms are more restrictive than BSS, the conversion is necessarily lossy. Nonetheless, we find that it is effective in practice (Section 3). Second, we present a novel transition-based broadcoverage parser, Broad-coverage Semantic Parser (BSP; Section 4) supporting multiple parents, nonterminal nodes and discontinuous units, based on extending existing transition-based parsers with new transitions and features.
We experiment with the English UCCAannotated corpora (Abend and Rappoport, 2013) as a test case, in both in-domain and out-ofdomain scenarios, reaching nearly 70% labeled Fscore for the highest scoring parser. The results suggest concrete paths for further improvement. All converters and parsers will be made publicly available upon publication.",,,"We have introduced the first parser that supports multiple parents, non-terminal nodes and discontinuous units. We further explored a conversionbased parsing approach to assess the ability of existing technology to address the task. The work makes a further contribution by first experimenting on UCCA parsing. Results show that UCCA can be parsed with 69.2% primary F-score, and suggest means for improvement by taking an LSTM-based approach to the local classifier of BSP. The quality of the results is underscored by UCCA’s inter-annotator agreement (often taken as an upper bound) of 80–85% F-score on primary edges (Abend and Rappoport, 2013).
While much recent work focused on semantic parsing of different types, the relations between the different representations have not been clarified. We intend to further explore conversionbased parsing approaches, including different target representations and more sophisticated conversion procedures (Kong et al., 2015), to shed light on the commonalities and differences between representations, suggesting ways to design better semantic representations. We believe that UCCA’s merits in providing a cross-linguistically applicable, broad-coverage annotation will support ongoing efforts to incorporate deeper semantic structures into a variety of applications, such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015).
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
8,"We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008--9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.","Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs",66,"This paper presents a Stack LSTM parser based on the work of Henderson et al.
(2008, 2013) on joint syntactic/semantic transition-based parsing and Dyer et
al. (2015) on stack LSTM syntactic parsing. The use of the transition system
from the former and the stack LSTM from the latter shows interesting results
compared to the joint systems on the CoNLL 2008 and 2009 shared tasks.

I like this paper a lot because it is well-written, well-explained, the related
work is good and the results are very interesting. The methodology is sound
(with a minor concern regarding the Chinese embeddings, leading me to believe
than very good embeddings can be more informative than a very clever model...).

Moreover, the description of the system is clear, the hyperparameters are
justified and the discussion is interesting.

The only thing I would say is that the proposed system lacks originality in the
sense that the work of Henderson et al. puts the basis of semi-synchronised
joint syntax-semantic transition-based parsing several years ago and Dyer et
al. came up with the stack LSTM last year, so it is not a new method, per say.
But in my opinion, we were waiting for such a parser to be designed and so I'm
glad it was done here.",,5,4,Oral Presentation,5,5,5,5,5,5,4,3,2016,"We largely follow the transition-based, synchronized algorithm of Henderson et al. (2013) to predict joint parse structures. The input to the algorithm is a sentence annotated with part-of-speech tags. The output consists of a labeled syntactic dependency tree and a directed SRL graph, in which a subset of words in the sentence are selected as predicates, disambiguated to a sense, and linked by labeled, directed edges to their semantic arguments and modifiers. Figure 1 shows an example.The two parses are constructed in a bottom-up fashion, incrementally processing words in the sentence from left to right. The state of the parsing algorithm at timestep t is represented by three stack data structures: a syntactic stack St, a semantic stack Mt—each containing partially built structures—and a buffer of input words Bt. Our algorithm also places partial syntactic and semantic parse structures onto the front of the buffer, so it is also implemented as a stack. Each arc in the output corresponds to a transition (or “action”) chosen based on the current state; every transition modifies the state by updating St, Mt, and Bt to St+1, Mt+1, and Bt+1, respectively. While each state may license several valid actions, each action has a deterministic effect on the state of the algorithm.
Initially, S0 and M0 are empty, and B0 contains the input sentence with the first word at the front of B and a special root symbol at the end.1 Execution ends on iteration t such that Bt is empty and St and Mt contain only a single structure headed by root.There are separate sets of syntactic and semantic transitions; the former manipulate S and B, the latter M and B. All are formally defined in Table 1. The syntactic transitions are from the “arceager” algorithm of Nivre (2008). They include:
• S-SHIFT, which copies2 an item from the front of B and pushes it on S. • S-REDUCE pops an item from S. • S-RIGHT(`) creates a syntactic dependency.
Let u be the element at the top of S and v be the element at the front of B. The new dependency has u as head, v as dependent, and label `. u is popped off S, and the resulting structure, rooted at u, is pushed on S. Finally, v is copied to the top of S. • S-LEFT(`) creates a syntactic dependency with label ` in the reverse direction as S-RIGHT. The top of S, u is popped. The front of B, v is replaced by the new structure, rooted at v.
The semantic transitions are similar, operating on the semantic stack.
• M-SHIFT removes an item from the front of B and pushes it on M . • M-REDUCE pops an item from M . • M-RIGHT(r) creates a semantic dependency.
Let u be the element at the top of M and v, the front of B. The new dependency has u as head, v as dependent, and label r. u is popped off the semantic stack, and the resulting structure, rooted at u, is pushed on M . • M-LEFT(r) creates a semantic dependency with label r in the reverse direction as MRIGHT. The buffer front, v is replaced by the new v-rooted structure. M remains unchanged.
Because SRL graphs allow a node to be a semantic argument of two parents—like all in the
1This works better for the arc-eager algorithm (Ballesteros and Nivre, 2013), in contrast to Henderson et al. (2013), who initialized with root at the buffer front.
2Note that in the original arc-eager algorithm (Nivre, 2008), SHIFT and RIGHT-ARC actions move the item on the buffer front to the stack, whereas we only copy it (to allow the semantic operations to have access to it).
3
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
example in Figure 1—M-LEFT and M-RIGHT do not remove the dependent from the semantic stack and buffer respectively, unlike their syntactic equivalents, S-LEFT and S-RIGHT. We use two other semantic transitions from Henderson et al. (2013) which have no syntactic analogues:
• M-SWAP swaps the top two items on M , to allow for crossing semantic arcs. • M-PRED(p) marks the item at the front of B as a semantic predicate with the sense p, and replaces it with the disambiguated predicate.
The CoNLL 2009 corpus introduces semantic self-dependencies where many nominal predicates (from NomBank) are marked as their own arguments; these account for 6.68% of all semantic arcs in the English corpus. An example involving an eventive noun is shown in Figure 2. We introduce a new semantic transition to handle such cases:
• M-SELF(r) adds a dependency, with label r between the item at the front of B and itself. The result replaces the item at the front of B.
Note that the syntactic and semantic transitions both operate on the same buffer, though they independently specify the syntax and semantics, respectively. In order to ensure that both syntactic and semantic parses are produced, the syntactic and semantic transitions are interleaved. Only syntactic transitions are considered until a transition is chosen that copies an item from the buffer front to the syntactic stack (either S-SHIFT or S-RIGHT). The algorithm then switches to semantic transitions until a buffer-modifying transition is taken (M-SHIFT).3 At this point, the buffer is modified and the algorithm returns to syntactic transitions. This implies that, for each word, its left-side
3Had we moved the item at the buffer front during the syntactic transitions, it would have been unavailable for the semantic transitions, hence we only copy it.
syntactic dependencies are resolved before its leftside semantic dependencies. For interested readers, an example run of the algorithm is given in §A.To ensure that the parser never enters an invalid state, the sequence of transitions is constrained, following Henderson et al. (2013). Actions that copy or move items from the buffer (S-SHIFT, S-RIGHT and M-SHIFT) are forbidden when the buffer is empty. Actions that pop from a stack (S-REDUCE and M-REDUCE) are forbidden when that stack is empty. We disallow actions corresponding to the same dependency, or the same predicate to be repeated in the sequence. Repetitive M-SWAP transitions are disallowed to avoid infinite swapping. Finally, as noted above, we restrict the parser to syntactic actions until it needs to shift an item from B to S, after which it can only execute semantic actions until it executes an M-SHIFT.
Asymptotic runtime complexity of this greedy algorithm is linear in the length of the input, as analyzed by Nivre (2009).The transitions in §2 describe the execution paths our algorithm can take; like past work, we apply a statistical classifier to decide which transition to take at each timestep, given the current state. (A full example of a transition sequence is given in the supplementary material.) The novelty of our model is that it learns a finite-length vector representation of the entire joint parser’s state (S, M , and B) in order to make this decision.LSTMs are recurrent neural networks equipped with specialized memory components in addition to a hidden state (Hochreiter and Schmidhuber, 1997; Graves, 2013) to model sequences. Stack LSTMs (Dyer et al., 2015) are LSTMs that allow for stack-based operations: query, push, and pop. A “stack pointer” is maintained which determines which cell in the LSTM provides memory unit and the hidden unit when computing the new memory cell contents. Query provides a summary of the stack in a single fixed-length vector. Push adds an element to the top of the stack, resulting in a new summary. Pop, which does not correspond to
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
St Mt Bt Action St+1 Mt+1 Bt+1 Dependency S M (v, v), B S-SHIFT (v, v), S M (v, v), B —
(u, u), S M B S-REDUCE S M B — (u, u), S M (v, v), B S-RIGHT(`) (v, v), (gs(u,v, l), u), S M (v, v), B S [ u
`! v (u, u), S M (v, v), B S-LEFT(`) S M (gs(v,u, l), v), B S [ u
` v S M (v, v), B M-SHIFT S (v, v),M B — S (u, u),M B M-REDUCE S M B — S (u, u),M (v, v), B M-RIGHT(r) S (gm(u,v, r), u),M (v, v), B M [ u
r! v S (u, u),M (v, v), B M-LEFT(r) S (u, u),M (gm(v,u, r), v), B M [ u
r v S (u, u), (v, v),M B M-SWAP S (v, v), (u, u),M B — S M (v, v), B M-PRED(p) S M (gd(v,p), v), B — S M (v, v), B M-SELF(r) S M (gm(v,v, r), v), B M [ v r$ v
Table 1: Parser transitions along with the modifications to the stacks and the buffer resulting from each. Syntactic transitions are shown above, semantic below. Script symbols denote symbolic representations of words and relations, and bold symbols indicate (learned) embeddings (§3.5) of words and relations; each element in a stack or buffer includes both symbolic and vector representations, either atomic or recursive. S represents the set of syntactic transitions; M the set of semantic transitions.
a conventional LSTM operation, moves the stack pointer to the preceding timestep, resulting in a stack summary as it was before the popped item was observed. Implementation details (Dyer et al., 2015; Goldberg, 2015) and code were made publicly available.4
Using stack LSTMs, we construct a representation of the algorithm state by decomposing it into smaller pieces that are combined by recursive function evaluations (similar to the way a list is built by a concatenate operation that operates on a list and an element). This enables information that would be distant from the “top” of the stack to be carried forward, potentially helping the learner.Our algorithm employs four stack LSTMs, one each for the S, M , and B data structures, so that already-built partial structures are available to the classifier. Like Dyer et al. (2015), we use a fourth stack LSTM, A, for the history of actions—A is never popped from, only pushed to. Figure 3 illustrates the architecture. The algorithm’s state at timestep t is encoded by the four vectors summarizing the four stack LSTMs, and this is the input to the classifier that chooses among the allowable transitions at that timestep.
Let st, mt, bt, and at denote the summaries of St, Mt, Bt, and At, respectively. Let At = Allowed(St,Mt, Bt, At) denote the allowed transitions given the current stacks and buffer. The parser state at time t is given by a rectified linear unit (Nair and Hinton, 2010) in vector yt:
yt = elementwisemax {0,d+W[st;mt;bt,at]}
4 https://github.com/clab/lstm-parser
root soon reopen to
all
are expected
expect.01 all
all
sbj
A1
M
B
S
S-RIGHT (oprd)
... M-PRED
(expect.01) M-REDUCE M-LEFT
(A1)
A
are
vc
M-SHIFT
Figure 3: Stack LSTM for joint parsing. The state illustrated corresponds to the ***-marked row in the example transition sequence in Fig. 5 in the supplementary material.
where W and d are the parameters of the classifier. The transition selected at timestep t is
argmax ⌧2At q⌧ + ✓⌧ · yt (1)
⌘ argmax ⌧2At score(⌧ ;St,Mt, Bt, At) (2)
where ✓⌧ and q⌧ are parameters for each transition type ⌧ . Note that only allowed transitions are considered in the decision rule (see §2.3).To use stack LSTMs, we require vector representations of the elements that are stored in the stacks. Specifically, we require vector representations of atoms (words, possibly with part-of-speech tags) and parse fragments. Word vectors can be pretrained or learned directly; we consider a concate-
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
nation of both in our experiments; part-of-speech vectors are learned and concatenated to the same.
To obtain vector representations of parse fragments, we use neural networks which recursively compute representations of the complex structured output (Dyer et al., 2015). The tree structures here are always ternary trees, with each internal node’s three children including a head, a dependent, and a label. The vectors for leaves are word vectors and vectors corresponding to syntactic and semantic relation types.
The vector for an internal node is a squashed (tanh) affine transformation of its children’s vectors. For syntactic and semantic attachments, respectively, the composition function is:
gs(v,u, l) = tanh(Zs[v;u; l] + es) (3) gm(v,u, r) = tanh(Zm[v;u; r] + em) (4)
where v and u are vectors correspodning to atomic words or composed parse fragments; l and r are learned vector representations for a syntactic and semantic labels respectively. Syntactic and semantic parameters are separated (Zs, es and Zm, em, respectively).
Finally, for predicates, we use another recursive function to compose the word representation, v with a learned representation for the dismabiguated sense of the predicate, p:
gd(v,p) = tanh(Zd[v;p] + ed) (5)
where Zd and ed are parameters of the model. Note that, because syntactic and semantic transitions are interleaved, the fragmented structures are a blend of syntactic and semantic compositions. Figure 4 shows an example.Training the classifier requires transforming each training instance (a joint parse) into a transition sequence, a deterministic operation under our transition set. Given a collection of algorithm states at time t and correct classification decisions ⌧t, we minimize the sum of log-loss terms, given (for one timestep) by:
log exp(q⌧t + ✓⌧t · yt)P ⌧ 02At exp(q⌧ 0 + ✓⌧ 0 · yt)
(6)
with respect to the classifier and LSTM parameters. Note that the loss is differentiable with respect to the parameters; gradients are calculated
using backpropagation. We apply stochastic gradient descent with dropout for all neural network parameters.Following Dyer et al. (2015), “structured skipgram” embeddings (Ling et al., 2015) were used, trained on the English (AFP section), German, Spanish and Chinese Gigaword corpora, with a window of size 5; training was stopped after 5 epochs. For out-of-vocabulary words, a randomly initialized vector of the same dimension was used.Predicate sense disambiguation is handled within the model (M-PRED transitions), but since senses are lexeme-specific, we need a way to handle unseen predicates at test time. When a predicate is encountered at test time that was not observed in training, our system constructs a predicate from the predicted lemma of the word at that position and defaults to the “01” sense, which is correct for 91.22% of predicates by type in the English CoNLL 2009 training data.Our model is evaluated on the CoNLL shared tasks on joint syntactic and semantic dependency parsing in 2008 (Surdeanu et al., 2008) and 2009 (Hajič et al., 2009). The standard training, development and test splits of all datasets were
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
used. Per the shared task guidelines, automatically predicted POS tags and lemmas provided in the datasets were used for all experiments. As a preprocessing step, pseudo-projectivization of the syntactic trees (Nivre et al., 2007) was used, which allowed an accurate conversion of even the nonprojective syntactic trees into syntactic transitions. However, the oracle conversion of semantic parses into transitions is not perfect despite using the MSWAP action due to the presence of multiple crossing arcs.5
The standard evaluation metrics include the syntactic labeled attachment score (LAS), the semantic F1 score on both in-domain (WSJ) and outof-domain (Brown corpus) data, and their macro average (Macro F1) to score joint systems. Because the task was defined somewhat differently in each year, each dataset is considered in turn.The CoNLL 2008 dataset contains annotations from the Penn Treebank (Marcus et al., 1993), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The shared task evaluated systems on predicate identification in addition to predicate sense disambiguation and SRL.
To identify predicates, we trained a zeroMarkov order bidirectional LSTM two-class classifier. As input to the classifier, we use learned representations of word lemmas and POS tags. This model achieves an F1 score of 91.43% on marking words as predicates (or not).
Hyperparameters The input representation for a word consists of pretrained embeddings (size 100 for English, 80 for Chinese, 64 for German and Spanish), concatenated with additional learned word and POS tag embeddings (size 32 and 10, resp.). Learned embeddings for syntactic and semantic arc labels are of size 20 and predicates 100. Two-layer LSTMs, with hidden state dimension 100 were used for each of the four stacks. The parser state, yt and the composition function, g are of dimension 100. A dropout rate of 0.2 (Zaremba et al., 2014) was used on all layers at training time, tuned on the dev data from the set of values {0.1, 0.2, 0.3, 1.0}. The learned representations for actions are of size 100, similarly tuned from {10, 20, 30, 40, 100}. Other hyperpa-
5For 1.5% of English sentences in the CoNLL 2009 English dataset, the transition sequence incorrectly encodes the gold-standard joint parse; details in Henderson et al. (2013).
rameters have been set intuitively; careful tuning is expected to yield improvements (Weiss et al., 2015).
An initial learning rate of 0.1 for stochastic gradient descent was used and updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the dev performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB.Relative to the CoNLL 2008 task (above), the main change in 2009 is that predicates are preidentified, and systems are only evaluated on predicate sense disambiguation (not identification). Hence, the bidirectional LSTM classifier is not used here. The preprocessing for projectivity, and the hyperparameter selection is the same as in §4.1.
In addition to the joint approach described in the preceding sections, we experiment here with several variants:
Semantics-only: all syntactic transitions in S , the syntactic stack S, and the syntactic composition function, gs are discarded. As a result, the set of constraints on transitions is a subset of the full set of constraints in §2.3. Effectively, this model does not use any syntactic features, similar to Collobert et al. (2011) and Zhou and Xu (2015). It provides a controlled test of the benefit of jointly predicting syntax in a semantic parser.
Syntax-only: all semantic transitions in M, the semantic stack M , and the semantic composition function gm are discarded. S-SHIFT and S-RIGHT now move the item from the front of the buffer to the syntactic stack, instead of copying. The set of constraints on the transitions is again a subset of the full set of constraints. This model is an arceager variant of Dyer et al. (2015), and serves to check whether semantic parsing degrades syntactic performance.
Hybrid: the semantics parameters are trained using automatically predicted syntax from the syntax-only model. At test time, only semantic parses are predicted. This setup bears similarity to other approaches which pipeline syntax and semantics, extracting features from the syntactic parse to help SRL. However, unlike other
7
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
approaches, this model does not offer the entire syntactic tree for feature extraction, since only the partial syntactic structures present on the syntactic stack (and potentially the buffer) are visible at a given timestep.","CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of transitions is derived, showing the benefit of learning a representation for the entire algorithmic state. Several other joint learning models have been proposed (Lluı́s and Màrquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models. The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system’s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower.
CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013); demonstrating the benefit of our entire-parser-state representation learner compared to the more locally scoped model.
Given that syntax has consistently proven useful
in SRL, our Semantics-only model underperforms Hybrid and Joint, as expected. In the training domain, syntax and semantics benefit each other (Joint outperforms Hybrid). Out-of-domain (the Brown test set), the Hybrid pulls ahead, a sign that Joint overfits to WSJ. As a syntactic parser, our Syntax-only model performs slightly better than Dyer et al. (2015), who achieve 89.56 LAS on this task.
The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who also carefully design languagespecific features, and use a series of pipelines for the joint task, resulting in an accurate but computationally expensive system.
State-of-the-art SRL systems (shown in the last block of Table 3) which use advancements orthogonal to the contributions in this paper, perform better than our models. Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Björkelund et al., 2009; Björkelund et al., 2010; Roth and Woodsend, 2014); we have not used these features or reranking. Lei et al. (2015) use syntactic parses to obtain interaction features between predicates and their arguments and then compress feature representations using low-rank tensor. Täckström et al. (2015) present an exact inference algorithm for SRL based on dynamic programming and their local and structured models make use of many syntactic features from a pipeline; our search procedure is greedy. Their algorithm is adopted by FitzGerald et al. (2015) for inference in a model that jointly learns representations from a combination of PropBank and FrameNet annotations; we have not experimented with extra annotations.
Our system achieves an end-to-end runtime of 177.6±18 seconds to parse the CoNLL 2009 English test set on a single core. This is almost 2.5 times faster than the pipeline model of Lei et al. (2015) (439.9±42 seconds) under identical settings.6
CoNLL 2009 Multilingual (Table 4) We tested the joint model on the non-English CoNLL 2009 datasets, and the results demonstrate that it adapts
6See https://github.com/taolei87/ SRLParser; we chose this system since it is publicly available, and other state-of-the-art systems are not.
8
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
easily—it is on par with the top three systems in most cases. We note that our Chinese parser relies on pretrained word embeddings for its superior performance; without them (not shown), it was on par with the others. Japanese is a small-data case (4,393 training examples), illustrating our model’s dependence on reasonably large training datasets.
We have not extended our model to incorporate morphological features, used by the systems to which we compare. Future work might incorporate morphological features where available; this could potentially improve performance, especially in highly inflective languages like Czech. An alternative might be to infer word-internal representations using character-based word embeddings, which was found beneficial for syntactic parsing (Ballesteros et al., 2015).","We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our multilingual results are comparable to the top systems at CoNLL 2009.
Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Lluı́s et al., 2013, inter alia).
One reason pipelines often dominate is that they make available the complete syntactic parse tree,
and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013).
This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing.
We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design.
Our system’s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Björkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (Täckström et al., 2015), or of systems that exploit additional, differently-annotated datasets (FitzGerald et al., 2015). Many advances in those systems are orthogonal to our model, and we expect future work to achieve further gains by integrating them.
Because our system is very fast—with an endto-end runtime of 177.6±18 seconds to parse the
2
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
CoNLL 2009 English test data on a single core— we believe it will be useful in practical settings. An open-source implementation will be made available on publication.","Other approaches to joint modeling, not considered in our experiments, are notable. Lluı́s et al. (2013) propose a graph-based joint model using dual decomposition for agreement between syntax and semantics, but do not achieve competitive performance on the CoNLL 2009 task. Lewis et al. (2015) proposed an efficient joint model for CCG syntax and SRL, which performs better than a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation is infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset.
There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and Màrquez, 2005), their approach inspired Zhou and Xu (2015), who achieved state-of-the-art results using deep bidirectional LSTMs.7 Our approach for dependency-based SRL is not directly comparable.",,"We presented an incremental, greedy parser for joint syntactic and semantic dependency parsing. Our model surpasses the performance of previous joint models on the CoNLL 2008 and 2009 English tasks, despite not using expert-crafted features of the full syntactic parse, as done by asymptotically more expensive models. An open-source implementation of our parser will be made available on publication."
9,"We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008--9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.","Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs",66,"General comments
================

The paper presents a joint syntactic and semantic transition-based dependency
parser,
inspired from the joint parser of Henderson et al. (2008).
The authors claim two main differences:
- vectorial representations are used for the whole parser's state, instead of
the top elements of the stack / the last parser's configurations
- the algorithm is a plain greedy search

The key idea is to take advantage of stack LSTMs so that the vector
representing the state of the parser
keeps memory of potentially large scoped syntactic features, which
are known to be decisive features for semantic role labeling
(such as the path between the predicate and the candidate role filler head).

The system is tested on the CoNLL 2008 data set (English) and on the
multilingual CoNLL 2009 data set.
The authors compare their system's performance to previously reported
performances,
showing their system does well compared to the 2008 / 2009 systems, 
but less compared to more recent proposals (cf. bottom of table 3).
They emphasized though that the proposed system does not require any hand-craft
features,
and is fast due to the simple greedy algorithm.

The paper is well written and describes a substantial amount of work,
building on the recently popular LSTMs, applied to the Henderson et al.
algorithm
which appears now to have been somewhat visionary.

I have reservations concerning the choice of the simple greedy algorithm:
it renders results not comparable to some of the cited works.
It would not have been too much additional work nor space to provide for
instance beam-searched performance.

More detailed comments / questions
==================================

Section 2:

A comment on the presence of both A1 and C-A1 links would help understanding
better the target task of the paper.

A summary of the differences between the set of transitions used in this work
and that of Henderson et al. should be provided. In its current form, it is
difficult to 
tell what is directly reused from Henderson et al. and what is new / slightly
modified.

Section 3.3

Why do you need representations concatenating the word predicate and its
disambiguated sense,
this seems redundant since the disambiguated sense are specific to a predicate
?

Section 4

The organization if the 4.1 / 4.2 sections is confusing concerning
multilinguality.
Conll 2008 focused on English, and CoNLL 2009 shared task extended it to a few
other languages.",,4,4,Oral Presentation,4,4,4,3,4,5,3,3,2016,"We largely follow the transition-based, synchronized algorithm of Henderson et al. (2013) to predict joint parse structures. The input to the algorithm is a sentence annotated with part-of-speech tags. The output consists of a labeled syntactic dependency tree and a directed SRL graph, in which a subset of words in the sentence are selected as predicates, disambiguated to a sense, and linked by labeled, directed edges to their semantic arguments and modifiers. Figure 1 shows an example.The two parses are constructed in a bottom-up fashion, incrementally processing words in the sentence from left to right. The state of the parsing algorithm at timestep t is represented by three stack data structures: a syntactic stack St, a semantic stack Mt—each containing partially built structures—and a buffer of input words Bt. Our algorithm also places partial syntactic and semantic parse structures onto the front of the buffer, so it is also implemented as a stack. Each arc in the output corresponds to a transition (or “action”) chosen based on the current state; every transition modifies the state by updating St, Mt, and Bt to St+1, Mt+1, and Bt+1, respectively. While each state may license several valid actions, each action has a deterministic effect on the state of the algorithm.
Initially, S0 and M0 are empty, and B0 contains the input sentence with the first word at the front of B and a special root symbol at the end.1 Execution ends on iteration t such that Bt is empty and St and Mt contain only a single structure headed by root.There are separate sets of syntactic and semantic transitions; the former manipulate S and B, the latter M and B. All are formally defined in Table 1. The syntactic transitions are from the “arceager” algorithm of Nivre (2008). They include:
• S-SHIFT, which copies2 an item from the front of B and pushes it on S. • S-REDUCE pops an item from S. • S-RIGHT(`) creates a syntactic dependency.
Let u be the element at the top of S and v be the element at the front of B. The new dependency has u as head, v as dependent, and label `. u is popped off S, and the resulting structure, rooted at u, is pushed on S. Finally, v is copied to the top of S. • S-LEFT(`) creates a syntactic dependency with label ` in the reverse direction as S-RIGHT. The top of S, u is popped. The front of B, v is replaced by the new structure, rooted at v.
The semantic transitions are similar, operating on the semantic stack.
• M-SHIFT removes an item from the front of B and pushes it on M . • M-REDUCE pops an item from M . • M-RIGHT(r) creates a semantic dependency.
Let u be the element at the top of M and v, the front of B. The new dependency has u as head, v as dependent, and label r. u is popped off the semantic stack, and the resulting structure, rooted at u, is pushed on M . • M-LEFT(r) creates a semantic dependency with label r in the reverse direction as MRIGHT. The buffer front, v is replaced by the new v-rooted structure. M remains unchanged.
Because SRL graphs allow a node to be a semantic argument of two parents—like all in the
1This works better for the arc-eager algorithm (Ballesteros and Nivre, 2013), in contrast to Henderson et al. (2013), who initialized with root at the buffer front.
2Note that in the original arc-eager algorithm (Nivre, 2008), SHIFT and RIGHT-ARC actions move the item on the buffer front to the stack, whereas we only copy it (to allow the semantic operations to have access to it).
3
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
example in Figure 1—M-LEFT and M-RIGHT do not remove the dependent from the semantic stack and buffer respectively, unlike their syntactic equivalents, S-LEFT and S-RIGHT. We use two other semantic transitions from Henderson et al. (2013) which have no syntactic analogues:
• M-SWAP swaps the top two items on M , to allow for crossing semantic arcs. • M-PRED(p) marks the item at the front of B as a semantic predicate with the sense p, and replaces it with the disambiguated predicate.
The CoNLL 2009 corpus introduces semantic self-dependencies where many nominal predicates (from NomBank) are marked as their own arguments; these account for 6.68% of all semantic arcs in the English corpus. An example involving an eventive noun is shown in Figure 2. We introduce a new semantic transition to handle such cases:
• M-SELF(r) adds a dependency, with label r between the item at the front of B and itself. The result replaces the item at the front of B.
Note that the syntactic and semantic transitions both operate on the same buffer, though they independently specify the syntax and semantics, respectively. In order to ensure that both syntactic and semantic parses are produced, the syntactic and semantic transitions are interleaved. Only syntactic transitions are considered until a transition is chosen that copies an item from the buffer front to the syntactic stack (either S-SHIFT or S-RIGHT). The algorithm then switches to semantic transitions until a buffer-modifying transition is taken (M-SHIFT).3 At this point, the buffer is modified and the algorithm returns to syntactic transitions. This implies that, for each word, its left-side
3Had we moved the item at the buffer front during the syntactic transitions, it would have been unavailable for the semantic transitions, hence we only copy it.
syntactic dependencies are resolved before its leftside semantic dependencies. For interested readers, an example run of the algorithm is given in §A.To ensure that the parser never enters an invalid state, the sequence of transitions is constrained, following Henderson et al. (2013). Actions that copy or move items from the buffer (S-SHIFT, S-RIGHT and M-SHIFT) are forbidden when the buffer is empty. Actions that pop from a stack (S-REDUCE and M-REDUCE) are forbidden when that stack is empty. We disallow actions corresponding to the same dependency, or the same predicate to be repeated in the sequence. Repetitive M-SWAP transitions are disallowed to avoid infinite swapping. Finally, as noted above, we restrict the parser to syntactic actions until it needs to shift an item from B to S, after which it can only execute semantic actions until it executes an M-SHIFT.
Asymptotic runtime complexity of this greedy algorithm is linear in the length of the input, as analyzed by Nivre (2009).The transitions in §2 describe the execution paths our algorithm can take; like past work, we apply a statistical classifier to decide which transition to take at each timestep, given the current state. (A full example of a transition sequence is given in the supplementary material.) The novelty of our model is that it learns a finite-length vector representation of the entire joint parser’s state (S, M , and B) in order to make this decision.LSTMs are recurrent neural networks equipped with specialized memory components in addition to a hidden state (Hochreiter and Schmidhuber, 1997; Graves, 2013) to model sequences. Stack LSTMs (Dyer et al., 2015) are LSTMs that allow for stack-based operations: query, push, and pop. A “stack pointer” is maintained which determines which cell in the LSTM provides memory unit and the hidden unit when computing the new memory cell contents. Query provides a summary of the stack in a single fixed-length vector. Push adds an element to the top of the stack, resulting in a new summary. Pop, which does not correspond to
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
St Mt Bt Action St+1 Mt+1 Bt+1 Dependency S M (v, v), B S-SHIFT (v, v), S M (v, v), B —
(u, u), S M B S-REDUCE S M B — (u, u), S M (v, v), B S-RIGHT(`) (v, v), (gs(u,v, l), u), S M (v, v), B S [ u
`! v (u, u), S M (v, v), B S-LEFT(`) S M (gs(v,u, l), v), B S [ u
` v S M (v, v), B M-SHIFT S (v, v),M B — S (u, u),M B M-REDUCE S M B — S (u, u),M (v, v), B M-RIGHT(r) S (gm(u,v, r), u),M (v, v), B M [ u
r! v S (u, u),M (v, v), B M-LEFT(r) S (u, u),M (gm(v,u, r), v), B M [ u
r v S (u, u), (v, v),M B M-SWAP S (v, v), (u, u),M B — S M (v, v), B M-PRED(p) S M (gd(v,p), v), B — S M (v, v), B M-SELF(r) S M (gm(v,v, r), v), B M [ v r$ v
Table 1: Parser transitions along with the modifications to the stacks and the buffer resulting from each. Syntactic transitions are shown above, semantic below. Script symbols denote symbolic representations of words and relations, and bold symbols indicate (learned) embeddings (§3.5) of words and relations; each element in a stack or buffer includes both symbolic and vector representations, either atomic or recursive. S represents the set of syntactic transitions; M the set of semantic transitions.
a conventional LSTM operation, moves the stack pointer to the preceding timestep, resulting in a stack summary as it was before the popped item was observed. Implementation details (Dyer et al., 2015; Goldberg, 2015) and code were made publicly available.4
Using stack LSTMs, we construct a representation of the algorithm state by decomposing it into smaller pieces that are combined by recursive function evaluations (similar to the way a list is built by a concatenate operation that operates on a list and an element). This enables information that would be distant from the “top” of the stack to be carried forward, potentially helping the learner.Our algorithm employs four stack LSTMs, one each for the S, M , and B data structures, so that already-built partial structures are available to the classifier. Like Dyer et al. (2015), we use a fourth stack LSTM, A, for the history of actions—A is never popped from, only pushed to. Figure 3 illustrates the architecture. The algorithm’s state at timestep t is encoded by the four vectors summarizing the four stack LSTMs, and this is the input to the classifier that chooses among the allowable transitions at that timestep.
Let st, mt, bt, and at denote the summaries of St, Mt, Bt, and At, respectively. Let At = Allowed(St,Mt, Bt, At) denote the allowed transitions given the current stacks and buffer. The parser state at time t is given by a rectified linear unit (Nair and Hinton, 2010) in vector yt:
yt = elementwisemax {0,d+W[st;mt;bt,at]}
4 https://github.com/clab/lstm-parser
root soon reopen to
all
are expected
expect.01 all
all
sbj
A1
M
B
S
S-RIGHT (oprd)
... M-PRED
(expect.01) M-REDUCE M-LEFT
(A1)
A
are
vc
M-SHIFT
Figure 3: Stack LSTM for joint parsing. The state illustrated corresponds to the ***-marked row in the example transition sequence in Fig. 5 in the supplementary material.
where W and d are the parameters of the classifier. The transition selected at timestep t is
argmax ⌧2At q⌧ + ✓⌧ · yt (1)
⌘ argmax ⌧2At score(⌧ ;St,Mt, Bt, At) (2)
where ✓⌧ and q⌧ are parameters for each transition type ⌧ . Note that only allowed transitions are considered in the decision rule (see §2.3).To use stack LSTMs, we require vector representations of the elements that are stored in the stacks. Specifically, we require vector representations of atoms (words, possibly with part-of-speech tags) and parse fragments. Word vectors can be pretrained or learned directly; we consider a concate-
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
nation of both in our experiments; part-of-speech vectors are learned and concatenated to the same.
To obtain vector representations of parse fragments, we use neural networks which recursively compute representations of the complex structured output (Dyer et al., 2015). The tree structures here are always ternary trees, with each internal node’s three children including a head, a dependent, and a label. The vectors for leaves are word vectors and vectors corresponding to syntactic and semantic relation types.
The vector for an internal node is a squashed (tanh) affine transformation of its children’s vectors. For syntactic and semantic attachments, respectively, the composition function is:
gs(v,u, l) = tanh(Zs[v;u; l] + es) (3) gm(v,u, r) = tanh(Zm[v;u; r] + em) (4)
where v and u are vectors correspodning to atomic words or composed parse fragments; l and r are learned vector representations for a syntactic and semantic labels respectively. Syntactic and semantic parameters are separated (Zs, es and Zm, em, respectively).
Finally, for predicates, we use another recursive function to compose the word representation, v with a learned representation for the dismabiguated sense of the predicate, p:
gd(v,p) = tanh(Zd[v;p] + ed) (5)
where Zd and ed are parameters of the model. Note that, because syntactic and semantic transitions are interleaved, the fragmented structures are a blend of syntactic and semantic compositions. Figure 4 shows an example.Training the classifier requires transforming each training instance (a joint parse) into a transition sequence, a deterministic operation under our transition set. Given a collection of algorithm states at time t and correct classification decisions ⌧t, we minimize the sum of log-loss terms, given (for one timestep) by:
log exp(q⌧t + ✓⌧t · yt)P ⌧ 02At exp(q⌧ 0 + ✓⌧ 0 · yt)
(6)
with respect to the classifier and LSTM parameters. Note that the loss is differentiable with respect to the parameters; gradients are calculated
using backpropagation. We apply stochastic gradient descent with dropout for all neural network parameters.Following Dyer et al. (2015), “structured skipgram” embeddings (Ling et al., 2015) were used, trained on the English (AFP section), German, Spanish and Chinese Gigaword corpora, with a window of size 5; training was stopped after 5 epochs. For out-of-vocabulary words, a randomly initialized vector of the same dimension was used.Predicate sense disambiguation is handled within the model (M-PRED transitions), but since senses are lexeme-specific, we need a way to handle unseen predicates at test time. When a predicate is encountered at test time that was not observed in training, our system constructs a predicate from the predicted lemma of the word at that position and defaults to the “01” sense, which is correct for 91.22% of predicates by type in the English CoNLL 2009 training data.Our model is evaluated on the CoNLL shared tasks on joint syntactic and semantic dependency parsing in 2008 (Surdeanu et al., 2008) and 2009 (Hajič et al., 2009). The standard training, development and test splits of all datasets were
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
used. Per the shared task guidelines, automatically predicted POS tags and lemmas provided in the datasets were used for all experiments. As a preprocessing step, pseudo-projectivization of the syntactic trees (Nivre et al., 2007) was used, which allowed an accurate conversion of even the nonprojective syntactic trees into syntactic transitions. However, the oracle conversion of semantic parses into transitions is not perfect despite using the MSWAP action due to the presence of multiple crossing arcs.5
The standard evaluation metrics include the syntactic labeled attachment score (LAS), the semantic F1 score on both in-domain (WSJ) and outof-domain (Brown corpus) data, and their macro average (Macro F1) to score joint systems. Because the task was defined somewhat differently in each year, each dataset is considered in turn.The CoNLL 2008 dataset contains annotations from the Penn Treebank (Marcus et al., 1993), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The shared task evaluated systems on predicate identification in addition to predicate sense disambiguation and SRL.
To identify predicates, we trained a zeroMarkov order bidirectional LSTM two-class classifier. As input to the classifier, we use learned representations of word lemmas and POS tags. This model achieves an F1 score of 91.43% on marking words as predicates (or not).
Hyperparameters The input representation for a word consists of pretrained embeddings (size 100 for English, 80 for Chinese, 64 for German and Spanish), concatenated with additional learned word and POS tag embeddings (size 32 and 10, resp.). Learned embeddings for syntactic and semantic arc labels are of size 20 and predicates 100. Two-layer LSTMs, with hidden state dimension 100 were used for each of the four stacks. The parser state, yt and the composition function, g are of dimension 100. A dropout rate of 0.2 (Zaremba et al., 2014) was used on all layers at training time, tuned on the dev data from the set of values {0.1, 0.2, 0.3, 1.0}. The learned representations for actions are of size 100, similarly tuned from {10, 20, 30, 40, 100}. Other hyperpa-
5For 1.5% of English sentences in the CoNLL 2009 English dataset, the transition sequence incorrectly encodes the gold-standard joint parse; details in Henderson et al. (2013).
rameters have been set intuitively; careful tuning is expected to yield improvements (Weiss et al., 2015).
An initial learning rate of 0.1 for stochastic gradient descent was used and updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the dev performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB.Relative to the CoNLL 2008 task (above), the main change in 2009 is that predicates are preidentified, and systems are only evaluated on predicate sense disambiguation (not identification). Hence, the bidirectional LSTM classifier is not used here. The preprocessing for projectivity, and the hyperparameter selection is the same as in §4.1.
In addition to the joint approach described in the preceding sections, we experiment here with several variants:
Semantics-only: all syntactic transitions in S , the syntactic stack S, and the syntactic composition function, gs are discarded. As a result, the set of constraints on transitions is a subset of the full set of constraints in §2.3. Effectively, this model does not use any syntactic features, similar to Collobert et al. (2011) and Zhou and Xu (2015). It provides a controlled test of the benefit of jointly predicting syntax in a semantic parser.
Syntax-only: all semantic transitions in M, the semantic stack M , and the semantic composition function gm are discarded. S-SHIFT and S-RIGHT now move the item from the front of the buffer to the syntactic stack, instead of copying. The set of constraints on the transitions is again a subset of the full set of constraints. This model is an arceager variant of Dyer et al. (2015), and serves to check whether semantic parsing degrades syntactic performance.
Hybrid: the semantics parameters are trained using automatically predicted syntax from the syntax-only model. At test time, only semantic parses are predicted. This setup bears similarity to other approaches which pipeline syntax and semantics, extracting features from the syntactic parse to help SRL. However, unlike other
7
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
approaches, this model does not offer the entire syntactic tree for feature extraction, since only the partial syntactic structures present on the syntactic stack (and potentially the buffer) are visible at a given timestep.","CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of transitions is derived, showing the benefit of learning a representation for the entire algorithmic state. Several other joint learning models have been proposed (Lluı́s and Màrquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models. The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system’s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower.
CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013); demonstrating the benefit of our entire-parser-state representation learner compared to the more locally scoped model.
Given that syntax has consistently proven useful
in SRL, our Semantics-only model underperforms Hybrid and Joint, as expected. In the training domain, syntax and semantics benefit each other (Joint outperforms Hybrid). Out-of-domain (the Brown test set), the Hybrid pulls ahead, a sign that Joint overfits to WSJ. As a syntactic parser, our Syntax-only model performs slightly better than Dyer et al. (2015), who achieve 89.56 LAS on this task.
The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who also carefully design languagespecific features, and use a series of pipelines for the joint task, resulting in an accurate but computationally expensive system.
State-of-the-art SRL systems (shown in the last block of Table 3) which use advancements orthogonal to the contributions in this paper, perform better than our models. Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Björkelund et al., 2009; Björkelund et al., 2010; Roth and Woodsend, 2014); we have not used these features or reranking. Lei et al. (2015) use syntactic parses to obtain interaction features between predicates and their arguments and then compress feature representations using low-rank tensor. Täckström et al. (2015) present an exact inference algorithm for SRL based on dynamic programming and their local and structured models make use of many syntactic features from a pipeline; our search procedure is greedy. Their algorithm is adopted by FitzGerald et al. (2015) for inference in a model that jointly learns representations from a combination of PropBank and FrameNet annotations; we have not experimented with extra annotations.
Our system achieves an end-to-end runtime of 177.6±18 seconds to parse the CoNLL 2009 English test set on a single core. This is almost 2.5 times faster than the pipeline model of Lei et al. (2015) (439.9±42 seconds) under identical settings.6
CoNLL 2009 Multilingual (Table 4) We tested the joint model on the non-English CoNLL 2009 datasets, and the results demonstrate that it adapts
6See https://github.com/taolei87/ SRLParser; we chose this system since it is publicly available, and other state-of-the-art systems are not.
8
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
easily—it is on par with the top three systems in most cases. We note that our Chinese parser relies on pretrained word embeddings for its superior performance; without them (not shown), it was on par with the others. Japanese is a small-data case (4,393 training examples), illustrating our model’s dependence on reasonably large training datasets.
We have not extended our model to incorporate morphological features, used by the systems to which we compare. Future work might incorporate morphological features where available; this could potentially improve performance, especially in highly inflective languages like Czech. An alternative might be to infer word-internal representations using character-based word embeddings, which was found beneficial for syntactic parsing (Ballesteros et al., 2015).","We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our multilingual results are comparable to the top systems at CoNLL 2009.
Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Lluı́s et al., 2013, inter alia).
One reason pipelines often dominate is that they make available the complete syntactic parse tree,
and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013).
This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing.
We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design.
Our system’s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Björkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (Täckström et al., 2015), or of systems that exploit additional, differently-annotated datasets (FitzGerald et al., 2015). Many advances in those systems are orthogonal to our model, and we expect future work to achieve further gains by integrating them.
Because our system is very fast—with an endto-end runtime of 177.6±18 seconds to parse the
2
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
CoNLL 2009 English test data on a single core— we believe it will be useful in practical settings. An open-source implementation will be made available on publication.","Other approaches to joint modeling, not considered in our experiments, are notable. Lluı́s et al. (2013) propose a graph-based joint model using dual decomposition for agreement between syntax and semantics, but do not achieve competitive performance on the CoNLL 2009 task. Lewis et al. (2015) proposed an efficient joint model for CCG syntax and SRL, which performs better than a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation is infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset.
There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and Màrquez, 2005), their approach inspired Zhou and Xu (2015), who achieved state-of-the-art results using deep bidirectional LSTMs.7 Our approach for dependency-based SRL is not directly comparable.",,"We presented an incremental, greedy parser for joint syntactic and semantic dependency parsing. Our model surpasses the performance of previous joint models on the CoNLL 2008 and 2009 English tasks, despite not using expert-crafted features of the full syntactic parse, as done by asymptotically more expensive models. An open-source implementation of our parser will be made available on publication."
10,"We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008--9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.","Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs",66,"This paper performs an overdue circling-back to the problem of joint semantic
and syntactic dependency parsing, applying the recent insights from neural
network models. Joint models are one of the most promising things about the
success of transition-based neural network parsers.

There are two contributions here. First, the authors present a new transition
system, that seems better than the Hendersen (2008) system it is based on. The
other contribution is to show that the neural network succeeds on this problem,
where linear models had previously struggled. The authors attribute this
success to the ability of the neural network to automatically learn which
features to extract. However, I think there's another advantage to the neural
network here, that might be worth mentioning. In a linear model, you need to
learn a weight for each feature/class pair. This means that if you jointly
learn two problems, you have to learn many more parameters. The neural network
is much more economical in this respect.

I suspect the transition-system would work just as well with a variety of other
neural network models, e.g. the global beam-search model of Andor (2016). There
are many other orthogonal improvements that could be made. I expect extensions
to the authors' method to produce state-of-the-art results.

It would be nice to see an attempt to derive a dynamic
oracle for this transition system, even if it's only in an appendix or in
follow-up work. At first glance, it seems similar to the
arc-eager oracle. The M-S action excludes all semantic arcs between the word at
the start of the buffer and the words on the semantic stack, and the M-D action
excludes all semantic arcs between the word at the top of the stack and the
words in the buffer. The L and R actions seem to each exclude the reverse arc,
and no other.",,5,5,Oral Presentation,5,5,4,4,5,5,4,3,2016,"We largely follow the transition-based, synchronized algorithm of Henderson et al. (2013) to predict joint parse structures. The input to the algorithm is a sentence annotated with part-of-speech tags. The output consists of a labeled syntactic dependency tree and a directed SRL graph, in which a subset of words in the sentence are selected as predicates, disambiguated to a sense, and linked by labeled, directed edges to their semantic arguments and modifiers. Figure 1 shows an example.The two parses are constructed in a bottom-up fashion, incrementally processing words in the sentence from left to right. The state of the parsing algorithm at timestep t is represented by three stack data structures: a syntactic stack St, a semantic stack Mt—each containing partially built structures—and a buffer of input words Bt. Our algorithm also places partial syntactic and semantic parse structures onto the front of the buffer, so it is also implemented as a stack. Each arc in the output corresponds to a transition (or “action”) chosen based on the current state; every transition modifies the state by updating St, Mt, and Bt to St+1, Mt+1, and Bt+1, respectively. While each state may license several valid actions, each action has a deterministic effect on the state of the algorithm.
Initially, S0 and M0 are empty, and B0 contains the input sentence with the first word at the front of B and a special root symbol at the end.1 Execution ends on iteration t such that Bt is empty and St and Mt contain only a single structure headed by root.There are separate sets of syntactic and semantic transitions; the former manipulate S and B, the latter M and B. All are formally defined in Table 1. The syntactic transitions are from the “arceager” algorithm of Nivre (2008). They include:
• S-SHIFT, which copies2 an item from the front of B and pushes it on S. • S-REDUCE pops an item from S. • S-RIGHT(`) creates a syntactic dependency.
Let u be the element at the top of S and v be the element at the front of B. The new dependency has u as head, v as dependent, and label `. u is popped off S, and the resulting structure, rooted at u, is pushed on S. Finally, v is copied to the top of S. • S-LEFT(`) creates a syntactic dependency with label ` in the reverse direction as S-RIGHT. The top of S, u is popped. The front of B, v is replaced by the new structure, rooted at v.
The semantic transitions are similar, operating on the semantic stack.
• M-SHIFT removes an item from the front of B and pushes it on M . • M-REDUCE pops an item from M . • M-RIGHT(r) creates a semantic dependency.
Let u be the element at the top of M and v, the front of B. The new dependency has u as head, v as dependent, and label r. u is popped off the semantic stack, and the resulting structure, rooted at u, is pushed on M . • M-LEFT(r) creates a semantic dependency with label r in the reverse direction as MRIGHT. The buffer front, v is replaced by the new v-rooted structure. M remains unchanged.
Because SRL graphs allow a node to be a semantic argument of two parents—like all in the
1This works better for the arc-eager algorithm (Ballesteros and Nivre, 2013), in contrast to Henderson et al. (2013), who initialized with root at the buffer front.
2Note that in the original arc-eager algorithm (Nivre, 2008), SHIFT and RIGHT-ARC actions move the item on the buffer front to the stack, whereas we only copy it (to allow the semantic operations to have access to it).
3
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
example in Figure 1—M-LEFT and M-RIGHT do not remove the dependent from the semantic stack and buffer respectively, unlike their syntactic equivalents, S-LEFT and S-RIGHT. We use two other semantic transitions from Henderson et al. (2013) which have no syntactic analogues:
• M-SWAP swaps the top two items on M , to allow for crossing semantic arcs. • M-PRED(p) marks the item at the front of B as a semantic predicate with the sense p, and replaces it with the disambiguated predicate.
The CoNLL 2009 corpus introduces semantic self-dependencies where many nominal predicates (from NomBank) are marked as their own arguments; these account for 6.68% of all semantic arcs in the English corpus. An example involving an eventive noun is shown in Figure 2. We introduce a new semantic transition to handle such cases:
• M-SELF(r) adds a dependency, with label r between the item at the front of B and itself. The result replaces the item at the front of B.
Note that the syntactic and semantic transitions both operate on the same buffer, though they independently specify the syntax and semantics, respectively. In order to ensure that both syntactic and semantic parses are produced, the syntactic and semantic transitions are interleaved. Only syntactic transitions are considered until a transition is chosen that copies an item from the buffer front to the syntactic stack (either S-SHIFT or S-RIGHT). The algorithm then switches to semantic transitions until a buffer-modifying transition is taken (M-SHIFT).3 At this point, the buffer is modified and the algorithm returns to syntactic transitions. This implies that, for each word, its left-side
3Had we moved the item at the buffer front during the syntactic transitions, it would have been unavailable for the semantic transitions, hence we only copy it.
syntactic dependencies are resolved before its leftside semantic dependencies. For interested readers, an example run of the algorithm is given in §A.To ensure that the parser never enters an invalid state, the sequence of transitions is constrained, following Henderson et al. (2013). Actions that copy or move items from the buffer (S-SHIFT, S-RIGHT and M-SHIFT) are forbidden when the buffer is empty. Actions that pop from a stack (S-REDUCE and M-REDUCE) are forbidden when that stack is empty. We disallow actions corresponding to the same dependency, or the same predicate to be repeated in the sequence. Repetitive M-SWAP transitions are disallowed to avoid infinite swapping. Finally, as noted above, we restrict the parser to syntactic actions until it needs to shift an item from B to S, after which it can only execute semantic actions until it executes an M-SHIFT.
Asymptotic runtime complexity of this greedy algorithm is linear in the length of the input, as analyzed by Nivre (2009).The transitions in §2 describe the execution paths our algorithm can take; like past work, we apply a statistical classifier to decide which transition to take at each timestep, given the current state. (A full example of a transition sequence is given in the supplementary material.) The novelty of our model is that it learns a finite-length vector representation of the entire joint parser’s state (S, M , and B) in order to make this decision.LSTMs are recurrent neural networks equipped with specialized memory components in addition to a hidden state (Hochreiter and Schmidhuber, 1997; Graves, 2013) to model sequences. Stack LSTMs (Dyer et al., 2015) are LSTMs that allow for stack-based operations: query, push, and pop. A “stack pointer” is maintained which determines which cell in the LSTM provides memory unit and the hidden unit when computing the new memory cell contents. Query provides a summary of the stack in a single fixed-length vector. Push adds an element to the top of the stack, resulting in a new summary. Pop, which does not correspond to
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
St Mt Bt Action St+1 Mt+1 Bt+1 Dependency S M (v, v), B S-SHIFT (v, v), S M (v, v), B —
(u, u), S M B S-REDUCE S M B — (u, u), S M (v, v), B S-RIGHT(`) (v, v), (gs(u,v, l), u), S M (v, v), B S [ u
`! v (u, u), S M (v, v), B S-LEFT(`) S M (gs(v,u, l), v), B S [ u
` v S M (v, v), B M-SHIFT S (v, v),M B — S (u, u),M B M-REDUCE S M B — S (u, u),M (v, v), B M-RIGHT(r) S (gm(u,v, r), u),M (v, v), B M [ u
r! v S (u, u),M (v, v), B M-LEFT(r) S (u, u),M (gm(v,u, r), v), B M [ u
r v S (u, u), (v, v),M B M-SWAP S (v, v), (u, u),M B — S M (v, v), B M-PRED(p) S M (gd(v,p), v), B — S M (v, v), B M-SELF(r) S M (gm(v,v, r), v), B M [ v r$ v
Table 1: Parser transitions along with the modifications to the stacks and the buffer resulting from each. Syntactic transitions are shown above, semantic below. Script symbols denote symbolic representations of words and relations, and bold symbols indicate (learned) embeddings (§3.5) of words and relations; each element in a stack or buffer includes both symbolic and vector representations, either atomic or recursive. S represents the set of syntactic transitions; M the set of semantic transitions.
a conventional LSTM operation, moves the stack pointer to the preceding timestep, resulting in a stack summary as it was before the popped item was observed. Implementation details (Dyer et al., 2015; Goldberg, 2015) and code were made publicly available.4
Using stack LSTMs, we construct a representation of the algorithm state by decomposing it into smaller pieces that are combined by recursive function evaluations (similar to the way a list is built by a concatenate operation that operates on a list and an element). This enables information that would be distant from the “top” of the stack to be carried forward, potentially helping the learner.Our algorithm employs four stack LSTMs, one each for the S, M , and B data structures, so that already-built partial structures are available to the classifier. Like Dyer et al. (2015), we use a fourth stack LSTM, A, for the history of actions—A is never popped from, only pushed to. Figure 3 illustrates the architecture. The algorithm’s state at timestep t is encoded by the four vectors summarizing the four stack LSTMs, and this is the input to the classifier that chooses among the allowable transitions at that timestep.
Let st, mt, bt, and at denote the summaries of St, Mt, Bt, and At, respectively. Let At = Allowed(St,Mt, Bt, At) denote the allowed transitions given the current stacks and buffer. The parser state at time t is given by a rectified linear unit (Nair and Hinton, 2010) in vector yt:
yt = elementwisemax {0,d+W[st;mt;bt,at]}
4 https://github.com/clab/lstm-parser
root soon reopen to
all
are expected
expect.01 all
all
sbj
A1
M
B
S
S-RIGHT (oprd)
... M-PRED
(expect.01) M-REDUCE M-LEFT
(A1)
A
are
vc
M-SHIFT
Figure 3: Stack LSTM for joint parsing. The state illustrated corresponds to the ***-marked row in the example transition sequence in Fig. 5 in the supplementary material.
where W and d are the parameters of the classifier. The transition selected at timestep t is
argmax ⌧2At q⌧ + ✓⌧ · yt (1)
⌘ argmax ⌧2At score(⌧ ;St,Mt, Bt, At) (2)
where ✓⌧ and q⌧ are parameters for each transition type ⌧ . Note that only allowed transitions are considered in the decision rule (see §2.3).To use stack LSTMs, we require vector representations of the elements that are stored in the stacks. Specifically, we require vector representations of atoms (words, possibly with part-of-speech tags) and parse fragments. Word vectors can be pretrained or learned directly; we consider a concate-
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
nation of both in our experiments; part-of-speech vectors are learned and concatenated to the same.
To obtain vector representations of parse fragments, we use neural networks which recursively compute representations of the complex structured output (Dyer et al., 2015). The tree structures here are always ternary trees, with each internal node’s three children including a head, a dependent, and a label. The vectors for leaves are word vectors and vectors corresponding to syntactic and semantic relation types.
The vector for an internal node is a squashed (tanh) affine transformation of its children’s vectors. For syntactic and semantic attachments, respectively, the composition function is:
gs(v,u, l) = tanh(Zs[v;u; l] + es) (3) gm(v,u, r) = tanh(Zm[v;u; r] + em) (4)
where v and u are vectors correspodning to atomic words or composed parse fragments; l and r are learned vector representations for a syntactic and semantic labels respectively. Syntactic and semantic parameters are separated (Zs, es and Zm, em, respectively).
Finally, for predicates, we use another recursive function to compose the word representation, v with a learned representation for the dismabiguated sense of the predicate, p:
gd(v,p) = tanh(Zd[v;p] + ed) (5)
where Zd and ed are parameters of the model. Note that, because syntactic and semantic transitions are interleaved, the fragmented structures are a blend of syntactic and semantic compositions. Figure 4 shows an example.Training the classifier requires transforming each training instance (a joint parse) into a transition sequence, a deterministic operation under our transition set. Given a collection of algorithm states at time t and correct classification decisions ⌧t, we minimize the sum of log-loss terms, given (for one timestep) by:
log exp(q⌧t + ✓⌧t · yt)P ⌧ 02At exp(q⌧ 0 + ✓⌧ 0 · yt)
(6)
with respect to the classifier and LSTM parameters. Note that the loss is differentiable with respect to the parameters; gradients are calculated
using backpropagation. We apply stochastic gradient descent with dropout for all neural network parameters.Following Dyer et al. (2015), “structured skipgram” embeddings (Ling et al., 2015) were used, trained on the English (AFP section), German, Spanish and Chinese Gigaword corpora, with a window of size 5; training was stopped after 5 epochs. For out-of-vocabulary words, a randomly initialized vector of the same dimension was used.Predicate sense disambiguation is handled within the model (M-PRED transitions), but since senses are lexeme-specific, we need a way to handle unseen predicates at test time. When a predicate is encountered at test time that was not observed in training, our system constructs a predicate from the predicted lemma of the word at that position and defaults to the “01” sense, which is correct for 91.22% of predicates by type in the English CoNLL 2009 training data.Our model is evaluated on the CoNLL shared tasks on joint syntactic and semantic dependency parsing in 2008 (Surdeanu et al., 2008) and 2009 (Hajič et al., 2009). The standard training, development and test splits of all datasets were
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
used. Per the shared task guidelines, automatically predicted POS tags and lemmas provided in the datasets were used for all experiments. As a preprocessing step, pseudo-projectivization of the syntactic trees (Nivre et al., 2007) was used, which allowed an accurate conversion of even the nonprojective syntactic trees into syntactic transitions. However, the oracle conversion of semantic parses into transitions is not perfect despite using the MSWAP action due to the presence of multiple crossing arcs.5
The standard evaluation metrics include the syntactic labeled attachment score (LAS), the semantic F1 score on both in-domain (WSJ) and outof-domain (Brown corpus) data, and their macro average (Macro F1) to score joint systems. Because the task was defined somewhat differently in each year, each dataset is considered in turn.The CoNLL 2008 dataset contains annotations from the Penn Treebank (Marcus et al., 1993), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The shared task evaluated systems on predicate identification in addition to predicate sense disambiguation and SRL.
To identify predicates, we trained a zeroMarkov order bidirectional LSTM two-class classifier. As input to the classifier, we use learned representations of word lemmas and POS tags. This model achieves an F1 score of 91.43% on marking words as predicates (or not).
Hyperparameters The input representation for a word consists of pretrained embeddings (size 100 for English, 80 for Chinese, 64 for German and Spanish), concatenated with additional learned word and POS tag embeddings (size 32 and 10, resp.). Learned embeddings for syntactic and semantic arc labels are of size 20 and predicates 100. Two-layer LSTMs, with hidden state dimension 100 were used for each of the four stacks. The parser state, yt and the composition function, g are of dimension 100. A dropout rate of 0.2 (Zaremba et al., 2014) was used on all layers at training time, tuned on the dev data from the set of values {0.1, 0.2, 0.3, 1.0}. The learned representations for actions are of size 100, similarly tuned from {10, 20, 30, 40, 100}. Other hyperpa-
5For 1.5% of English sentences in the CoNLL 2009 English dataset, the transition sequence incorrectly encodes the gold-standard joint parse; details in Henderson et al. (2013).
rameters have been set intuitively; careful tuning is expected to yield improvements (Weiss et al., 2015).
An initial learning rate of 0.1 for stochastic gradient descent was used and updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the dev performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB.Relative to the CoNLL 2008 task (above), the main change in 2009 is that predicates are preidentified, and systems are only evaluated on predicate sense disambiguation (not identification). Hence, the bidirectional LSTM classifier is not used here. The preprocessing for projectivity, and the hyperparameter selection is the same as in §4.1.
In addition to the joint approach described in the preceding sections, we experiment here with several variants:
Semantics-only: all syntactic transitions in S , the syntactic stack S, and the syntactic composition function, gs are discarded. As a result, the set of constraints on transitions is a subset of the full set of constraints in §2.3. Effectively, this model does not use any syntactic features, similar to Collobert et al. (2011) and Zhou and Xu (2015). It provides a controlled test of the benefit of jointly predicting syntax in a semantic parser.
Syntax-only: all semantic transitions in M, the semantic stack M , and the semantic composition function gm are discarded. S-SHIFT and S-RIGHT now move the item from the front of the buffer to the syntactic stack, instead of copying. The set of constraints on the transitions is again a subset of the full set of constraints. This model is an arceager variant of Dyer et al. (2015), and serves to check whether semantic parsing degrades syntactic performance.
Hybrid: the semantics parameters are trained using automatically predicted syntax from the syntax-only model. At test time, only semantic parses are predicted. This setup bears similarity to other approaches which pipeline syntax and semantics, extracting features from the syntactic parse to help SRL. However, unlike other
7
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
approaches, this model does not offer the entire syntactic tree for feature extraction, since only the partial syntactic structures present on the syntactic stack (and potentially the buffer) are visible at a given timestep.","CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of transitions is derived, showing the benefit of learning a representation for the entire algorithmic state. Several other joint learning models have been proposed (Lluı́s and Màrquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models. The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system’s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower.
CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013); demonstrating the benefit of our entire-parser-state representation learner compared to the more locally scoped model.
Given that syntax has consistently proven useful
in SRL, our Semantics-only model underperforms Hybrid and Joint, as expected. In the training domain, syntax and semantics benefit each other (Joint outperforms Hybrid). Out-of-domain (the Brown test set), the Hybrid pulls ahead, a sign that Joint overfits to WSJ. As a syntactic parser, our Syntax-only model performs slightly better than Dyer et al. (2015), who achieve 89.56 LAS on this task.
The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who also carefully design languagespecific features, and use a series of pipelines for the joint task, resulting in an accurate but computationally expensive system.
State-of-the-art SRL systems (shown in the last block of Table 3) which use advancements orthogonal to the contributions in this paper, perform better than our models. Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Björkelund et al., 2009; Björkelund et al., 2010; Roth and Woodsend, 2014); we have not used these features or reranking. Lei et al. (2015) use syntactic parses to obtain interaction features between predicates and their arguments and then compress feature representations using low-rank tensor. Täckström et al. (2015) present an exact inference algorithm for SRL based on dynamic programming and their local and structured models make use of many syntactic features from a pipeline; our search procedure is greedy. Their algorithm is adopted by FitzGerald et al. (2015) for inference in a model that jointly learns representations from a combination of PropBank and FrameNet annotations; we have not experimented with extra annotations.
Our system achieves an end-to-end runtime of 177.6±18 seconds to parse the CoNLL 2009 English test set on a single core. This is almost 2.5 times faster than the pipeline model of Lei et al. (2015) (439.9±42 seconds) under identical settings.6
CoNLL 2009 Multilingual (Table 4) We tested the joint model on the non-English CoNLL 2009 datasets, and the results demonstrate that it adapts
6See https://github.com/taolei87/ SRLParser; we chose this system since it is publicly available, and other state-of-the-art systems are not.
8
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
easily—it is on par with the top three systems in most cases. We note that our Chinese parser relies on pretrained word embeddings for its superior performance; without them (not shown), it was on par with the others. Japanese is a small-data case (4,393 training examples), illustrating our model’s dependence on reasonably large training datasets.
We have not extended our model to incorporate morphological features, used by the systems to which we compare. Future work might incorporate morphological features where available; this could potentially improve performance, especially in highly inflective languages like Czech. An alternative might be to infer word-internal representations using character-based word embeddings, which was found beneficial for syntactic parsing (Ballesteros et al., 2015).","We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our multilingual results are comparable to the top systems at CoNLL 2009.
Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Lluı́s et al., 2013, inter alia).
One reason pipelines often dominate is that they make available the complete syntactic parse tree,
and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013).
This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing.
We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design.
Our system’s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Björkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (Täckström et al., 2015), or of systems that exploit additional, differently-annotated datasets (FitzGerald et al., 2015). Many advances in those systems are orthogonal to our model, and we expect future work to achieve further gains by integrating them.
Because our system is very fast—with an endto-end runtime of 177.6±18 seconds to parse the
2
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
CoNLL 2009 English test data on a single core— we believe it will be useful in practical settings. An open-source implementation will be made available on publication.","Other approaches to joint modeling, not considered in our experiments, are notable. Lluı́s et al. (2013) propose a graph-based joint model using dual decomposition for agreement between syntax and semantics, but do not achieve competitive performance on the CoNLL 2009 task. Lewis et al. (2015) proposed an efficient joint model for CCG syntax and SRL, which performs better than a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation is infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset.
There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and Màrquez, 2005), their approach inspired Zhou and Xu (2015), who achieved state-of-the-art results using deep bidirectional LSTMs.7 Our approach for dependency-based SRL is not directly comparable.",,"We presented an incremental, greedy parser for joint syntactic and semantic dependency parsing. Our model surpasses the performance of previous joint models on the CoNLL 2008 and 2009 English tasks, despite not using expert-crafted features of the full syntactic parse, as done by asymptotically more expensive models. An open-source implementation of our parser will be made available on publication."
11,"Sentiment Analysis has nowadays a crucial role in social media analysis and, more generally, in analyzing user opinions about general topics or user reviews about product/services, enabling a huge number of applications. Many methods and software implementing different approaches exist and there is not a clear best approach for Sentiment classification/quantification. We believe that performance reached by machine learning approaches is a key advantage to apply to sentiment analysis in order to reach a performance which is very close to the one obtained by a group of humans, who evaluate subjective sentences such as user reviews. In this paper, we present the results of our experimental evaluation of both research and commercial state-of-the-art tools for Sentiment Analysis, considering as benchmarks, both user reviews related to the evaluation of apps published to app stores and tweets. Thus, we show that our tools App2Check and Tweet2Check --developed mainly applying supervised learning techniques-- are the best tools for sentiment evaluation on apps reviews and tweets for both Italian and English language, compared to the state-of-the-art research tools.",App2Check and Tweet2Check: machine learning-based tools for Sentiment Analysis of Apps Reviews and Tweets,86,"No details are provided on the methods used in this paper to produce the
results, due to issues of 'non-disclosure restrictions'.  If the reader doesn't
know the learning algorithm or the training data (or other resources made use
of in the approach), then there is nothing in the paper to help with the
reader's own sentiment analysis methods, which is why we share research.  This
is not a research paper, hence does not belong in this conference.  Perhaps a
submission to a demo session somewhere would be a good idea.  Even with a demo
paper, however, you would need to share more details about the methods used
than you do here.",,1,1,Poster,2,2,1,5,2,2,1,2,2016,"In this section we describe our tools and briefly describe iFeel (Araújo et al., 2014) (v.2.0), a research platform which allows to perform experimental evaluation on 19 state-of-the-art research tools for sentiment analysis, SentiStrength for Italian, which we used to perform our experimental evaluation on Italian language –which is currently our main focus– and on English language, and we describe our systems .App2Check and Tweet2Check implement supervised learning techniques that allowed us to create two predictive models for sentiment quantification: one specialized on apps reviews and one on tweets. Training of predictive models is performed by considering a huge variety of language domains and different kinds of user reviews. Both tools provide, as answer to a sentence in Italian or English language, a quantification of the sentiment polarity scored from 1 to 5, according to the most recent trend shown in the last sentiment evaluation SemEval (Sem, 2016), where tracks considering quantification have been introduced. Thus, we consider the following quantification: as positive, sentences with score 4 (positive) or 5 (very positive); as negative, sentences with score 1 (very negative) or 2 (negative); as neutral, sentences with score 3. In order to compute the final answer, App2Check/Tweet2Check also apply a set of algorithms which take into account some natural language processing techniques, allowing e.g. to also automatically perform topic/named entity extraction. It is not possible to give more details on the engine due to non-disclosure restrictions.
App2Check and Tweet2Check are not only constituted by a web service providing access to the sentiment prediction of sentences (with free trial for research purposes), but also provide a full userfriendly web application whose features are out of the scope of this paper. A full demo of the tools will be available for paper presentation.iFeel is a research web platform (Araújo et al., 2016; Araújo et al., 2014) allowing to run multiple sentiment analysis tools on the specified list of sentences. It allows to natively run tools support-
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
ing English and to first translate sentences from other languages to English and then run 19 tools on the English translated sentences. Since it has been experimentally shown in (Araújo et al., 2016) that well known language specific methods do not have a significant advantage over a simple machine translation approach before sentiment evaluation, and since most of the research tools do not have a publicly available Italian version, the results of the tools ran by iFeel are used for both Italian and English. However, we also considered for the comparison the only research tool we found which natively processes Italian language (Sentistrenght) and it is our reference tool with no translation before sentiment evaluation.
The tools included in iFeel are (in alphabetical order): AFINN, Emolex, Emoticon Distance Supervised, Emoticons, Happiness Index, NRC Hashtag, Opinion Finder, Opinion Lexicon, Panas-t, SANN, SASA, Senticnet, Sentiment140, SentiStrength, SentiWordNet, SO-CAL, Stanford Deep Learning, Umigon, Vader.SentiStrength was produced as part of the CyberEmotions project, supported by EU FP7. It estimates the strength of positive and negative sentiment in short texts, even for informal language. According to the authors, it has human-level accuracy for short social web texts in English, except political texts (Thelwall et al., 2010). SentiStrength authors make available a version of the tool which natively manages Italian language. All tests have been carried out with both average emotion and strongest emotion options, but in this paper we only report the results obtained with the latter option turned on, due to better performance. Since the English version of SentiStrength is also included in iFeel, in the following we call the Italian version SentiStrengthIta.","In our experimental evaluation we evaluated App2Check and Tweet2Check sentiment predictors on user reviews of apps from Apple App Store and Google Play Store and on tweets, for both Italian and English language.
Apps reviews benchmark set is constituted by: i. Test sets A-ita and A-eng , both constituted by 1 thousand comments from the famous Candy Crush Saga app, respectively in Italian and En-
glish. We performed a manual quantification (in the 1-5 range) of the sentiment, performed by a trained person, in order to have a reference sentiment score (from now on called human sentiment quantification, or HSQ). ii. Test set B made of 10 thousands comments from the following 10 different very popular apps (one thousand comments per app).
• Test set B-ita: Angry Birds, Banco Posta, Facebook, Fruit Ninja, Gmail, Mobile Banking Unicredit, My Vodafone, PayPal, Twitter, Whatsapp.
• B-eng: Candy Crush Soda Saga, Chase Mobile, Clash Of Clans, Facebook Messenger, Gmail, Instagram, My Verizon, PayPal, Snapchat and Wells Fargo.
Every comment has a score, called app rating, associated to the app.
Tweet benchmark set is constituted by 3899 tweets in Italian language from (Araújo et al., 2016) (and (Sem, 2016)) and 1000 tweets in English randomly selected from the English dataset from the same source. While the latter does not contain the neutral class, the former contains also neutral sentences.
All tables show columns macro F1 (MF1), accuracy (Acc), F1 on each class (resp. F1(-) for negative, F1(x) for neutral –if present–, and F1(+) for positive), and are sorted by macro F1, considered as scoring system for tools. In all of the tables We highlight in bold the winner tool per column. We ran App2Check/Tweet2Check, the 19 research tools through iFeel on all test sets, and SentiStrengthIta on Test set A-ita, Test set B-ita and Italian tweets, in order to add a comparison with a tool supporting Italian.
All of the benchmarks are submitted with the paper, together with a demo access to App2Check and Tweet2Check prediction web services, in order to extend or make repeatable the experiments.Results shown in Table 1 highlight that App2Check outperforms all of the research tools in Italian with respect to HSQ, overcoming the theoretical 80% of accuracy; in Table 2 is shown that our tool is better than the research ones also for English apps reviews. Very good results are obtained in both languages by Sentiment 140, which has the best macro-F1 and accuracy among
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
Tool M-F1 Acc F1(-) F1(x) F1(+) App2Check 65.8 81.8 85.9 25.2 86.4 Sentiment140 58.1 71.6 80.4 21.4 72.5 SentiWordNet 57.2 67.3 73.5 26.6 71.4 Stanford DL 53.7 60.5 70.0 21.2 69.9 NRC Hashtag 52.9 81.0 76.4 16.4 66.0 Umigon 50.9 56.4 54.0 20.5 78.1 Sentistrength 50.4 57.8 47.5 25.6 78.0 Op. Lexicon 49.8 54.2 53.1 23.7 72.6 AFINN 49.7 57.4 52.4 21.8 74.9 Vader 40.9 42.8 31.9 22.8 68.1 Senticnet 40.2 50.6 37.4 19.7 63.6 Emolex 40.0 43.3 43.1 16.1 60.9 SASA 39.3 44.3 40.8 15.8 61.3 SO-CAL 39.0 40.8 45.2 17.0 54.8 SentiStr. Ita 38.2 41.5 34.3 20.5 59.7 H. Index 32.3 39.8 16.9 18.6 61.4 Op. Finder 23.6 22.9 24.9 18.1 27.9 Emoticon DS 21.5 41.6 1.8 4.1 58.7 SANN 16.5 17.8 1.8 19.9 27.7 Panas-t 7.4 11.0 1.3 18.7 2.2 Emoticons - 10.3 - 18.6 -
Table 1: Comparison on 1K Candy Crush Saga app reviews in Italian wrt HSQ
Tool M-F1 Acc F1(-) F1(x) F1(+) App2Check 61.9 74.5 82.0 23.2 80.5 Sentiment140 59.9 70.1 77.2 29.7 73.0 Stanford DL 57.6 61.9 71.2 33.3 68.3 NRC Hashtag 56.8 64.9 75.6 31.7 63.2 Umigon 56.4 59.9 59.1 33.3 76.8 Op. Lexicon 53.0 55.6 54.7 36.5 67.7 AFINN 52.8 57.6 54.1 33.6 70.9 SentiWordNet 52.5 58.9 63.4 31.5 62.5 SentiStrength 46.3 49.9 44.0 29.1 65.9 SO-CAL 43.1 44.5 50.4 27.2 51.6 Emolex 41.3 43.1 38.0 32.6 53.4 Vader 40.4 43.0 24.8 31.8 64.7 SASA 40.0 44.4 40.0 19.2 60.7 Senticnet 38.1 43.0 32.9 29.0 52.4 H. Index 33.9 40.1 19.7 26.4 55.7 Op. Finder 27.9 66.9 29.5 26.3 28.0 SANN 19.2 22.8 0.9 29.9 26.8 Panas-t 11.1 17.7 3.6 29.1 0.5 Emoticon DS - 39.9 - 15.8 56.2 Emoticons - 16.8 - 28.8 -
Table 2: Comparison on 1K Candy Crush Saga app reviews in English wrt HSQ
research tools, and is closer to App2Check on Test set A-eng. We can see that for all of the tools is hard to identify neutral reviews, according to the low F1(x) score, despite a good performance on positive and negative class.
We experienced that, as already said, considering a single comment, the score/rating expressed by a user respect to an app can be in general substantially different respect to the sentiment expressed by a human. However, the average score/rating of many (hundreds of) comments can
be an approximation of the average sentiment expressed by a human on the same set. Indeed, we compared HSQ with Candy Crush Saga app rating and they agree on about 80% of cases: this data allows us to consider app rating as an approximated indicator of sentiment when averaging thousands of comments.
Tool M-F1 Acc F1(-) F1(x) F1(+) App2Check 73.3 85.7 82.7 45.6 91.7 SentiWordNet 47.9 65.9 60.4 6.2 77.1 AFINN 47.5 60.3 49.2 16.6 76.7 SentiStrength 47.5 59.7 46.3 19.3 76.8 Stanford DL 45.6 54.0 56.5 13.5 66.8 Op. Lexicon 44.9 55.3 45.1 17.5 72.2 Sentiment140 44.1 58.7 57.4 6.7 68.2 Umigon 42.8 50.1 47.8 14.6 66.2 SO-CAL 41.8 49.3 45.8 13.8 65.6 NRC Hashtag 41.2 65.7 53.6 8.3 61.7 Senticnet 40.9 63.1 36.6 9.1 76.9 Vader 38.5 46.2 29.5 19.7 66.3 Emolex 38.3 45.5 38.5 14.1 62.3 SASA 37.7 48.8 29.6 16.4 67.1 SentiStr. Ita 34.0 39.6 32.0 13.9 56.2 H. Index 31.1 39.0 21.9 13.4 57.9 Emoticon DS 27.8 63.6 3.0 2.5 77.8 Op. Finder 26.0 26.6 25.3 15.4 37.2 SANN 12.2 14.9 3.6 16.5 16.4 Panas-t 6.0 9.0 1.2 16.0 0.9 Emoticons 5.4 8.8 0.1 15.9 0.4
Table 3: Comparison on 10.000 apps reviews in Italian wrt app rating.
Tool M-F1 Acc F1(-) F1(x) F1(+) App2Check 53.4 61.2 66.1 22.5 71.7 Umigon 48.8 51.1 52.7 29.4 64.4 Stanford DL 47.5 51.4 60.7 24.2 57.4 AFINN 45.5 49.9 46.0 27.3 63.4 Op. Lexicon 45.0 48.1 43.4 28.5 63.2 SentiStrength 45.0 47.5 42.9 30.1 62.0 SO-CAL 42.4 44.7 46.5 25.0 55.7 Sentiment140 42.0 53.7 64.3 9.2 52.5 Vader 40.5 42.1 26.8 34.3 60.4 NRC Hashtag 40.5 51.8 63.4 9.2 48.9 SentiWordNet 40.3 50.8 52.9 8.0 60.0 Emolex 39.3 40.7 37.9 27.6 52.3 SASA 38.0 40.2 34.8 26.4 52.7 H. Index 34.4 37.7 22.2 28.3 52.7 Senticnet 33.5 44.1 32.7 10.7 57.1 Op. Finder 33.1 32.9 30.6 29.9 38.7 Emoticon DS 20.1 39.6 1.5 2.0 56.7 SANN 17.2 23.5 1.5 34.0 16.1 Panas-t 12.1 20.4 2.1 33.2 1.1 Emoticons - 20.0 - 33.4 0.1
Table 4: Comparison on 10.000 apps reviews in English wrt app rating.
In Tables 3 and 4 we extend our evaluation respectively on Test set B-ita and Test set B-eng with respect to app rating, in order to have a reference
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
indicator when HSQ is not available. We can see that App2Check generalizes well when tested on many 10 thousand reviews and is the best tool for all measures on the Italian test set, outperforming the research tools with about a 20% of better accuracy from the second tool in the table, and shows also a 45% of better accuracy respect to SentiStrenght for Italian. App2Check is also the best tool for the English app reviews on all but the F1(x) measure.
On these test sets, it is difficult to identify the best research tool or the tool having results that are similar to the ones obtained by App2Check. Also Sentiment140, which has been the second best tool (and the best among the research ones) on Candy Crush Saga reviews, is the seventh and the eighth on, respectively, Test set B-ita and Test set B-eng, even if it still has the highest accuracy among the research tools. We can see that its low macro-F1 is due to the bad results on neutral sentences, which, as previously noticed, are the most difficult to identify.
Figure 1: Sentiment quantification plot on 10 thousand app reviews in Italian language.
Figure 2: Sentiment quantification plot on 10 thousand app reviews in English language.
In order to understand the reason why App2Check is so much better than the other tools on apps reviews, we show in Figures 1 and 2 two plots of the average sentiment per month of the comments, which have been chronologically
sorted. In order to obtain a better representation of the plot, only the most significant research tools are plotted together with Rating and App2Check.
In Figure 1, we choose SentiWordNet (SWN in the plot), which has the best Macro-F1 on this test set, NRC Hashtag (NRC H.), which has the best Accuracy, and SentiStrengthIta (SS. ITA), which is the only research tool supporting Italian. We can see that all tools have similar trend, but NRC Hashtag plot is widely shifted down, and the points in the plots of both SentiWordNet and SentiStrengthIta are closer to the global average sentiment. App2Check plot, instead, is almost overlapped to the rating plot.
In Figure 2, we choose Umigon and Stanford Deep Learning (Stanford DL), which have the best Macro-F1, and Sentiment140 (S140), which has the best accuracy, despite its low Macro-F1. In order to have a clearer chart, we show only the last 12 months. First of all, we can see that Sentiment140 plot highlights a tendency to classify sentences as negative. Once again we can see that all tools follows the trend of the rating, but Stanford Deep Learning is, except for 2015/12, on average more negative than the rating, and Umigon has a plot closer to the rating among the research tools. App2Check is again the closest to the app rating.
We think that App2Check on Test set B has even higher accuracy, considering as a reference the HSQ instead of the app rating: this is made clear while using the web application and evaluating the system answers on every single user comment.The first highlighted rows (with *) in Tables 5, 6 and 7 show our tools feature –native thanks to the machine learning approach–, allowing to learn and extend our model with the provided examples. This means that the predicted model has been updated including in the training set also the sentences belonging to the specific benchmarks under evaluation. We show such results for tweets since, compared to app reviews, all of the tools show a lower accuracy which is always under the theoretical 80%. In this cases, having a tool allowing a training on a specific domain, may in general help to reach a higher accuracy: this is useful when a client asks for a high accuracy on a specific domain, e.g. politics, movies, etc. In this cases, our tool –and in general– machine learning-based approaches– allow to meet this important goal by
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
performing learning on a subset of sentences of the required domain, while different approaches with no learning feature cannot reach it or anyway reach it with more effort.
Tool MF1 Acc F1(-) F1(x) F1(+) Tweet2Check* 88.2 88.7 89.0 89.8 85.9 Tweet2Check 46.0 46.4 52.1 39.3 46.6 SentiStr. Ita 44.7 46.7 39.4 54.3 40.5 SentiStrength 41.4 43.1 38.8 52.9 32.5 SO-CAL 40.4 42.7 37.9 54.1 29.3 AFINN 40.4 42.2 36.3 54.4 30.5 Umigon 40.1 46.3 30.4 61.7 28.2 Op. Lexicon 39.0 42.9 32.9 56.6 27.6 Op. Finder 37.0 44.9 27.7 60.1 23.0 Vader 36.1 44.0 21.4 60.1 26.9 Emolex 36.0 37.3 39.6 42.1 26.2 SASA 35.7 37.2 30.8 48.1 28.1 Stanford DL 35.0 38.3 44.3 38.3 22.4 H. Index 33.0 36.3 20.0 49.0 29.9 SentiWordNet 32.0 32.9 40.3 24.4 31.2 Sentiment140 29.3 33.5 49.3 12.3 26.4 NRC Hashtag 28.3 35.1 50.5 11.8 22.6 Senticnet 25.1 26.8 28.2 14.8 32.2 Emoticons 22.5 42.9 1.0 60.0 6.6 Panas-t 22.1 42.6 1.6 59.7 5.0 SANN 20.4 42.7 0.7 59.8 0.7 Emoticon DS 12.1 21.0 0.6 1.4 34.4
Table 5: Comparison on 3899 tweets in Italian
Evaluating tools on tweets in Italian language, we can see in Table 5 that Tweet2Check is better than research tools with respect to M-F1, but in this case tools show a closer performance, and the Italian version of SentiStrength has almost the same (slightly higher) accuracy.
In table 6 we show the results on the same test set in a different way, i.e. excluding the tweets that have been manually classified as neutral, since this helps to clear the influence of the neutral class on the performance of all systems. Tweets, indeed, can be as said in general very different than app reviews, and tweets manually labeled as neutral may not contain any opinion and being objective sentences; these sentences can be more difficult to recognize as neutral by tools.
We can see that the MF1 ranking changed, but Tweet2Check is still the best tool, and obtained a Macro-F1 that is 12% higher than Sentiment140, which, once again, is the second best tool.
Finally, in Table 7, results for English tweets are reported, and we can see that, considering only the macro-F1 measure, three tools, Tweet2Check, AFINN, and SentiStrength outperform the others and obtained similar scores, but, considering accuracy too, Tweet2Check is the best one, since it reaches a score of 78.4%, while AFINN, which is
Tool MF1 Acc F1(-) F1(+) Tweet2Check* 90.5 86.7 91.4 89.7 Tweet2Check 63.3 57.8 66.9 59.7 Sentiment140 51.1 53.1 63.8 38.4 NRC Hashtag 49.5 56.0 69.5 29.5 SentiStr. Ita 47.7 36.5 45.5 50.0 SentiWordNet 47.2 46.0 50.5 43.9 Alchemy 45.9 34.9 36.5 55.3 Semantria 45.2 33.0 40.7 49.7 SentiStrength 43.3 36.9 46.8 39.7 Stanford DL 42.8 43.1 60.3 25.3 Emolex 41.8 36.4 49.7 34.0 SO-CAL 40.4 33.5 43.5 37.4 AFINN 40.3 34.2 42.1 38.5 Senticnet 40.3 40.3 31.3 49.3 SASA 36.5 29.8 34.8 38.1 Op. Lexicon 35.7 27.9 37.4 33.9 Umigon 32.9 24.4 33.4 32.4 H. Index 31.3 24.0 21.9 40.7 Op. Finder 28.3 19.9 30.6 26.1 Vader 27.1 18.5 22.9 31.4 Emoticon DS 26.8 36.0 0.6 53.1 Emoticons 3.9 1.7 1.0 6.7 Panas-t 3.4 1.5 1.7 5.1 SANN 0.7 0.4 0.7 0.7
the second best tool, only reached 70.9%.","Sentiment Analysis has nowadays a crucial role in social media analysis and, more generally, in analysing user opinions about general topics or user reviews about product/services, enabling a huge number of applications. For instance, sentiment analysis can be applied to monitoring the
reputation or opinion of a company or a brand with the analysis of reviews of consumer products or services (Hu and Liu, 2004), marketing campaigns in politics (Tumasjan et al., 2010), and financial applications (Oliveira et al., 2013) (Bollen et al., 2010). App stores can be seen as another, not yet well explored, field of application of sentiment analysis. Indeed, they are another social media where users can freely express their own opinion through app reviews about a product, i.e. the specific app under evaluation, or a service, to which the considered app is connecting the user (e.g., a mobile banking app connects users to mobile banking services). In addition, reading user reviews on app stores shows that people frequently talk about and evaluate also the brand associated to the app under review: thus, it is possible to extract people opinion about a brand or the sentiment about a company or the provided service quality.
Twitter, on the other hand, is one of the most popular and largely used social networks which is very interesting to monitor from the perspective of sentiment analysis, and that is already considered in research challenges in this research area.
In this paper, we focus on the app store as a social media platform and on the sentiment evaluation of app reviews and tweets. The former are examples of reviews related to a product, or a service or the associated brand; the latter can include more general sentences, which can be significantly different from reviews as for app reviews, and can be even more difficult to evaluate respect to sentiment analysis. On the other hand, apps reviews are also interesting because they include a score which is not available for tweets. Moreover, in apps reviews, the sentiment score detected in a comment can significantly differ from the score assigned by the user to the app under evaluation. For example, a user can assign his good score to the app (i.e. assigning 5 stars) but also express in natu-
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
ral language some suggestions or highlight some minor bugs that do not influence his overall app evaluation. For example, the comment Excellent app, but it crashes and closes while code scanning of with camera. Do something! was rated 4 stars by the user, but the sentence contains overall a negative sentiment from the perspective of the app developer since it explains a serious bug. All of this non-structured information is fully missing by only superficially evaluating an app through a 1 to 5 overall score – or any other product evaluated by the user with a sentence and a score–.
About the methods of processing user reviews and tweets, many methods and software implementing different approaches exist and there is not a clear best approach for Sentiment classification/quantification (Araújo et al., 2016) (Chen, 2010) (Gao and Sebastiani, 2015). In our opinion, performance reached by machine learning approaches is a key advantage to apply to sentiment analysis in order to reach a performance which is very close to the one obtained by a group of humans evaluating subjective sentences such as user reviews. As a reference sentiment score, it is well known in literature that a group of humans agree only about in the 80% of cases when evaluating sentiment sentences (Wilson et al., 2009) (Grimes, 2010).
In this paper, we present the App2Check and Tweet2Check tools in their English and Italian versions, built on top of two specialized predictive models developed applying supervised learning techniques, and present the results of our experimental evaluation. This shows that App2Check outperforms 19 state-of-the-art research tools on 11 thousand apps reviews in Italian, by overcoming the theoretical reference of 80% of accuracy; and it is better than those tools on 11 thousand app reviews in English. About apps reviews we considered for experiments, these are new test sets we are making available for the research community. About tweets, we show that Tweet2Check is better than competitors on 3899 tweets in Italian language and on 1 thousand randomly selected tweets in English from the latest competitions.
The structure of the paper is the following. In section 2 we report a brief description of research tools iFeel platform, SentiStrength for Italian and our tools, App2Check and Tweet2Check. In section 3 we present and discuss our experimental evaluation and in section 4 we show the conclu-
sions.",,,"In this paper we presented our models for sentiment analysis, App2Check and Tweet2Check, in their Italian and English versions, based on two predictive models for apps reviews and tweets applying supervised learning techniques. We showed that App2Check outperforms research competitors on apps reviews, and that Tweet2Check is better than competitors on tweets in both Italian and English language."
12,"Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture.  Our main result is that with with retraining, we can recover and even surpass the original performance with an 80%-pruned model.",Compression of Neural Machine Translation Models via Pruning,91,"This paper investigates three simple weight-pruning techniques for NMT, and
shows that pruning weights based on magnitude works best, and that retraining
after pruning can recover original performance, even with fairly severe
pruning.

The main strength of paper is that the technique is very straightforward and
the results are good. Itâs also clearly written and does a nice job covering
previous work.

A weakness is that the work isnât very novel, being just an application of a
known technique to a new kind of neural net and application (namely NMT), with
results that arenât very surprising. 

Itâs not clear to me what practical significance these results have, since to
take advantage of them you would need sparse matrix representations, which are
trickier to get working fast on a GPU - and after all, speed is the main
problem with NMT, not space. (There may be new work that changes this picture,
since the field is evolving fast, but if so you need to describe it, and
generally do a better job explaining why we should care about pruning.)

A suggestion for dealing with the above weakness would be to use the pruning
results to inform architecture changes. For instance, figure 3 suggests that
you might be able to reduce the number of hidden layers to two, and also
potentially reduce the dimension of source and target embeddings.

Another suggestion is that you try to make a link between pruning+retraining
and dropout (eg âA Theoretically Grounded Application of Dropout in Recurrent
Neural Networksâ, Gal, arXiv 2016).

Detailed comments:

Line 111: âsoftmax weightsâ - âoutput embeddingsâ may be a preferable
term

S3.2: Itâs misleading to call n the âdimensionâ of the network, and
specify all parameter sizes as integer multiples of this number as if this were
a logical constraint.

Line 319: You should cite Bahdanau et al here for the attention idea, rather
than Luong for their use of it.

S3.3: Class-uniform and class-distribution seem very similar (and naturally get
very similar results); consider dropping one or the other.

Figure 3 suggestion that you could hybridize pruning: use class-blind for most
classes, but class-uniform for the embeddings.

Figure 4 should show perplexity too.

What pruning is used in section 4.2 & figure 6?

Figure 7: does loss pertain to training or test corpora?

Figure 8: This seems to be missing softmax weights. I found this diagram
somewhat hard to interpret; it might be better to give relevant statistics,
such as the proportion of each class that is removed by class-blind pruning at
various levels.

Line 762: You might want to cite Le et al, âA Simple Way to Initialize
Recurrent Networks of Rectified Linear Unitsâ, arXiv 2015.",,3,4,Poster,4,4,3,4,4,5,3,2,2016,"We first give a brief overview of Neural Machine Translation before delving into a model architecture of interest, the deep multi-layer recurrent model with LSTM. We then explain the different types of NMT weights together with our approaches to pruning and retraining.Neural machine translation aims to directly model the conditional probability p(y|x) of translating a source sentence, x1, . . . , xn, to a target sentence, y1, . . . , ym. It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The encoder computes a representation s for each source sentence. Based on that source representation, the decoder generates a translation, one target word at a time, and hence, decomposes the log conditional probability as:
log p(y|x) = ∑m
t=1 log p (yt|y<t, s) (1)
Most NMT work uses RNNs, but approaches differ in terms of: (a) architecture, which can be unidirectional, bidirectional, or deep multilayer RNN; and (b) RNN type, which can be
Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or the gated recurrent unit (Cho et al., 2014).
In this work, we specifically consider the deep multi-layer recurrent architecture with LSTM as the hidden unit type. Figure 1 illustrates an instance of that architecture during training in which the source and target sentence pair are input for supervised learning. During testing, the target sentence is not known in advance; instead, the most probable target words predicted by the model are fed as inputs into the next timestep. The network stops when it emits the end-of-sentence symbol— a special ‘word’ in the vocabulary, represented by a dash in Figure 1.We show in Figure 2 the same system in more detail, highlighting the different types of parameters, or weights, in the model. We will go through the architecture from bottom to top. First, a vocabulary is chosen for each language, assuming that the top V frequent words are selected. Thus, every word in the source or target vocabulary can be represented by a one-hot vector of length V . The source input sentence and target input sentence, represented as a sequence of one-hot vec-
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
tors, are transformed into a sequence of word embeddings by the embedding weights. These embedding weights, which are learned during training, are different for the source words and the target words. The word embeddings are vectors of length n, the dimension of the network.
The word embeddings are then fed as input into the main network, which consists of two multilayer RNNs ‘stuck together’—an encoder for the source language and a decoder for the target language, each with their own weights. The feedforward (vertical) weights connect the hidden unit from the layer below to the upper RNN block, and the recurrent (horizontal) weights connect the hidden unit from the previous time-step RNN block to the current time-step RNN block.
The hidden state at the top layer of the decoder is fed through an attention layer, which guides the translation by ‘paying attention’ to relevant parts of the source sentence; for more information see Section 3 of (Luong et al., 2015a). Finally, for each target word, the top layer hidden unit is transformed by the softmax weights into a score vector of length V . The target word with the highest score is selected as the output translation.
Weight Subgroups in LSTM – For the aforementioned RNN block, we choose to use LSTM as a hidden unit type. To facilitate our discussion later on the different subgroups of weights within LSTM, we first revise details of an LSTM suggested by Zaremba et al. (2014) as follows: i f o
ĥ
 = 
sigm sigm sigm tanh
T4n,2n(hl−1thlt−1 )
(2)
clt = f ◦ clt−1 + i ◦ ĥ (3) hlt = o ◦ tanh(clt) (4)
Here, each LSTM block at time t and layer l computes as outputs a pair of hidden and memory vectors (hlt, c l t) given the previous pair (h l t−1, c l t−1) and an input vector hl−1t (either from the below LSTM block or the embedding weights if l = 1). All of these vectors have dimensions of n.
The core of an LSTM block is the weight matrix T4n,2n of size 4n×2n. This matrix can be decomposed into 8 subgroups that are responsible for the interractions between {input gate i, forget gate f , output gate o, input signal ĥ} × {feed-forward input hl−1t , recurrent input h l t−1}.We follow the general magnitude-based approach of (Han et al., 2015b), which consists of pruning weights with smallest absolute value. However, we question the authors’ pruning scheme with respect to the different weight classes, and experiment with three pruning schemes. Suppose we wish to prune x% of the total parameters in the model. How do we distribute the pruning over the different weight classes (illustrated in Figure 2) of our model? We propose to examine three different pruning schemes:
1. Class-blind: Take all parameters, sort them by magnitude and prune the x% with smallest magnitude, regardless of weight class. (So some classes are pruned proportionally more than others).
2. Class-uniform: Within each class, sort the weights by magnitude and prune the x% with smallest magnitude. (So all classes have exactly x% of their parameters pruned).
3. Class-distribution: For each class c, weights with magnitude less than λσc are pruned. Here, σc is the standard deviation of that class and λ is a universal parameter chosen such that in total, x% of all parameters are pruned. This is used by (Han et al., 2015b).
All these schemes have their seeming advantages. Class-blind pruning is the simplest and adheres to the principle that pruning weights (or equivalently, setting them to zero) is least damaging when those weights are small, regardless of their locations in an architecture. Class-uniform pruning and classdistribution pruning both seek to prune proportionally within each weight class, either absolutely, or relative to the standard deviation of that class. We find that class-blind pruning outperforms both other schemes (see Section 4.1).In order to prune NMT models aggressively without performance loss, we retrain our pruned networks. In our implementation, we keep “mask” matrices, which represent the sparse structure of a network, so as to ignore weights at pruned locations. We detail in Section 4.1 a successful “formula” to retrain pruned NMT models.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
so urc
e l ay
er 1
so urc
e l ay
er 2
so urc
e l ay
er 3
so urc
e l ay
er 4
tar ge
t la ye
r 1
tar ge
t la ye
r 2
tar ge
t la ye
r 3
tar ge
t la ye
r 4
att en
tio n
so ftm
ax
so urc
e e mb
ed din
g
tar ge
t e mb
ed din
g
0
5
10 15 pe rp le xi ty ch an
ge class-blind class-uniform class-distribution
Figure 3: ‘Breakdown’ of performance loss (i.e., perplexity increase) by weight class, when pruning 90% of weights using each of the three pruning schemes. Each of the first eight classes have 8 million weights, attention has 2 million, and the last three have 50 million weights each.We evaluate the effectiveness of our pruning approaches on a state-of-the-art NMT model.1 Specifically, an attention-based English-German NMT system from (Luong et al., 2015a) is considered. Training data was obtained from WMT’14 consisting of 4.5M sentence pairs (116M English words, 110M German words). For more details on training hyperparameters, we refer readers to Section 4.1 of (Luong et al., 2015a). All models are tested on newstest2014 (2737 sentences). The model achieves a perplexity of 6.1 and a BLEU score of 20.5 (after unknown word replacement).2
When retraining pruned NMT systems, we use the following settings: (a) we start with a smaller learning rate of 0.5 (the original model uses a learning rate of 1.0), (b) we train for fewer epochs, 4 instead of 12, using plain SGD, (c) a simple learning rate schedule is employed; after 2 epochs, we begin to halve the learning rate every half an epoch, and (d) all other hyperparameters are the same, such as mini-batch size 128, maximum gradient norm 5, and dropout with probability 0.2.Despite its simplicity, we observe in Figure 4 that class-blind pruning outperforms both other
1We thank the authors of (Luong et al., 2015a) for providing their trained models and assistance in using the codebase at https://github.com/lmthang/nmt.matlab.
2The performance of this model is reported under row global (dot) in Table 4 of (Luong et al., 2015a).
schemes in terms of translation quality at all pruning percentages. Furthermore, to explain the poor performance of class-uniform and classdistribution pruning, for each of the three pruning schemes, we pruned each class separately and recorded the effect on performance (as measured by perplexity). Figure 3 shows that with classuniform pruning, the overall performance loss is caused disproportionately by a few classes: target layer 4, attention and softmax weights. Looking at Figure 5, we see that the most damaging classes to prune also tend to be those with weights of greater magnitude—these classes have much larger weights than others at the same percentile, so pruning them under the class-uniform pruning scheme is more damaging. The situation is similar for class-distribution pruning.
By contrast, Figure 3 shows that under class-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
0 0.1 0.2 0.3 0.4 0.5
100
101
magnitude of largest deleted weight
pe rp
le xi
ty ch
an ge
Figure 5: Magnitude of largest deleted weight vs. perplexity change, for the 12 different weight classes when pruning 90% of parameters by classuniform pruning.
blind pruning, the damage caused by pruning softmax, attention and target layer 4 weights is greatly decreased, and the contribution of each class towards the performance loss is overall more uniform. In fact, the distribution begins to reflect the number of parameters in each class—for example, the source and target embedding classes have larger contributions because they have more weights. We use only class-blind pruning for the rest of the experiments.
Figure 3 also reveals some interesting information about the distribution of redundancy in NMT architectures—namely it seems that higher layers are more important than lower layers, and that attention and softmax weights are crucial. We will explore the distribution of redundancy further in Section 4.3.Pruning has an immediate negative impact on performance (as measured by BLEU score on the validation set) that is exponential in pruning percentage; this is demonstrated by the blue line in Figure 6. However we find that up to about 40% pruning, performance is mostly unaffected, indicating a large amount of redundancy and overparameterization in NMT.
We now consider the effect of retraining pruned models. The orange line in Figure 6 shows that after retraining the pruned models, baseline performance (20.48 BLEU) is both recovered and improved upon, up to 80% pruning (20.91 BLEU), with only a small performance loss at 90% pruning (20.13 BLEU). This may seem surprising, as we might not expect a sparse model to significantly
out-perform a model with five times as many parameters. There are several possible explanations, two of which are given below.
Firstly, we found that the less-pruned models perform better on the training set than the validation set, whereas the more-pruned models have closer performance on the two sets. This indicates that pruning has a regularizing effect on the retraining phase, though clearly more is not always better, as the 50% pruned and retrained model performs better than the 90% pruned and retrained model. Nonetheless, this regularization effect may explain why the pruned and retrained models outperform the baseline.
0 1 2 3 4 5
·105
2
4
6
8
training iterations
lo ss
Figure 7: The loss function during training, pruning and retraining. The vertical dotted line marks the point when 80% of the parameters are pruned. The horizontal dotted line marks the best performance of the unpruned baseline.
Alternatively, pruning may serve as a means to escape a local optimum. Figure 7 shows the loss function over time during the training, pruning and retraining process. During the original training
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
target embedding weights
source embedding weights
least common wordmost common word
source layer 1 weights
recurrentfeed-forward
input gate
forget gate
output gate
input
source layer 2 weights source layer 3 weights source layer 4 weights
target layer 1 weights target layer 2 weights target layer 3 weights target layer 4 weights
Figure 8: Graphical representation of the location of small weights in the model. Black pixels represent weights with absolute size in the bottom 80%; white pixels represent those with absolute size in the top 20%. Equivalently, these pictures illustrate which parameters remain after pruning 80% using our class-blind pruning scheme.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
process, the loss curve flattens out and seems to converge (note that we use early stopping to obtain our baseline model, so the original model was trained for longer than shown in Figure 7). Pruning causes an immediate increase in the loss function, but enables further gradient descent, allowing the retraining process to find a new, better local optimum. It seems that the disruption caused by pruning is beneficial in the long-run.We visualize in Figure 8 the redundancy structore of our NMT baseline model. Black pixels represent weights near to zero; white pixels represent larger ones. First we consider the embedding weight matrices, whose columns correspond to words in the vocabulary. Unsurprisingly, in Figure 8, we see that the parameters corresponding to the less common words are more dispensable. In fact, at the 80% pruning rate, for 100 uncommon source words and 1194 uncommon target words, we delete all parameters corresponding to that word. This is not quite the same as removing the word from the vocabulary—true out-ofvocabulary words are mapped to the embedding for the ‘unknown word’ symbol, whereas these ‘pruned-out’ words are mapped to a zero embedding. However in the original unpruned model these uncommon words already had near-zero embeddings, indicating that the model was unable to learn sufficiently distinctive representations.
Returning to Figure 8, now look at the eight weight matrices for the source and target connections at each of the four layers. Each matrix corresponds to the 4n × 2n matrix T4n,2n in Equation (1). In all eight matrices, we observe—as does (Lu et al., 2016)—that the weights connecting to the input ĥ are most crucial, followed by the input gate i, then the output gate o, then the forget gate f . This is particularly true of the lower layers, which focus primarily on the input ĥ. However for higher layers, especially on the target side, weights connecting to the gates are as important as those connecting to the input ĥ. The gates represent the LSTM’s ability to add to, delete from or retrieve information from the memory cell. Figure 8 therefore shows that these sophisticated memory cell abilities are most important at the end of the NMT pipeline (the top layer of the decoder). This is reasonable, as we expect higher-level features to be learned later in a deep learning pipeline.
We also observe that for lower layers, the feedforward input is much more important than the recurrent input, whereas for higher layers the recurrent input becomes more important. This makes sense: lower layers concentrate on the low-level information from the current word embedding (the feed-forward input), whereas higher layers make use of the higher-level representation of the sentence so far (the recurrent input).
Lastly, on close inspection, we notice several white diagonals emerging within the subsquares of the matrices in Figure 8, indicating that even without initializing the weights to identity matrices, an identity-like weight matrix is learned. At higher pruning percentages, these diagonals become more pronounced.The pruning method described in (Han et al., 2015b) includes several iterations of pruning and retraining. Implementing this for NMT would likely result in further compression and performance improvements. If possible it would be highly valuable to exploit the sparsity of the pruned models to speed up training and runtime, perhaps through sparse matrix representations and multiplications. Though we have found magnitude-based pruning to perform very well, it would be instructive to revisit the original claim that other pruning methods (for example Optimal Brain Damage and Optimal Brain Surgery) are more principled, and perform a comparative study.",,"Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014). NMT is a single deep neural network that is trained end-to-end, holding several advantages such as the ability to capture long-range dependencies in sentences, and generalization to unseen texts. Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs including EnglishFrench (Luong et al., 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015; Sennrich et al., 2015), English-
Turkish (Sennrich et al., 2015), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016). Figure 1 gives an example of an NMT system.
While NMT has a significantly smaller memory footprint than traditional phrase-based approaches (which need to store gigantic phrase-tables and language models), the model size of NMT is still prohibitively large for mobile devices. For example, a recent state-of-the-art NMT system requires over 200 million parameters, resulting in a storage size of hundreds of megabytes (Luong et al., 2015a). Though the trend for bigger and deeper neural networks has brought great progress, it has also introduced over-parameterization, resulting in long running times, overfitting, and the large storage size discussed above. Thus a solution to the over-parameterization problem could potentially aid all three issues.
Our contribution. In this paper we investigate the efficacy of weight pruning for NMT as a means of compression. We show that despite its simplicity, magnitude-based pruning with retraining is highly effective, and we compare three
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
magnitude-based pruning schemes—class-blind, class-uniform and class-distribution. Though recent work has chosen to use the latter two, we find the first and simplest scheme—class-blind— the most successful. We are able to prune 40% of the weights of a state-of-the-art NMT system with negligible performance loss, and by adding a retraining phase after pruning, we can prune 80% with no performance loss. Our pruning experiments also reveal some patterns in the distribution of redundancy in NMT. In particular we find that higher layers, attention and softmax weights are the most important, while lower layers and the embedding weights hold a lot of redundancy. For the Long Short-Term Memory (LSTM) architecture, we find that at lower layers the parameters for the input are most crucial, but at higher layers the parameters for the gates also become important.","Pruning the parameters from a neural network, referred to as weight pruning or network pruning, is a well-established idea though it can be implemented in many ways. Among the most popular are the Optimal Brain Damage (OBD) (Le Cun et al., 1989) and Optimal Brain Surgeon (OBS) (Hassibi and Stork, 1993) techniques, which involve computing the Hessian matrix of the loss function with respect to the parameters, in order to assess the saliency of each parameter. Parameters with low saliency are then pruned from the network and the remaining sparse network is retrained. Both OBD and OBS were shown to perform better than the so-called ‘naive magnitude-based approach’, which prunes parameters according to their magnitude (deleting parameters close to zero). However, the high computational complexity of OBD and OBS compare unfavorably to the computational simplicity of the magnitude-based approach, especially for large networks (Augasta and Kathirvalavakumar, 2013).
In recent years, the deep learning renaissance has prompted a re-investigation of network pruning for modern models and tasks. Magnitudebased pruning (with iterative retraining) has yielded strong results for Convolutional Neural Nets (CNNs) performing visual tasks. (Collins and Kohli, 2014) prune 75% of AlexNet parameters with small accuracy loss on the ImageNet task, while (Han et al., 2015b) prune 89% of AlexNet parameters with no accuracy loss on the
ImageNet task.
Other approaches focus on pruning neurons rather than parameters, via sparsity-inducing regularizers (Murray and Chiang, 2015) or ‘wiring together’ pairs of neurons with similar input weights (Srinivas and Babu, 2015). These approaches are much more constrained than weight-pruning schemes; they necessitate finding entire zero rows of weight matrices, or (near-) identical pairs of rows, before a single neuron can be pruned. By contrast weight-pruning approaches allow weights to be pruned freely and independently of each other. The neuron-pruning approach of (Srinivas and Babu, 2015) was shown to perform poorly (it suffered performance loss after removing only 35% of AlexNet parameters) compared to the weight-pruning approach of (Han et al., 2015b). Though (Murray and Chiang, 2015) demonstrates neuron-pruning for language modeling as part of a (non-neural) Machine Translation pipeline, their approach is more geared towards architecture selection than compression.
There are many other compression techniques for neural networks, including approaches based on on low-rank approximations for weight matrices (Jaderberg et al., 2014; Denton et al., 2014), or weight sharing via hash functions (Chen et al., 2015). Several methods involve reducing the precision of the weights or activations (Courbariaux et al., 2015), sometimes in conjunction with specialized hardware (Gupta et al., 2015), or even using binary weights (Lin et al., 2016). The ‘knowledge distillation’ technique of (Hinton et al., 2015) involves training a small ‘student’ network on the soft outputs of a large ‘teacher’ network. Some approaches use a sophisticated pipeline of several techniques to achieve impressive feats of compression (Han et al., 2015a; Iandola et al., 2016).
Most of the above work has focused on compressing CNNs for vision tasks. We extend the magnitude-based pruning approach of (Han et al., 2015b) to recurrent neural networks (RNN), in particular LSTM architectures for NMT, and to our knowledge we are the first to do so. There has been some recent work on compression for RNNs (Lu et al., 2016; Prabhavalkar et al., 2016), but it focuses on other, non-pruning compression techniques. Nonetheless, our general observations on the distribution of redundancy in a LSTM are corroborated by (Lu et al., 2016).
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
studentaamI Je
Je suis
suis étudiant
étudiant _
one-hot vectors length V
word embeddings length n
hidden layer 1 length n
hidden layer 2 length n
scores length V
one-hot vectors length V
_
source language input target language input
initial (zero) states
target language output
softmax weights size: V × n
Key to weight classes
attention hidden layer length n
context vector (one for each target word)
length n
target layer 2
weights size: 4n x 2n
source embedding weights
size: n x V
attention weights size: n x 2n
target layer 1
weights size: 4n x 2n
source layer 2
weights size: 4n x 2n
source layer 1
weights size: 4n x 2n
target embedding weights
size: n x V
Figure 2: NMT architecture. This example has two layers, but our system has four. The different weight classes are indicated by arrows of different color (the black arrows in the top right represent simply choosing the highest-scoring word, and thus require no parameters). Best viewed in color.",,"We have shown that weight pruning with retraining is a highly effective method of compression and regularization on a state-of-the-art NMT system, compressing the model to 20% of its size with no loss of performance. Though we are the first to apply compression techniques to NMT, we obtain a similar degree of compression to other current work on compressing state-of-the-art deep neural networks, with an approach that is simpler than most. We have found that the absolute size of parameters is of primary importance when choosing which to prune, leading to an approach that is extremely simple to implement, and can be applied to any neural network. Lastly, we have gained insight into the distribution of redundancy in the NMT architecture.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
13,"Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture.  Our main result is that with with retraining, we can recover and even surpass the original performance with an 80%-pruned model.",Compression of Neural Machine Translation Models via Pruning,91,"This paper applies the idea of translation model pruning to neural MT. The
authors explore three simple threshold and histogram pruning schemes, two of
which are applied separately to each weight class, while the third is applied
to the entire model. The authors also show that retraining the models produces
performance equal to the full model, even when 90% of the weights are pruned.
An extensive analysis explains the superiority of the class-blind pruning
scheme, as well as the performance boost through retraining. 

While the main idea of the paper is simple, it seems quite useful for
memory-restricted applications of NMT. I particularly liked the analysis
section which gives further insight into the model components that are usually
treated like black boxes. While these insights are interesting by themselves,
the paper's main motivation is model compression. This argument would be
stronger if the paper included some numbers on actual memory consumption of the
compressed model in comparison to the uncompressed model.     

Some minor remarks:
- There is a substantial amount of work on pruning translation models in
phrase-based SMT, which could be referenced in related work, e.g. 
Johnson, J., Martin, J., Foster, G. and Kuhn, R.: Improving Translation Quality
by Discarding Most of the Phrasetable. EMNLP 07 or
Zens, R., Stanton, D. and Peng X.: A Systematic Comparison of Phrase Table
Pruning Techniques. EMNLP 12

- It took me a while to understand Figure 5. I would find it more informative
to add an additional barplot under figure 4 showing highest discarded weight
magnitude by class. This would also allow a comparison across all pruning
methods.",,4,4,Poster,4,4,4,4,4,5,3,3,2016,"We first give a brief overview of Neural Machine Translation before delving into a model architecture of interest, the deep multi-layer recurrent model with LSTM. We then explain the different types of NMT weights together with our approaches to pruning and retraining.Neural machine translation aims to directly model the conditional probability p(y|x) of translating a source sentence, x1, . . . , xn, to a target sentence, y1, . . . , ym. It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The encoder computes a representation s for each source sentence. Based on that source representation, the decoder generates a translation, one target word at a time, and hence, decomposes the log conditional probability as:
log p(y|x) = ∑m
t=1 log p (yt|y<t, s) (1)
Most NMT work uses RNNs, but approaches differ in terms of: (a) architecture, which can be unidirectional, bidirectional, or deep multilayer RNN; and (b) RNN type, which can be
Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or the gated recurrent unit (Cho et al., 2014).
In this work, we specifically consider the deep multi-layer recurrent architecture with LSTM as the hidden unit type. Figure 1 illustrates an instance of that architecture during training in which the source and target sentence pair are input for supervised learning. During testing, the target sentence is not known in advance; instead, the most probable target words predicted by the model are fed as inputs into the next timestep. The network stops when it emits the end-of-sentence symbol— a special ‘word’ in the vocabulary, represented by a dash in Figure 1.We show in Figure 2 the same system in more detail, highlighting the different types of parameters, or weights, in the model. We will go through the architecture from bottom to top. First, a vocabulary is chosen for each language, assuming that the top V frequent words are selected. Thus, every word in the source or target vocabulary can be represented by a one-hot vector of length V . The source input sentence and target input sentence, represented as a sequence of one-hot vec-
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
tors, are transformed into a sequence of word embeddings by the embedding weights. These embedding weights, which are learned during training, are different for the source words and the target words. The word embeddings are vectors of length n, the dimension of the network.
The word embeddings are then fed as input into the main network, which consists of two multilayer RNNs ‘stuck together’—an encoder for the source language and a decoder for the target language, each with their own weights. The feedforward (vertical) weights connect the hidden unit from the layer below to the upper RNN block, and the recurrent (horizontal) weights connect the hidden unit from the previous time-step RNN block to the current time-step RNN block.
The hidden state at the top layer of the decoder is fed through an attention layer, which guides the translation by ‘paying attention’ to relevant parts of the source sentence; for more information see Section 3 of (Luong et al., 2015a). Finally, for each target word, the top layer hidden unit is transformed by the softmax weights into a score vector of length V . The target word with the highest score is selected as the output translation.
Weight Subgroups in LSTM – For the aforementioned RNN block, we choose to use LSTM as a hidden unit type. To facilitate our discussion later on the different subgroups of weights within LSTM, we first revise details of an LSTM suggested by Zaremba et al. (2014) as follows: i f o
ĥ
 = 
sigm sigm sigm tanh
T4n,2n(hl−1thlt−1 )
(2)
clt = f ◦ clt−1 + i ◦ ĥ (3) hlt = o ◦ tanh(clt) (4)
Here, each LSTM block at time t and layer l computes as outputs a pair of hidden and memory vectors (hlt, c l t) given the previous pair (h l t−1, c l t−1) and an input vector hl−1t (either from the below LSTM block or the embedding weights if l = 1). All of these vectors have dimensions of n.
The core of an LSTM block is the weight matrix T4n,2n of size 4n×2n. This matrix can be decomposed into 8 subgroups that are responsible for the interractions between {input gate i, forget gate f , output gate o, input signal ĥ} × {feed-forward input hl−1t , recurrent input h l t−1}.We follow the general magnitude-based approach of (Han et al., 2015b), which consists of pruning weights with smallest absolute value. However, we question the authors’ pruning scheme with respect to the different weight classes, and experiment with three pruning schemes. Suppose we wish to prune x% of the total parameters in the model. How do we distribute the pruning over the different weight classes (illustrated in Figure 2) of our model? We propose to examine three different pruning schemes:
1. Class-blind: Take all parameters, sort them by magnitude and prune the x% with smallest magnitude, regardless of weight class. (So some classes are pruned proportionally more than others).
2. Class-uniform: Within each class, sort the weights by magnitude and prune the x% with smallest magnitude. (So all classes have exactly x% of their parameters pruned).
3. Class-distribution: For each class c, weights with magnitude less than λσc are pruned. Here, σc is the standard deviation of that class and λ is a universal parameter chosen such that in total, x% of all parameters are pruned. This is used by (Han et al., 2015b).
All these schemes have their seeming advantages. Class-blind pruning is the simplest and adheres to the principle that pruning weights (or equivalently, setting them to zero) is least damaging when those weights are small, regardless of their locations in an architecture. Class-uniform pruning and classdistribution pruning both seek to prune proportionally within each weight class, either absolutely, or relative to the standard deviation of that class. We find that class-blind pruning outperforms both other schemes (see Section 4.1).In order to prune NMT models aggressively without performance loss, we retrain our pruned networks. In our implementation, we keep “mask” matrices, which represent the sparse structure of a network, so as to ignore weights at pruned locations. We detail in Section 4.1 a successful “formula” to retrain pruned NMT models.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
so urc
e l ay
er 1
so urc
e l ay
er 2
so urc
e l ay
er 3
so urc
e l ay
er 4
tar ge
t la ye
r 1
tar ge
t la ye
r 2
tar ge
t la ye
r 3
tar ge
t la ye
r 4
att en
tio n
so ftm
ax
so urc
e e mb
ed din
g
tar ge
t e mb
ed din
g
0
5
10 15 pe rp le xi ty ch an
ge class-blind class-uniform class-distribution
Figure 3: ‘Breakdown’ of performance loss (i.e., perplexity increase) by weight class, when pruning 90% of weights using each of the three pruning schemes. Each of the first eight classes have 8 million weights, attention has 2 million, and the last three have 50 million weights each.We evaluate the effectiveness of our pruning approaches on a state-of-the-art NMT model.1 Specifically, an attention-based English-German NMT system from (Luong et al., 2015a) is considered. Training data was obtained from WMT’14 consisting of 4.5M sentence pairs (116M English words, 110M German words). For more details on training hyperparameters, we refer readers to Section 4.1 of (Luong et al., 2015a). All models are tested on newstest2014 (2737 sentences). The model achieves a perplexity of 6.1 and a BLEU score of 20.5 (after unknown word replacement).2
When retraining pruned NMT systems, we use the following settings: (a) we start with a smaller learning rate of 0.5 (the original model uses a learning rate of 1.0), (b) we train for fewer epochs, 4 instead of 12, using plain SGD, (c) a simple learning rate schedule is employed; after 2 epochs, we begin to halve the learning rate every half an epoch, and (d) all other hyperparameters are the same, such as mini-batch size 128, maximum gradient norm 5, and dropout with probability 0.2.Despite its simplicity, we observe in Figure 4 that class-blind pruning outperforms both other
1We thank the authors of (Luong et al., 2015a) for providing their trained models and assistance in using the codebase at https://github.com/lmthang/nmt.matlab.
2The performance of this model is reported under row global (dot) in Table 4 of (Luong et al., 2015a).
schemes in terms of translation quality at all pruning percentages. Furthermore, to explain the poor performance of class-uniform and classdistribution pruning, for each of the three pruning schemes, we pruned each class separately and recorded the effect on performance (as measured by perplexity). Figure 3 shows that with classuniform pruning, the overall performance loss is caused disproportionately by a few classes: target layer 4, attention and softmax weights. Looking at Figure 5, we see that the most damaging classes to prune also tend to be those with weights of greater magnitude—these classes have much larger weights than others at the same percentile, so pruning them under the class-uniform pruning scheme is more damaging. The situation is similar for class-distribution pruning.
By contrast, Figure 3 shows that under class-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
0 0.1 0.2 0.3 0.4 0.5
100
101
magnitude of largest deleted weight
pe rp
le xi
ty ch
an ge
Figure 5: Magnitude of largest deleted weight vs. perplexity change, for the 12 different weight classes when pruning 90% of parameters by classuniform pruning.
blind pruning, the damage caused by pruning softmax, attention and target layer 4 weights is greatly decreased, and the contribution of each class towards the performance loss is overall more uniform. In fact, the distribution begins to reflect the number of parameters in each class—for example, the source and target embedding classes have larger contributions because they have more weights. We use only class-blind pruning for the rest of the experiments.
Figure 3 also reveals some interesting information about the distribution of redundancy in NMT architectures—namely it seems that higher layers are more important than lower layers, and that attention and softmax weights are crucial. We will explore the distribution of redundancy further in Section 4.3.Pruning has an immediate negative impact on performance (as measured by BLEU score on the validation set) that is exponential in pruning percentage; this is demonstrated by the blue line in Figure 6. However we find that up to about 40% pruning, performance is mostly unaffected, indicating a large amount of redundancy and overparameterization in NMT.
We now consider the effect of retraining pruned models. The orange line in Figure 6 shows that after retraining the pruned models, baseline performance (20.48 BLEU) is both recovered and improved upon, up to 80% pruning (20.91 BLEU), with only a small performance loss at 90% pruning (20.13 BLEU). This may seem surprising, as we might not expect a sparse model to significantly
out-perform a model with five times as many parameters. There are several possible explanations, two of which are given below.
Firstly, we found that the less-pruned models perform better on the training set than the validation set, whereas the more-pruned models have closer performance on the two sets. This indicates that pruning has a regularizing effect on the retraining phase, though clearly more is not always better, as the 50% pruned and retrained model performs better than the 90% pruned and retrained model. Nonetheless, this regularization effect may explain why the pruned and retrained models outperform the baseline.
0 1 2 3 4 5
·105
2
4
6
8
training iterations
lo ss
Figure 7: The loss function during training, pruning and retraining. The vertical dotted line marks the point when 80% of the parameters are pruned. The horizontal dotted line marks the best performance of the unpruned baseline.
Alternatively, pruning may serve as a means to escape a local optimum. Figure 7 shows the loss function over time during the training, pruning and retraining process. During the original training
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
target embedding weights
source embedding weights
least common wordmost common word
source layer 1 weights
recurrentfeed-forward
input gate
forget gate
output gate
input
source layer 2 weights source layer 3 weights source layer 4 weights
target layer 1 weights target layer 2 weights target layer 3 weights target layer 4 weights
Figure 8: Graphical representation of the location of small weights in the model. Black pixels represent weights with absolute size in the bottom 80%; white pixels represent those with absolute size in the top 20%. Equivalently, these pictures illustrate which parameters remain after pruning 80% using our class-blind pruning scheme.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
process, the loss curve flattens out and seems to converge (note that we use early stopping to obtain our baseline model, so the original model was trained for longer than shown in Figure 7). Pruning causes an immediate increase in the loss function, but enables further gradient descent, allowing the retraining process to find a new, better local optimum. It seems that the disruption caused by pruning is beneficial in the long-run.We visualize in Figure 8 the redundancy structore of our NMT baseline model. Black pixels represent weights near to zero; white pixels represent larger ones. First we consider the embedding weight matrices, whose columns correspond to words in the vocabulary. Unsurprisingly, in Figure 8, we see that the parameters corresponding to the less common words are more dispensable. In fact, at the 80% pruning rate, for 100 uncommon source words and 1194 uncommon target words, we delete all parameters corresponding to that word. This is not quite the same as removing the word from the vocabulary—true out-ofvocabulary words are mapped to the embedding for the ‘unknown word’ symbol, whereas these ‘pruned-out’ words are mapped to a zero embedding. However in the original unpruned model these uncommon words already had near-zero embeddings, indicating that the model was unable to learn sufficiently distinctive representations.
Returning to Figure 8, now look at the eight weight matrices for the source and target connections at each of the four layers. Each matrix corresponds to the 4n × 2n matrix T4n,2n in Equation (1). In all eight matrices, we observe—as does (Lu et al., 2016)—that the weights connecting to the input ĥ are most crucial, followed by the input gate i, then the output gate o, then the forget gate f . This is particularly true of the lower layers, which focus primarily on the input ĥ. However for higher layers, especially on the target side, weights connecting to the gates are as important as those connecting to the input ĥ. The gates represent the LSTM’s ability to add to, delete from or retrieve information from the memory cell. Figure 8 therefore shows that these sophisticated memory cell abilities are most important at the end of the NMT pipeline (the top layer of the decoder). This is reasonable, as we expect higher-level features to be learned later in a deep learning pipeline.
We also observe that for lower layers, the feedforward input is much more important than the recurrent input, whereas for higher layers the recurrent input becomes more important. This makes sense: lower layers concentrate on the low-level information from the current word embedding (the feed-forward input), whereas higher layers make use of the higher-level representation of the sentence so far (the recurrent input).
Lastly, on close inspection, we notice several white diagonals emerging within the subsquares of the matrices in Figure 8, indicating that even without initializing the weights to identity matrices, an identity-like weight matrix is learned. At higher pruning percentages, these diagonals become more pronounced.The pruning method described in (Han et al., 2015b) includes several iterations of pruning and retraining. Implementing this for NMT would likely result in further compression and performance improvements. If possible it would be highly valuable to exploit the sparsity of the pruned models to speed up training and runtime, perhaps through sparse matrix representations and multiplications. Though we have found magnitude-based pruning to perform very well, it would be instructive to revisit the original claim that other pruning methods (for example Optimal Brain Damage and Optimal Brain Surgery) are more principled, and perform a comparative study.",,"Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014). NMT is a single deep neural network that is trained end-to-end, holding several advantages such as the ability to capture long-range dependencies in sentences, and generalization to unseen texts. Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs including EnglishFrench (Luong et al., 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015; Sennrich et al., 2015), English-
Turkish (Sennrich et al., 2015), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016). Figure 1 gives an example of an NMT system.
While NMT has a significantly smaller memory footprint than traditional phrase-based approaches (which need to store gigantic phrase-tables and language models), the model size of NMT is still prohibitively large for mobile devices. For example, a recent state-of-the-art NMT system requires over 200 million parameters, resulting in a storage size of hundreds of megabytes (Luong et al., 2015a). Though the trend for bigger and deeper neural networks has brought great progress, it has also introduced over-parameterization, resulting in long running times, overfitting, and the large storage size discussed above. Thus a solution to the over-parameterization problem could potentially aid all three issues.
Our contribution. In this paper we investigate the efficacy of weight pruning for NMT as a means of compression. We show that despite its simplicity, magnitude-based pruning with retraining is highly effective, and we compare three
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
magnitude-based pruning schemes—class-blind, class-uniform and class-distribution. Though recent work has chosen to use the latter two, we find the first and simplest scheme—class-blind— the most successful. We are able to prune 40% of the weights of a state-of-the-art NMT system with negligible performance loss, and by adding a retraining phase after pruning, we can prune 80% with no performance loss. Our pruning experiments also reveal some patterns in the distribution of redundancy in NMT. In particular we find that higher layers, attention and softmax weights are the most important, while lower layers and the embedding weights hold a lot of redundancy. For the Long Short-Term Memory (LSTM) architecture, we find that at lower layers the parameters for the input are most crucial, but at higher layers the parameters for the gates also become important.","Pruning the parameters from a neural network, referred to as weight pruning or network pruning, is a well-established idea though it can be implemented in many ways. Among the most popular are the Optimal Brain Damage (OBD) (Le Cun et al., 1989) and Optimal Brain Surgeon (OBS) (Hassibi and Stork, 1993) techniques, which involve computing the Hessian matrix of the loss function with respect to the parameters, in order to assess the saliency of each parameter. Parameters with low saliency are then pruned from the network and the remaining sparse network is retrained. Both OBD and OBS were shown to perform better than the so-called ‘naive magnitude-based approach’, which prunes parameters according to their magnitude (deleting parameters close to zero). However, the high computational complexity of OBD and OBS compare unfavorably to the computational simplicity of the magnitude-based approach, especially for large networks (Augasta and Kathirvalavakumar, 2013).
In recent years, the deep learning renaissance has prompted a re-investigation of network pruning for modern models and tasks. Magnitudebased pruning (with iterative retraining) has yielded strong results for Convolutional Neural Nets (CNNs) performing visual tasks. (Collins and Kohli, 2014) prune 75% of AlexNet parameters with small accuracy loss on the ImageNet task, while (Han et al., 2015b) prune 89% of AlexNet parameters with no accuracy loss on the
ImageNet task.
Other approaches focus on pruning neurons rather than parameters, via sparsity-inducing regularizers (Murray and Chiang, 2015) or ‘wiring together’ pairs of neurons with similar input weights (Srinivas and Babu, 2015). These approaches are much more constrained than weight-pruning schemes; they necessitate finding entire zero rows of weight matrices, or (near-) identical pairs of rows, before a single neuron can be pruned. By contrast weight-pruning approaches allow weights to be pruned freely and independently of each other. The neuron-pruning approach of (Srinivas and Babu, 2015) was shown to perform poorly (it suffered performance loss after removing only 35% of AlexNet parameters) compared to the weight-pruning approach of (Han et al., 2015b). Though (Murray and Chiang, 2015) demonstrates neuron-pruning for language modeling as part of a (non-neural) Machine Translation pipeline, their approach is more geared towards architecture selection than compression.
There are many other compression techniques for neural networks, including approaches based on on low-rank approximations for weight matrices (Jaderberg et al., 2014; Denton et al., 2014), or weight sharing via hash functions (Chen et al., 2015). Several methods involve reducing the precision of the weights or activations (Courbariaux et al., 2015), sometimes in conjunction with specialized hardware (Gupta et al., 2015), or even using binary weights (Lin et al., 2016). The ‘knowledge distillation’ technique of (Hinton et al., 2015) involves training a small ‘student’ network on the soft outputs of a large ‘teacher’ network. Some approaches use a sophisticated pipeline of several techniques to achieve impressive feats of compression (Han et al., 2015a; Iandola et al., 2016).
Most of the above work has focused on compressing CNNs for vision tasks. We extend the magnitude-based pruning approach of (Han et al., 2015b) to recurrent neural networks (RNN), in particular LSTM architectures for NMT, and to our knowledge we are the first to do so. There has been some recent work on compression for RNNs (Lu et al., 2016; Prabhavalkar et al., 2016), but it focuses on other, non-pruning compression techniques. Nonetheless, our general observations on the distribution of redundancy in a LSTM are corroborated by (Lu et al., 2016).
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
studentaamI Je
Je suis
suis étudiant
étudiant _
one-hot vectors length V
word embeddings length n
hidden layer 1 length n
hidden layer 2 length n
scores length V
one-hot vectors length V
_
source language input target language input
initial (zero) states
target language output
softmax weights size: V × n
Key to weight classes
attention hidden layer length n
context vector (one for each target word)
length n
target layer 2
weights size: 4n x 2n
source embedding weights
size: n x V
attention weights size: n x 2n
target layer 1
weights size: 4n x 2n
source layer 2
weights size: 4n x 2n
source layer 1
weights size: 4n x 2n
target embedding weights
size: n x V
Figure 2: NMT architecture. This example has two layers, but our system has four. The different weight classes are indicated by arrows of different color (the black arrows in the top right represent simply choosing the highest-scoring word, and thus require no parameters). Best viewed in color.",,"We have shown that weight pruning with retraining is a highly effective method of compression and regularization on a state-of-the-art NMT system, compressing the model to 20% of its size with no loss of performance. Though we are the first to apply compression techniques to NMT, we obtain a similar degree of compression to other current work on compressing state-of-the-art deep neural networks, with an approach that is simpler than most. We have found that the absolute size of parameters is of primary importance when choosing which to prune, leading to an approach that is extremely simple to implement, and can be applied to any neural network. Lastly, we have gained insight into the distribution of redundancy in the NMT architecture.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
14,"In this paper, we compare delexicalized transfer and minimally supervised parsing techniques on 32 different languages from Universal Dependencies treebank collection. The minimal supervision is in adding handcrafted universal grammatical rules for POS tags. The rules are incorporated into the unsupervised dependency parser in forms of external prior probabilities. We also experiment with learning this probabilities from other treebanks. The average attachment score of our parser is slightly lower then the delexicalized transfer parser, however, it performs better for languages from less resourced language families (non-Indo-European) and is therefore suitable for those, for which the treebanks often do not exist.",Delexicalized and Minimally Supervised Parsing on Universal Dependencies,98,"This paper presents results on the UD treebanks to test delexicalized transfer
parsers and an unsupervised parser which is enriched with external
probabilities.

The paper is interesting, but I think it could be improved further.

(5.2) ""McDonald et al. (2011) presented 61.7% of averaged accuracy over 8
languages. On the same languages, our transfer parser on UD reached 70.1%.""
Mcdonald et al could not use the UD treebanks since they were not available,
you should definitely state that this is the case here.

In footnote 9 you say: ""We used the Malt parser with its default feature set.
Tuning in this specific delexicalized task would probably bring a
bit better results."" You are using MaltParser with default settings, why don't
you use MaltOptimizer? Optimizing one model would be very easy. 
In the same way MSTParser could be optimized further.
In the same line, why don't you use more recent parsers that produce better
results? These parsers have been already applied to universal dependencies with
the leave one out setup (see references below). For instance, the authors say
that  the unsupervised parser ""performs better for languages from less
resourced language families (non-Indo-European)"", it would be interesting to
see whether this holds with more recent (and cross lingual) parsers.

Probabilities: Why do you use this probabilities? it seems like a random
decision (Tables 3-4) (esp 3), at least we need more details or a set of
experiments to see whether they make sense or not.

There are some papers that the authors should take into account.

1. Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted
PoS Labels
J Tiedemann
2. One model, two languages: training bilingual parsers with harmonized
treebanks
D Vilares, MA Alonso, C GÃ³mez-RodrÃ­guez  (it presents results with
MaltParser)

And for results with more recent parsers (and also delexicalized parsers):
1. Crosslingual dependency parsing based on distributed representations. 
Jiang Guo, Wanxiang Che, David
Yarowsky, Haifeng Wang, and Ting Liu. 2015.  In Proc. of ACL

2. Many languages, one parser
W Ammar, G Mulcaire, M Ballesteros, C Dyer, NA Smith

-Minor points:
 I don't think we need Table 1 and Table 2, this could be solved with a
footnote to the UD website. Perhaps Table 2 should be included due to the
probabilities, but Table 1 definitely not.",,2,4,Poster,4,3,2,4,4,5,2,3,2016,"In all our experiments, we use the Universal Dependencies treebank collection2 in its current version 1.2. For languages for which there is more than one treebank, we experiment only with the first one.3 We also exclude ’Japan-KTC’ treebank, since the full data are not available. Finally, we experiment with 32 dependency treebanks, each representing a different language. The treebanks, their language families, and their sizes are listed in Table 1.
Before training the parsers, all the treebanks are delexicalized. We substitute all the forms and lemmas by underscores, which are used for undefined values. The same is done with the morphological features and dependency relations. The only information remained is the universal POS tags and the dependency structure (the parent number for each token). The Universal Dependencies use POS tagset consisting of 17 POS tags listed in Table 2.In the following experiments, we compare delexicalized transfer parsing methods and minimallysupervised methods on the UD treebanks. All the experiments are conducted as if we parsed a language whose syntax is unknown for us. This means that we do not prefer training on syntactically similar languages, we do not prefer right branching or left branching, and do not add language specific word-order rules like preferring SVO or SOV, adjectives before nouns, prepositions vs. postpositions etc.
2universaldependencices.org 3We exclude ’Ancient Greek-PROIEL’, ’Finnish-FTB’,
’Japan-KTC’, ’Latin-ITT’, and ’Latin-PROIEL’ treebanks.
3
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
language family tokens ar Arabic Semitic 282384 bg Bulgarian Slavic 156319 cu Old Slav. Slavic 57507 cs Czech Slavic 1503738 da Danish Germanic 100733 de German Germanic 293088 el Greek Hellenic 59156 en English Germanic 254830 es Spanish Romance 423346 et Estonian Uralic 6461 eu Basque isolate 121443 fa Persian Iranian 151624 fi Finnish Uralic 181022 fr French Romance 389764 ga Irish Celtic 23686 got Gothic Germanic 56128 grc Old Greek Hellenic 244993 he Hebrew Semitic 115535 hi Hindi Indo-Iranian 351704 hr Croatian Slavic 87765 hu Hungarian Uralic 26538 id Indonesian Malayic 121923 it Italian Romance 252967 la Latin Romance 47303 nl Dutch Germanic 200654 no Norwegian Germanic 311277 pl Polish Slavic 83571 pt Portuguese Romance 212545 ro Romanian Romance 12094 sl Slovenian Slavic 140418 sv Swedish Germanic 96819 ta Tamil Dravidian 9581
Table 1: Languages and their families used in the experiments and sizes of the respective treebanks.
ADJ adjective PART particle ADP adposition PRON pronoun ADV adverb PROPN proper noun AUX auxiliary verb PUNCT punctuation
CONJ coord. conj. SCONJ subord. conj. DET determiner SYM symbol INTJ interjection VERB verb
NOUN noun X other NUM numeral
Table 2: List of part-of-speech tags used in Universal-Dependencies treebanks.We apply the multi-source transfer of delexicalized parser on the UD treebanks in a similar way as McDonald et al. (2011). We use the leave-oneout method: for each language, the delexicalized parser is trained on all other treebanks excluding the one on which the parser is tested. Since all the treebanks share the tagset and annotation style, the training data can be simply concatenated together. To decrease the size of the training data and to reduce the training time, we decided to take only first 10,000 tokens for each language, so the final size of the training data is about 300,000 tokens, which is enough for training delexicalized parser. We use the Malt parser4 (Nivre, 2009), and MST parser (McDonald et al., 2005) with several parameter settings. The results are shown in Table 5.The goal of this paper is to investigate whether the unsupervised parser with added external prior probabilities reflecting the universal annotation scheme is able to compete with the delexicalized methods described in Section 4.1.
We use the unsupervised dependency parser (UDP) implemented by Mareček and Straka (2013). The reason for this choice was that it has reasonably good results across many languages (Mareček, 2015), the source code is freely available,5 and because it includes a mechanism how to import external probabilities. The UDP is based on Dependency Model with Valence, a generative model which consists of two sub-models:
• Stop model pstop(·|tg, dir) represents probability of not generating another dependent in direction dir to a node with POS tag tg. The direction dir can be left or right. If pstop = 1, the node with the tag tg cannot have any dependent in direction dir. If it is 1 in both directions, the node is a leaf.
• Attach model pattach(td|tg, dir) represents probability that the dependent of the node with POS tag tg in direction dir is labeled with POS tag td.
In other words, the stop model generates edges, while the attachmodel generates POS tags for the
4Malt parser in the current version 1.8.1 (http://maltparser.org)
5http://ufal.mff.cuni.cz/udp
4
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
tg p ext stop ADP, ADV, AUX, CONJ, DET, INTJ, NUM, PART, 1.0 PRON, PUNCT, SCONJ, SYM ADJ 0.9 PROPN 0.7 X 0.5 NOUN 0.3 VERB 0.1
Table 3: Manual assignment of stop probabilities for individual POS tags.
new nodes. The inference is done using blocked Gibbs sampling (Gilks et al., 1996). During the inference, the attach and the stop probabilities can be combined linearly with external prior probabilities pext:
pfinalstop = (1− λstop) · pstop + λstop · pextstop,
pfinalattach = (1− λatach) · pattach + λattach · p ext attach,
where the parameters λ define their weights. In the original paper (Mareček and Straka, 2013), the external priors pextstop were computed based on the reducibility principle on a big raw corpora.We use the external prior probabilities to define grammatical rules for POS tags based on UD annotation style. The first type of priors describes how likely a node with a particular POS is a leaf. We manually set the pextstop as listed in Table 3. Even though it is possible to define different left and right pextstop, we decided to set it equally for both the directions, since it is linguistically more language independent.
In a similar way, we predefine external priors for pextattach, describing dependency edges.
6 Preliminary experiments showed that less is more in this type of rules. We ended up only with four rules for attaching punctuation and prepositions, as defined in Table 4.7 Similarly as for pextstop, we set them equally for both left and right directions. We set λattach = 0 for all other possible types of edges, since the priors are not defined for them.
6We had to change the original parser code to do this. 7Note that for example pextattach(PUNC|V ERB, dir) = 1 does not mean that all the dependents of VERB must be PUNC. Since the λattach is less than one, the value 1 only pushes punctuation to be attached below verbs.Instead of setting the external probabilities manually, we can compute them automatically from other treebanks. Such experiments are somewhere in the middle between delexicalized parsers and the minimally supervised parser with some manually added knowledge. They learn some regularities but not as many as the delexicalized parsers do.
Similarly as for delexicalized transfer parser, we compute the probabilities on all treebanks but the one which is currently tested. The probabilities are computed in the following way:
pextstop(·|tg, dir) = NC(tg)
CC(tg, dir) +NC(tg) ,
where NC(tg) is count of all nodes labelled with tag tg across all the training treebanks, CC(tg, dir) is the total number of children in direction dir of all tg nodes in the treebanks, and
pextattach(td|tg, dir) = NE(tg, td, dir)
NE(tg, ∗, dir) ,
where NE(tg, td, dir) is number of dependency edges where the governing node has the POS tag tg, and the dependent node td and is in direction dir from the governing one.
We introduce two additional experiments: direction-dependent learned priors (DDLP) and direction-independent learned priors (DILP). The external probabilities for DDLP are computed according to the previously mentioned formulas.
In DILP, the probabilities are independent on the direction parameter dir. pextstop(·|tg) and pextattach(td|tg) obtain the same values for both directions. Such approach is therefore less supervised. We suppose, that it gains worse results form majority of languages, however, it could be better for some of languages with word ordering different from the majority of languages.
5
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
MST parser Malt parser UDP lang. proj nproj lazy nivre basic +rules DDLP DILP ar 48.8 51.2 50.2 50.4 42.9 51.7 55.2 48.0 bg 79.0 78.5 78.1 77.4 52.6 74.6 73.2 66.8 cu 64.8 66.0 63.1 62.6 46.8 58.1 64.5 59.7 cs 68.0 68.4 66.3 65.8 43.6 60.2 62.8 55.4 da 71.0 71.7 66.9 67.2 40 9 57.7 89.6 54.8 de 69.8 70.0 65.2 65.4 37.4 60.9 63.5 59.8 el 64.3 64.9 63.8 64.1 13.1 63.2 62.3 55.9 en 62.1 62.4 58.2 58.3 28.1 54.6 54.5 53.0 es 71.5 72.2 68.8 69.0 20.4 63.7 66.3 56.1 et 76.4 75.1 70.9 70.5 26.8 79.2 74.6 80.3 eu 50.0 51.2 51.8 50.9 47.1 53.8 50.5 52.3 fa 52.8 54.8 54.0 54.0 41.0 54.8 57.5 45.0 fi 55.1 55.8 50.5 50.4 27.6 48.8 46.6 48.7 fr 74.3 74.8 71.5 71.5 36.0 65.8 69.0 57.9 ga 60.7 61.4 61.1 61.3 37.1 60.2 61.5 57.3 got 63.6 64.5 62.8 62.1 47.3 60.2 62.4 57.4 grc 47.2 48.0 45.8 45.5 41.2 50.6 51.4 51.2 he 62.5 64.0 63.1 62.7 28.2 62.4 64.0 56.5 hi 33.5 34.2 35.5 35.1 42.3 50.9 38.4 54.0 hr 69.3 69.4 67.3 67.1 24.7 61.5 63.4 54.8 hu 57.4 58.0 54.6 54.2 53.4 57.4 55.4 62.8 id 58.5 61.0 59.2 58.6 22.7 48.4 61.3 51.6 it 76.4 77.1 74.0 73.8 42.3 68.8 71.5 60.1 la 56.5 55.9 55.5 55.8 47.0 51.8 52.0 47.1 nl 60.2 60.1 56.5 57.3 37.5 51.2 54.9 48.5 no 70.2 70.4 67.2 66.9 40.9 58.5 61.4 55.7 pl 75.6 76.0 74.7 75.0 63.8 68.0 67.7 64.6 pt 73.9 74.3 72.4 71.7 40.1 64.6 69.4 58.2 ro 68.3 69.3 68.2 67.7 60.4 57.9 66.3 58.9 sl 72.2 72.8 71.2 70.6 48.6 68.6 64.9 56.8 sv 70.2 70.8 66.2 66.2 41.5 59.5 61.7 58.7 ta 34.3 36.5 35.5 35.6 52.2 52.9 48.4 58.4 avg 63.1 63.8 61.6 61.4 39.9 59.4 60.5 56.5
Table 5: Unlabeled attachment scores for the parsers across the languages. The best results are in bold. For MST parser, we used the second order features and its projective (proj) and non-projective (nonproj) variant. For the Malt parser, we used lib-SVM training and stacklazy (lazy) and nivreeager (nivre) algorithms. Unsupervised dependency parser (UDP) was tested without any external priors (basic), with manual prior probabilities (+rules), and with automatically learned probabilities direction dependent (DDLP) and direction independent (DILP).We compare the best transfer parser results also with the previous works. Even though the results are not directly comparable, because different annotation styles were used, we suppose that the annotation unification across the treebanks in Universal Dependencies should improve the transfer parser scores. McDonald et al. (2011) presented 61.7% of averaged accuracy over 8 languages. On the same languages, our transfer parser on UD reached 70.1%. When compared to Rosa (2015), we experimented with 23 common languages, our average score on them is 62.5%, Rosa’s is 56.6%. The higher attachment scores in our experiments confirms that the annotations in UD treebanks are more unified and serve better for transferring between languages.","The results of delexicalized transfer parsers, unsupervised parser and minimally supervised parsers with different degrees of supervision on Universal Dependencies are compared in Table 5. We try several settings of parameters for both Malt parser and MST parser, and show the results of two of
them for each one.8 We run the Unsupervised dependency parser by Mareček and Straka (2013), labeled as UDP. For UDP, we report four different setings. The basic variant is completely unsupervised parsing without any external prior probabilities. The +rules column shows the results of
8The results of different parameter settings for both parser varied only little (at most 2% difference for all the languages).
6
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
0
20
40
60
80
100
bg cu cs hr pl sl da de en got nl no sv et fi hu
+rules dilp ddlp mst
0
20
40
60
80
100
es fr it la pt ro el grc fa ga ar eu he hi id ta
+rules dilp ddlp mst
Figure 1: Comparison of delexicalized parsing methods with different degrees of supervision. UDP with manually set priors (+rules), direction dependent (DDLP) and independent (DILP) learning of priors versus delexicalized transfer of MST parser (mst). Languages are ordered according to their language families: Slavic (bg, cu, cs, hr, pl, sl), Germanic (da, de, en, got, nl, no, sv), Romance (es, fr, it, la, pt, ro), Hellenic (el, grc), Uralic (et, fi, hu), and others (fa, ga, ar, eu, he, hi, id, ta).
our minimally supervised parser (Section 4.2.1) using the external probabilities defined manually (Tables 3 and 4). Both the λstop and λattach parameters are set to 0.5. The DDLP and DILP variants use automatically learned prior probabilities form other treebanks (Section 4.2.2).","In the last two decades, many dependency treebanks for various languages have been manually annotated. They differ in word categories (POS tagset), syntactic categories (dependency relations), and structure for individual language phenomena. The CoNLL shared tasks for dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) unified the file format, and thus the dependency parsers could easily work with 20 different treebanks. Still, the parsing outputs were not comparable between languages since the annotation styles differed even between closely related languages.
In recent years, there have been a huge effort to normalize dependency annotation styles. The Stanford dependencies (de Marneffe and Manning, 2008) were adjusted to be more universal across languages (de Marneffe et al., 2014). Mc-
donald et al. (2013) started to develop Google Universal Treebank, a collection of new treebanks with common annotation style using the Stanford dependencies and Universal tagset (Petrov et al., 2012) consisting of 12 part-of-speech tags. Zeman et al. (2012) produced a collection of treebanks HamleDT, in which about 30 treebanks were automatically converted to a Prague Dependency Treebank style (Hajič et al., 2006). Later, they converted all the treebanks also into the Stanford style (Rosa et al., 2014).
The researchers from the previously mentioned projects joined their efforts to create one common standard: Universal Dependencies (Nivre et al., 2016). They used the Stanford dependencies (de Marneffe et al., 2014) with minor changes, extended the Google universal tagset (Petrov et al., 2012) from 12 to 17 part-of-speech tags and used the Interset morphological features (Zeman, 2008) from the HamleDT project (Zeman et al., 2014). In the current version 1.2, Universal Dependencies collection (UD) consists of 37 treebanks of 33 different languages and it is very likely that it will continue growing and become common source and standard for many researchers. Now, it is time to revisit the dependency parsing methods and to investigate their behavior on this new unified style.
The goal of this paper is to apply cross language delexicalized transfer parsers (e.g. (McDonald et al., 2011)) on UD and compare their results with unsupervised and minimally supervised parser. Both the methods are intended for parsing languages, for which no annotated treebank exists and both the methods can profit from UD.
In the area of dependency parsing, the term ”unsupervised” is understood as that no annotated treebanks are used for training and when supervised POS tags are used for grammar inference, we can deal with them only as with further un-
2
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
specified types of word.1 Therefore, we introduce a minimally supervised parser: We use unsupervised dependency parser operating on supervised POS tags, however, we add external prior probabilities that push the inferred dependency trees in the right way. These external priors can be set manually as handwritten rules or trained on other treebanks, similarly as the transfer parsers. This allows us to compare the parser settings with different degrees of supervision:
• delexicalized training of supervised parsers
• minimally supervised parser using some external probabilities learned in supervised way
• minimally supervised parser using a couple of external probabilities set manually
• fully unsupervised parser
Ideally, the parser should learn only the language-independent characteristics of dependency trees. However, it is hard to define what such characteristics are. For each particular language, we will show what degree of supervision is the best for parsing. Our hypothesis is that a kind of minimally supervised parser can compete with delexicalized transfer parsers.","There were many papers dealing with delexicalized parsing. Zeman and Resnik (2008) transfer a delexicalized parsing model to Danish and Swedish. McDonald et al. (2011) present a transfer-parser matrix from/to 9 European languages and introduce also multi-source transfer, where more training treebanks are concatenated to form more universal data. Both papers mention the problem of different annotation styles across treebanks, which complicates the transfer. Rosa (2015) uses already harmonized treebanks (Rosa et al., 2014) and compare the delexicalized parsing for Prague and Stanford annotation styles.
Unsupervised dependency parsing methods made a big progress started by the Dependency Model with Valence (Klein and Manning, 2004), which was further improved by many other researchers (Headden III et al., 2009; Blunsom and Cohn, 2010; Spitkovsky et al., 2011b; Spitkovsky
1In the fully unsupervised setting, we cannot for example simply push verbs to the roots and nouns to become their dependents. This is already a kind of supervision.
et al., 2012). Many of these works induce grammar based on the gold POS tags, some of them use unsupervised word classes (Spitkovsky et al., 2011a; Mareček, 2015). However, it seems that the research in this field declines in the recent years, probably because its results are still not able to compete with projection and delexicalized methods. Naseem et al. (2010) joined unsupervised grammar induction with a couple of syntactic rules.","It is evident that the MST parser achieved the best scores. It parsed best 20 out of 32 languages and its non-projective variant reached 63.8% averaged attachment score. The Malt parser was worse than MST by 2% in the averaged attachment score.9 The basic UDP without additional rules performs very poorly, however, with added external prior probabilities, it is competitive with the delexicalized transfer parser methods. 12 out of 32 languages were parsed better by UDP using one variant of the external priors.
With hand-written prior probabilities (+rules),
9We used the Malt parser with its default feature set. Tuning in this specific delexicalized task would probably bring a bit better results.
the averaged attachment score reached only 59%, however, it is better than the MST parser on 6 languages: Arabic, Estonian, Basque, Old Greek, Hindi, and Tamil, in two cases by a wide margin. For Persian, the scores are equal.
The averaged attachment score for UDP with direction-independent learned priors (DILP) is even lower (56.5%), however, it parsed 6 languages better than MST: Estonian, Basque, Old Greek, Hindi, Hungarian, and Tamil. Direction dependent learning of priors end up with 60.5% attachment score and 9 languages better than MST.
Based on these results, we can say that the minimally supervised parser, which takes less information from other annotated treebanks, is more suitable for the more exotic languages, i.e. for languages whose families are less common among the annotated treebanks. Figure 4.2.2 shows histograms of attachment scores across languages, now ordered according to the language families. All the Slavic and Romance languages and almost all the Germanic languages10 are parsed best by
10Danish is the only exception.
7
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
the MST parser. Finnish from the three Uralic languages and Greek from the two Hellenic languages are also parsed best by MST. Other 12 languages were better parsed by one of the less supervised methods.
Less-resourced languages, for which the annotated treebanks are missing, may be therefore better parsed by less supervised parsers, especially if they do not belong to the Indo-European language family. The MST transfer parser has probably been over-trained on these Indo-European family languages and is not able to generalize enough to more distant languages. The rules we added to the unsupervised dependency parser (+rules experiment) are universal in the direction of dependencies (left/right branching) and cover much more languages.","We used the Universal Dependencies treebank collection to test delexicalized transfer parsers and unsupervised dependency parser enriched by external attach and stop prior probabilities. We found that whereas the MST delexicalized transfer parser is better in average, our minimally supervised parser performs better on many non-IndoEuropean languages and therefore can be suitable to parse often low-resourced exotic languages, for which treebanks do not exist."
15,"In this paper, we compare delexicalized transfer and minimally supervised parsing techniques on 32 different languages from Universal Dependencies treebank collection. The minimal supervision is in adding handcrafted universal grammatical rules for POS tags. The rules are incorporated into the unsupervised dependency parser in forms of external prior probabilities. We also experiment with learning this probabilities from other treebanks. The average attachment score of our parser is slightly lower then the delexicalized transfer parser, however, it performs better for languages from less resourced language families (non-Indo-European) and is therefore suitable for those, for which the treebanks often do not exist.",Delexicalized and Minimally Supervised Parsing on Universal Dependencies,98,"This paper evaluates a minimally supervised dependency parser -- a version of
the DMV model with manually set prior probabilities -- on (most of) the
treebanks from Universal Dependencies, v1.2. It reports results that are on
average slightly lower than a couple of delexicalized transfer parsers but
(sometimes substantially) better on a few non-Indo-European languages.

The idea of biasing an otherwise unsupervised parser with some basic
""universal"" rules have been used a number of times before in the literature, so
the main value of the present paper is an empirical evaluation of this approach
on the new UD treebanks. However, the approach and evaluation leaves some 
questions unanswered.

First of all, I want to know why only unlabeled parsing is considered. This may
have been appropriate (or at least necessary) before dependency labels were
standardised, but the whole point of UD is to give a uniform analysis in terms
of typed dependencies, and any parsing approach that does not take this into
account seems misguided. And since the approach is based on manually defined
universal rules, it would have been easy enough to formulate rules for labeled
dependencies.

Second, I would like to know more about how the prior probabilities were set
or, in other words, what universal grammar they are meant to encode and how.
Were alternatives tested and, if so, how were they evaluated? In the present
version of the paper, we are just presented with a bunch of numbers without any
explanation or justification except that they are âbased on UD annotation
styleâ.

Third, one of the main claims of the paper is that the unsupervised system
works better for non-Indo-European languages. This seems to be supported by the
raw numbers, but what exactly is going on here? What types of dependencies are
handled better by the unsupervised system? Even though a full error analysis
would be out of scope in a short paper, an analysis of a small sample could be
really interesting.

Finally, the comparison to the delexicalized transfer parsers seems to be
biased by a number of factors. Restricting it to unlabeled dependencies is one
such thing, since the delexicalized parser could easily have produced labeled
dependencies. Another thing is the amount of training data, which was
arbitrarily restricted to 10,000 tokens per treebank. Finally, it seems that
the delexicalized parsers were not properly tuned. Just replacing word forms
and lemmas by underscores without revising the feature models is not likely to
produce optimal results.",,3,3,Poster,4,4,4,4,3,5,2,2,2016,"In all our experiments, we use the Universal Dependencies treebank collection2 in its current version 1.2. For languages for which there is more than one treebank, we experiment only with the first one.3 We also exclude ’Japan-KTC’ treebank, since the full data are not available. Finally, we experiment with 32 dependency treebanks, each representing a different language. The treebanks, their language families, and their sizes are listed in Table 1.
Before training the parsers, all the treebanks are delexicalized. We substitute all the forms and lemmas by underscores, which are used for undefined values. The same is done with the morphological features and dependency relations. The only information remained is the universal POS tags and the dependency structure (the parent number for each token). The Universal Dependencies use POS tagset consisting of 17 POS tags listed in Table 2.In the following experiments, we compare delexicalized transfer parsing methods and minimallysupervised methods on the UD treebanks. All the experiments are conducted as if we parsed a language whose syntax is unknown for us. This means that we do not prefer training on syntactically similar languages, we do not prefer right branching or left branching, and do not add language specific word-order rules like preferring SVO or SOV, adjectives before nouns, prepositions vs. postpositions etc.
2universaldependencices.org 3We exclude ’Ancient Greek-PROIEL’, ’Finnish-FTB’,
’Japan-KTC’, ’Latin-ITT’, and ’Latin-PROIEL’ treebanks.
3
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
language family tokens ar Arabic Semitic 282384 bg Bulgarian Slavic 156319 cu Old Slav. Slavic 57507 cs Czech Slavic 1503738 da Danish Germanic 100733 de German Germanic 293088 el Greek Hellenic 59156 en English Germanic 254830 es Spanish Romance 423346 et Estonian Uralic 6461 eu Basque isolate 121443 fa Persian Iranian 151624 fi Finnish Uralic 181022 fr French Romance 389764 ga Irish Celtic 23686 got Gothic Germanic 56128 grc Old Greek Hellenic 244993 he Hebrew Semitic 115535 hi Hindi Indo-Iranian 351704 hr Croatian Slavic 87765 hu Hungarian Uralic 26538 id Indonesian Malayic 121923 it Italian Romance 252967 la Latin Romance 47303 nl Dutch Germanic 200654 no Norwegian Germanic 311277 pl Polish Slavic 83571 pt Portuguese Romance 212545 ro Romanian Romance 12094 sl Slovenian Slavic 140418 sv Swedish Germanic 96819 ta Tamil Dravidian 9581
Table 1: Languages and their families used in the experiments and sizes of the respective treebanks.
ADJ adjective PART particle ADP adposition PRON pronoun ADV adverb PROPN proper noun AUX auxiliary verb PUNCT punctuation
CONJ coord. conj. SCONJ subord. conj. DET determiner SYM symbol INTJ interjection VERB verb
NOUN noun X other NUM numeral
Table 2: List of part-of-speech tags used in Universal-Dependencies treebanks.We apply the multi-source transfer of delexicalized parser on the UD treebanks in a similar way as McDonald et al. (2011). We use the leave-oneout method: for each language, the delexicalized parser is trained on all other treebanks excluding the one on which the parser is tested. Since all the treebanks share the tagset and annotation style, the training data can be simply concatenated together. To decrease the size of the training data and to reduce the training time, we decided to take only first 10,000 tokens for each language, so the final size of the training data is about 300,000 tokens, which is enough for training delexicalized parser. We use the Malt parser4 (Nivre, 2009), and MST parser (McDonald et al., 2005) with several parameter settings. The results are shown in Table 5.The goal of this paper is to investigate whether the unsupervised parser with added external prior probabilities reflecting the universal annotation scheme is able to compete with the delexicalized methods described in Section 4.1.
We use the unsupervised dependency parser (UDP) implemented by Mareček and Straka (2013). The reason for this choice was that it has reasonably good results across many languages (Mareček, 2015), the source code is freely available,5 and because it includes a mechanism how to import external probabilities. The UDP is based on Dependency Model with Valence, a generative model which consists of two sub-models:
• Stop model pstop(·|tg, dir) represents probability of not generating another dependent in direction dir to a node with POS tag tg. The direction dir can be left or right. If pstop = 1, the node with the tag tg cannot have any dependent in direction dir. If it is 1 in both directions, the node is a leaf.
• Attach model pattach(td|tg, dir) represents probability that the dependent of the node with POS tag tg in direction dir is labeled with POS tag td.
In other words, the stop model generates edges, while the attachmodel generates POS tags for the
4Malt parser in the current version 1.8.1 (http://maltparser.org)
5http://ufal.mff.cuni.cz/udp
4
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
tg p ext stop ADP, ADV, AUX, CONJ, DET, INTJ, NUM, PART, 1.0 PRON, PUNCT, SCONJ, SYM ADJ 0.9 PROPN 0.7 X 0.5 NOUN 0.3 VERB 0.1
Table 3: Manual assignment of stop probabilities for individual POS tags.
new nodes. The inference is done using blocked Gibbs sampling (Gilks et al., 1996). During the inference, the attach and the stop probabilities can be combined linearly with external prior probabilities pext:
pfinalstop = (1− λstop) · pstop + λstop · pextstop,
pfinalattach = (1− λatach) · pattach + λattach · p ext attach,
where the parameters λ define their weights. In the original paper (Mareček and Straka, 2013), the external priors pextstop were computed based on the reducibility principle on a big raw corpora.We use the external prior probabilities to define grammatical rules for POS tags based on UD annotation style. The first type of priors describes how likely a node with a particular POS is a leaf. We manually set the pextstop as listed in Table 3. Even though it is possible to define different left and right pextstop, we decided to set it equally for both the directions, since it is linguistically more language independent.
In a similar way, we predefine external priors for pextattach, describing dependency edges.
6 Preliminary experiments showed that less is more in this type of rules. We ended up only with four rules for attaching punctuation and prepositions, as defined in Table 4.7 Similarly as for pextstop, we set them equally for both left and right directions. We set λattach = 0 for all other possible types of edges, since the priors are not defined for them.
6We had to change the original parser code to do this. 7Note that for example pextattach(PUNC|V ERB, dir) = 1 does not mean that all the dependents of VERB must be PUNC. Since the λattach is less than one, the value 1 only pushes punctuation to be attached below verbs.Instead of setting the external probabilities manually, we can compute them automatically from other treebanks. Such experiments are somewhere in the middle between delexicalized parsers and the minimally supervised parser with some manually added knowledge. They learn some regularities but not as many as the delexicalized parsers do.
Similarly as for delexicalized transfer parser, we compute the probabilities on all treebanks but the one which is currently tested. The probabilities are computed in the following way:
pextstop(·|tg, dir) = NC(tg)
CC(tg, dir) +NC(tg) ,
where NC(tg) is count of all nodes labelled with tag tg across all the training treebanks, CC(tg, dir) is the total number of children in direction dir of all tg nodes in the treebanks, and
pextattach(td|tg, dir) = NE(tg, td, dir)
NE(tg, ∗, dir) ,
where NE(tg, td, dir) is number of dependency edges where the governing node has the POS tag tg, and the dependent node td and is in direction dir from the governing one.
We introduce two additional experiments: direction-dependent learned priors (DDLP) and direction-independent learned priors (DILP). The external probabilities for DDLP are computed according to the previously mentioned formulas.
In DILP, the probabilities are independent on the direction parameter dir. pextstop(·|tg) and pextattach(td|tg) obtain the same values for both directions. Such approach is therefore less supervised. We suppose, that it gains worse results form majority of languages, however, it could be better for some of languages with word ordering different from the majority of languages.
5
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
MST parser Malt parser UDP lang. proj nproj lazy nivre basic +rules DDLP DILP ar 48.8 51.2 50.2 50.4 42.9 51.7 55.2 48.0 bg 79.0 78.5 78.1 77.4 52.6 74.6 73.2 66.8 cu 64.8 66.0 63.1 62.6 46.8 58.1 64.5 59.7 cs 68.0 68.4 66.3 65.8 43.6 60.2 62.8 55.4 da 71.0 71.7 66.9 67.2 40 9 57.7 89.6 54.8 de 69.8 70.0 65.2 65.4 37.4 60.9 63.5 59.8 el 64.3 64.9 63.8 64.1 13.1 63.2 62.3 55.9 en 62.1 62.4 58.2 58.3 28.1 54.6 54.5 53.0 es 71.5 72.2 68.8 69.0 20.4 63.7 66.3 56.1 et 76.4 75.1 70.9 70.5 26.8 79.2 74.6 80.3 eu 50.0 51.2 51.8 50.9 47.1 53.8 50.5 52.3 fa 52.8 54.8 54.0 54.0 41.0 54.8 57.5 45.0 fi 55.1 55.8 50.5 50.4 27.6 48.8 46.6 48.7 fr 74.3 74.8 71.5 71.5 36.0 65.8 69.0 57.9 ga 60.7 61.4 61.1 61.3 37.1 60.2 61.5 57.3 got 63.6 64.5 62.8 62.1 47.3 60.2 62.4 57.4 grc 47.2 48.0 45.8 45.5 41.2 50.6 51.4 51.2 he 62.5 64.0 63.1 62.7 28.2 62.4 64.0 56.5 hi 33.5 34.2 35.5 35.1 42.3 50.9 38.4 54.0 hr 69.3 69.4 67.3 67.1 24.7 61.5 63.4 54.8 hu 57.4 58.0 54.6 54.2 53.4 57.4 55.4 62.8 id 58.5 61.0 59.2 58.6 22.7 48.4 61.3 51.6 it 76.4 77.1 74.0 73.8 42.3 68.8 71.5 60.1 la 56.5 55.9 55.5 55.8 47.0 51.8 52.0 47.1 nl 60.2 60.1 56.5 57.3 37.5 51.2 54.9 48.5 no 70.2 70.4 67.2 66.9 40.9 58.5 61.4 55.7 pl 75.6 76.0 74.7 75.0 63.8 68.0 67.7 64.6 pt 73.9 74.3 72.4 71.7 40.1 64.6 69.4 58.2 ro 68.3 69.3 68.2 67.7 60.4 57.9 66.3 58.9 sl 72.2 72.8 71.2 70.6 48.6 68.6 64.9 56.8 sv 70.2 70.8 66.2 66.2 41.5 59.5 61.7 58.7 ta 34.3 36.5 35.5 35.6 52.2 52.9 48.4 58.4 avg 63.1 63.8 61.6 61.4 39.9 59.4 60.5 56.5
Table 5: Unlabeled attachment scores for the parsers across the languages. The best results are in bold. For MST parser, we used the second order features and its projective (proj) and non-projective (nonproj) variant. For the Malt parser, we used lib-SVM training and stacklazy (lazy) and nivreeager (nivre) algorithms. Unsupervised dependency parser (UDP) was tested without any external priors (basic), with manual prior probabilities (+rules), and with automatically learned probabilities direction dependent (DDLP) and direction independent (DILP).We compare the best transfer parser results also with the previous works. Even though the results are not directly comparable, because different annotation styles were used, we suppose that the annotation unification across the treebanks in Universal Dependencies should improve the transfer parser scores. McDonald et al. (2011) presented 61.7% of averaged accuracy over 8 languages. On the same languages, our transfer parser on UD reached 70.1%. When compared to Rosa (2015), we experimented with 23 common languages, our average score on them is 62.5%, Rosa’s is 56.6%. The higher attachment scores in our experiments confirms that the annotations in UD treebanks are more unified and serve better for transferring between languages.","The results of delexicalized transfer parsers, unsupervised parser and minimally supervised parsers with different degrees of supervision on Universal Dependencies are compared in Table 5. We try several settings of parameters for both Malt parser and MST parser, and show the results of two of
them for each one.8 We run the Unsupervised dependency parser by Mareček and Straka (2013), labeled as UDP. For UDP, we report four different setings. The basic variant is completely unsupervised parsing without any external prior probabilities. The +rules column shows the results of
8The results of different parameter settings for both parser varied only little (at most 2% difference for all the languages).
6
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
0
20
40
60
80
100
bg cu cs hr pl sl da de en got nl no sv et fi hu
+rules dilp ddlp mst
0
20
40
60
80
100
es fr it la pt ro el grc fa ga ar eu he hi id ta
+rules dilp ddlp mst
Figure 1: Comparison of delexicalized parsing methods with different degrees of supervision. UDP with manually set priors (+rules), direction dependent (DDLP) and independent (DILP) learning of priors versus delexicalized transfer of MST parser (mst). Languages are ordered according to their language families: Slavic (bg, cu, cs, hr, pl, sl), Germanic (da, de, en, got, nl, no, sv), Romance (es, fr, it, la, pt, ro), Hellenic (el, grc), Uralic (et, fi, hu), and others (fa, ga, ar, eu, he, hi, id, ta).
our minimally supervised parser (Section 4.2.1) using the external probabilities defined manually (Tables 3 and 4). Both the λstop and λattach parameters are set to 0.5. The DDLP and DILP variants use automatically learned prior probabilities form other treebanks (Section 4.2.2).","In the last two decades, many dependency treebanks for various languages have been manually annotated. They differ in word categories (POS tagset), syntactic categories (dependency relations), and structure for individual language phenomena. The CoNLL shared tasks for dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) unified the file format, and thus the dependency parsers could easily work with 20 different treebanks. Still, the parsing outputs were not comparable between languages since the annotation styles differed even between closely related languages.
In recent years, there have been a huge effort to normalize dependency annotation styles. The Stanford dependencies (de Marneffe and Manning, 2008) were adjusted to be more universal across languages (de Marneffe et al., 2014). Mc-
donald et al. (2013) started to develop Google Universal Treebank, a collection of new treebanks with common annotation style using the Stanford dependencies and Universal tagset (Petrov et al., 2012) consisting of 12 part-of-speech tags. Zeman et al. (2012) produced a collection of treebanks HamleDT, in which about 30 treebanks were automatically converted to a Prague Dependency Treebank style (Hajič et al., 2006). Later, they converted all the treebanks also into the Stanford style (Rosa et al., 2014).
The researchers from the previously mentioned projects joined their efforts to create one common standard: Universal Dependencies (Nivre et al., 2016). They used the Stanford dependencies (de Marneffe et al., 2014) with minor changes, extended the Google universal tagset (Petrov et al., 2012) from 12 to 17 part-of-speech tags and used the Interset morphological features (Zeman, 2008) from the HamleDT project (Zeman et al., 2014). In the current version 1.2, Universal Dependencies collection (UD) consists of 37 treebanks of 33 different languages and it is very likely that it will continue growing and become common source and standard for many researchers. Now, it is time to revisit the dependency parsing methods and to investigate their behavior on this new unified style.
The goal of this paper is to apply cross language delexicalized transfer parsers (e.g. (McDonald et al., 2011)) on UD and compare their results with unsupervised and minimally supervised parser. Both the methods are intended for parsing languages, for which no annotated treebank exists and both the methods can profit from UD.
In the area of dependency parsing, the term ”unsupervised” is understood as that no annotated treebanks are used for training and when supervised POS tags are used for grammar inference, we can deal with them only as with further un-
2
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
specified types of word.1 Therefore, we introduce a minimally supervised parser: We use unsupervised dependency parser operating on supervised POS tags, however, we add external prior probabilities that push the inferred dependency trees in the right way. These external priors can be set manually as handwritten rules or trained on other treebanks, similarly as the transfer parsers. This allows us to compare the parser settings with different degrees of supervision:
• delexicalized training of supervised parsers
• minimally supervised parser using some external probabilities learned in supervised way
• minimally supervised parser using a couple of external probabilities set manually
• fully unsupervised parser
Ideally, the parser should learn only the language-independent characteristics of dependency trees. However, it is hard to define what such characteristics are. For each particular language, we will show what degree of supervision is the best for parsing. Our hypothesis is that a kind of minimally supervised parser can compete with delexicalized transfer parsers.","There were many papers dealing with delexicalized parsing. Zeman and Resnik (2008) transfer a delexicalized parsing model to Danish and Swedish. McDonald et al. (2011) present a transfer-parser matrix from/to 9 European languages and introduce also multi-source transfer, where more training treebanks are concatenated to form more universal data. Both papers mention the problem of different annotation styles across treebanks, which complicates the transfer. Rosa (2015) uses already harmonized treebanks (Rosa et al., 2014) and compare the delexicalized parsing for Prague and Stanford annotation styles.
Unsupervised dependency parsing methods made a big progress started by the Dependency Model with Valence (Klein and Manning, 2004), which was further improved by many other researchers (Headden III et al., 2009; Blunsom and Cohn, 2010; Spitkovsky et al., 2011b; Spitkovsky
1In the fully unsupervised setting, we cannot for example simply push verbs to the roots and nouns to become their dependents. This is already a kind of supervision.
et al., 2012). Many of these works induce grammar based on the gold POS tags, some of them use unsupervised word classes (Spitkovsky et al., 2011a; Mareček, 2015). However, it seems that the research in this field declines in the recent years, probably because its results are still not able to compete with projection and delexicalized methods. Naseem et al. (2010) joined unsupervised grammar induction with a couple of syntactic rules.","It is evident that the MST parser achieved the best scores. It parsed best 20 out of 32 languages and its non-projective variant reached 63.8% averaged attachment score. The Malt parser was worse than MST by 2% in the averaged attachment score.9 The basic UDP without additional rules performs very poorly, however, with added external prior probabilities, it is competitive with the delexicalized transfer parser methods. 12 out of 32 languages were parsed better by UDP using one variant of the external priors.
With hand-written prior probabilities (+rules),
9We used the Malt parser with its default feature set. Tuning in this specific delexicalized task would probably bring a bit better results.
the averaged attachment score reached only 59%, however, it is better than the MST parser on 6 languages: Arabic, Estonian, Basque, Old Greek, Hindi, and Tamil, in two cases by a wide margin. For Persian, the scores are equal.
The averaged attachment score for UDP with direction-independent learned priors (DILP) is even lower (56.5%), however, it parsed 6 languages better than MST: Estonian, Basque, Old Greek, Hindi, Hungarian, and Tamil. Direction dependent learning of priors end up with 60.5% attachment score and 9 languages better than MST.
Based on these results, we can say that the minimally supervised parser, which takes less information from other annotated treebanks, is more suitable for the more exotic languages, i.e. for languages whose families are less common among the annotated treebanks. Figure 4.2.2 shows histograms of attachment scores across languages, now ordered according to the language families. All the Slavic and Romance languages and almost all the Germanic languages10 are parsed best by
10Danish is the only exception.
7
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
the MST parser. Finnish from the three Uralic languages and Greek from the two Hellenic languages are also parsed best by MST. Other 12 languages were better parsed by one of the less supervised methods.
Less-resourced languages, for which the annotated treebanks are missing, may be therefore better parsed by less supervised parsers, especially if they do not belong to the Indo-European language family. The MST transfer parser has probably been over-trained on these Indo-European family languages and is not able to generalize enough to more distant languages. The rules we added to the unsupervised dependency parser (+rules experiment) are universal in the direction of dependencies (left/right branching) and cover much more languages.","We used the Universal Dependencies treebank collection to test delexicalized transfer parsers and unsupervised dependency parser enriched by external attach and stop prior probabilities. We found that whereas the MST delexicalized transfer parser is better in average, our minimally supervised parser performs better on many non-IndoEuropean languages and therefore can be suitable to parse often low-resourced exotic languages, for which treebanks do not exist."
16,"Measuring topic quality is essential for scoring the learned topics and their subsequent use in Information Retrieval and Text classification. To measure quality of Latent Dirichlet Allocation (LDA) based topics learned from text, we propose a novel approach based on grouping of topic words into buckets (TBuckets). A single large bucket signifies a single coherent theme, in turn indicating high topic coherence. TBuckets represents topic words using their word embeddings and employs 3 different techniques for creating buckets of words - i) clustering based, ii) using singular value decomposition (SVD) and iii) SVD with reorganization. The TBuckets approach outperforms the state-of-the-art techniques when evaluated using three publicly available datasets. Further, we demonstrate the usefulness of TBuckets for the task of weakly supervised text classification.",Measuring Topic Quality using Word Buckets,103,"This paper proposes a method for evaluating topic quality based on using word
embeddings to calculate similarity (either directly or indirectly via matrix
factorisation), achieving impressive results over standard datasets.

The proposed method represents a natural but important next step in the
evolutionary path of research on topic evaluation. The thing that troubled me
most with the results was that, while you achieve state-of-the-art results for
all three datasets, there are large inconsistencies in which methods perform
and which methods perform less well (below the state of the art). In practice,
none of the proposed methods consistently beats the state of the art, and the
SVD-based methods perform notably badly over the genomics dataset. For someone
who wants to take your method off the shelf and use it over any arbitrary
dataset, this is a considerable worry. I suspect that the lower results for
SVD over genomics relate to the proportion of OOV terms (see comment below),
and that it may be possible to automatically predict which method will perform
best based on vocab match with GloVe etc., but there is no such discussion in
the paper.

Other issues:

- the proposed method has strong similarities with methods proposed in the
  lexical chaining literature, which I would encourage the authors to read up
  on and include in any future version of the paper

- you emphasis that your method has no parameters, but the word embedding
  methods have a large number of parameters, which are implicit in your
  method. Not a huge deal, but worth acknowledging

- how does your method deal with OOV terms, e.g. in the genomics dataset
  (i.e. terms not present in the pretrained GloVe embeddings)? Are they simply
  ignored? What impact does this have on the method?

Low-level issues:

- in your description of word embeddings in Section 2.1, you implicitly assume
  that the length of the vector is unimportant (in saying that cosine
  similarity can be used to measure the similarity between two vectors). If
  the vectors are unit length, this is unproblematic, but word2vec actually
  doesn't return unit-length vectors (the pre-trained vectors have been
  normalised post hoc, and if you run word2vec yourself, the vector length is
  certainly not uniform). A small detail, but important.

- the graphs in Figure 1 are too small to be readable",,2,4,Poster,4,5,3,4,3,5,3,3,2016,"Word embeddings are vector representations of words which have become popular recently. Some efficient approaches to learn these vector representations from large unlabeled corpora are by Mikolov et al. (2013) and Pennington et al. (2014). Each word is mapped to a real-valued vector in d dimensions, such that vectors of semantically similar words lie close to each other. The cosine similarity of word vectors in this space is hence, a good estimation of semantic similarity between the corresponding words.Through SVD, a rectangular matrix A can be factorized to three components U , S and V where the matrix U contains eigenvectors of AAT , the matrix V contains the eigenvectors of ATA and the matrix S is a diagonal matrix containing the singular values of A, which are effectively square roots of eigenvalues of AAT and ATA. Intuitively, the SVD of a rectangular matrix, allows to express the matrix as a combination of three geometrical operations - rotation through U , scaling through S and another rotation through V . More clearly, for an m × n matrix A, where m entities are represented by their n features, the U matrix helps to identify important dimensions among the entities and the V matrix does so for important dimensions among the entities in terms of their features. In the paper, we focus particularly on the V matrix allowing us to obtain eigenvectors of the matrix ATA, which captures feature-feature interactions of the m entities.The TBuckets idea is based on how we humans generally observe a topic and conclude on its coherence. Assuming a topic with words ordered (descending) by their probability of getting generated from the topic, the general procedure involves observing the topic words one by one and putting them in some form of logical groups (or buckets, as we call them). Starting with a fresh bucket for the first word, every new word is put in an already created bucket if the word is semantically similar or semantically associated with the words in the bucket; otherwise the word is put in a new bucket. On completion of this exercise, all topic words would be distributed in various buckets. A distribution with a single large bucket and few small buckets would signify better coherence. On the other hand, a distribution with multiple medium sized buckets would indicate lower coherence.
To see an example dry run of the above procedure, lets consider a coherent and a non-coherent topic. The topic {storm, weather, wind, temperature, rain, snow, air, high, cold, northern} is quite coherent and deals with weather and its factors. The bucket procedure for this topic executes as follows:
1. Word Seen: storm. Bucket-1: {storm}
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
2. Word Seen: weather. Bucket-1: {storm, weather}
3. Word Seen: wind. Bucket-1: {storm, weather, wind}
4. Word Seen: temperature. Bucket-1: {storm, weather, wind, temperature}
5. Word Seen: rain. Bucket-1: {storm, weather, wind, temperature, rain}.
6. Word Seen: snow. Bucket-1: {storm, weather, wind, temperature, rain, snow}.
7. Word Seen: air. Bucket-1: {storm, weather, wind, temperature, rain, snow, air}.
8. Word Seen: high. Bucket-1: {storm, weather, wind, temperature, rain, snow, air}; Bucket-2: {high}
9. Word Seen: cold. Bucket-1: {storm, weather, wind, temperature, rain, snow, air, cold}; Bucket-2: {high}
10. Word Seen: northern. Bucket-1: {storm, weather, wind, temperature, rain, snow, air, cold}; Bucket-2: {high}; Bucket-3: {northern}
Another topic {karzai, afghan, miner, official, mine, assange, government, kabul, afghanistan, wikileaks} is not coherent and deals with multiple areas like afghanistan, wikileaks and mining. The bucket procedure for this topic executes as follows:
1. Word Seen: karzai. Bucket-1: {karzai}
2. Word Seen: afghan. Bucket-1: {karzai, afghan}
3. Word Seen: miner. Bucket-1: {karzai, afghan}; Bucket-2: {miner}
4. Word Seen: official. Bucket-1: {storm, weather}; Bucket-2: {miner}; Bucket-3: {official}
5. Word Seen: mine. Bucket-1: {karzai, afghan}; Bucket-2: {miner, mine}; Bucket-3: {official}
6. Word Seen: assange. Bucket-1: {karzai, afghan}; Bucket-2: {miner, mine}; Bucket-3: {official}; Bucket-4: {assange}
7. Word Seen: government. Bucket-1: {karzai, afghan}; Bucket-2: {miner, mine}; Bucket-3: {official, government}; Bucket-4: {assange}
8. Word Seen: kabul. Bucket-1: {karzai, afghan, kabul}; Bucket-2: {miner, mine}; Bucket-3: {official, government}; Bucket-4: {assange}
9. Word Seen: afghanistan. Bucket-1: {karzai, afghan, kabul, afghanistan}; Bucket-2: {miner, mine}; Bucket-3: {official, government}; Bucket-4: {assange}
10. Word Seen: wikileaks. Bucket-1: {karzai, afghan, kabul, afghanistan}; Bucket-2: {miner, mine}; Bucket-3: {official, government}; Bucket-4: {assange, wikileaks}
It is evident from the above iterations that final distributions of topic words into buckets, reflects the coherence of a topic quite closely. Based on this idea, we devise approaches to carry out topic word distribution into buckets, automatically. We consider properties of the finally formed buckets to compute a coherence score for a topic.
The TBuckets approach powers us to do this bucketing automatically and generates a coherence score for a topic. It mainly requires two resources namely the word embeddings of topic words and the topic model (i.e. complete set of topics with words ordered according to their generation probability). These resources are not difficult to obtain as the topic model is available de facto and word embeddings of a large set of words, trained on various corpora, are now available publicly. The approach can be characterized through three techniques.
The first technique (TBuckets-Clustering) is based on a the idea of clustering which arises intuitively when we think of forming related groups among a set of items (words here). The TBucketsClustering technique involves representing words by their word embeddings and clustering them using agglomerative clustering. We use a maximum distance threshold as an input to the clustering for facilitating the agglomeration. Further we try with all single, average and complete linkage configurations. The technique proves to be competent involving only a single parameter.
The notion for the SVD-based technique (TBuckets-SVD) is to find the various orthogonal
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
dimensions or sub-themes inside a topic and attach words to those sub-themes. The technique starts by constructing a matrix of d dimensional word embeddings of n words of a topic leading to a n × d rectangular matrix A. An SVD operation on A provides the orthonormal bases in U and V and singular values in S. We focus particularly on the V matrix, as that records the eigenvectors of ATA, which are in turn the orthogonal d dimensional directions we are interested in. This is intuitive as a d dimensional vector would represent some definite concept in the space of word embeddings and can become a distinct bucket identifier. Further, the V matrix, provides eigenvectors ordered according to most significant singular values first. We use the first n eigenvectors as bucket identifiers to attach words to. The attachment is simple - the word goes to the bucket represented by the word’s most similar eigenvector.Observation of buckets getting formed by TBuckets-SVD revealed that some words in spite of being similar/associated with words in the largest bucket, were being put to another bucket due to high similarity with that eigenvector. An example of such a topic is - {show, television, tv, news, network, medium, fox, cable, channel, series}. The output of TBuckets-SVD reveals the distribution - Bucket 1: {show, television, tv, network, cable, channel}; Bucket 2: {medium}; Bucket 3: {series}; Bucket 4: {news}; Bucket 5: {fox}. We can clearly observe that words news, fox and series belonging to other buckets should be a part of the largest bucket (Bucket 1). Many such similar examples were observed and hence the need for a reorganization step over TBuckets-SVD was desirable. Moreover, there were examples of topics where some words in the largest bucket were a better fit to other buckets than the current one and hence needed to be evicted for better coherence of the largest bucket. An example is a topic where TBuckets-SVD generates the largest bucket as {cocktail, glass, drink, girl}, where it is important to shift the word girl out from the bucket.
We collectively denote both the reorganization steps i.e. into and from the largest bucket, as the TBuckets-SVD-Reorg technique. The methodol-
ogy for automatic reorganization is based on four important parameters for each word:
• Native Bucket Pull - Words (NBPW): Maximum semantic similarity of the word with all other words in the word’s native bucket.
• Native Bucket Pull - EigenVector (NBPE): Similarity of the word with the eigenvector representing the word’s native bucket.
• Largest Bucket Pull - Words (LBPW): Maximum semantic similarity of the word with all words in the largest bucket.
• Other Bucket Pull - Words (OBPW): Maximum semantic similarity of the word with all words in a bucket other than the native bucket.
Now, it is intuitive that for a word in the nonlargest bucket, a condition where LBPW > NBPE, is a indication for carrying out the into step and for a word in the largest bucket, a condition where OBPW > max(NBPW, NBPE) is an indication for carrying out the from step. However, considering the into case, even a marginal difference (say of the order 0.01 or less) between LBPW and NBPE would lead to the shift. A similar argument holds for the from case. Hence, an additive threshold to the Right Hand Side (RHS) in both conditions would be necessary. We do not decide the threshold empirically, instead devise it based on the topic under consideration and hence removing possibility of any parameter tuning. The dynamic threshold depends on two values namely average word pair similarity (AWPS) among the topic words and average word eigenvector similarity (AWES). The former value is computed as an average of semantic similarity among all pairs of words in the topic. The latter value is obtained after the buckets are formed using TBuckets-SVD and the value is computed as an average of similarities between each word and its corresponding bucket’s eigenvector. The threshold is computed as AWES − AWPSAWES . The ratio in the threshold denotes flexibility among words for reorganization. A higher value of the ratio indicates better word-word similarities and lenient word-eigenvector similarities making reorganization meaningful. The subtraction from AWES manages the gap in the ranges of word pair similarities and word eigenvector similarities.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499For all techniques we consider the properties of words in the largest bucket to compute the coherence score. This scoring is in line with the important idea - more topic words belonging to a common theme signifies higher coherence. The coherence is computed based on various word and bucket properties like length of the largest bucket, function of word order in the topic, similarity to the eigenvector and most significant singular value.
Consider the largest bucket obtained from a topic, Bucket 1: {w0, · · ·wk−1}. A basic coherence score is the size of Bucket 1 which is k in this case. We also consider the order of words in a topic which is based on their generation probability. A weight is computed for each ith word in a topic as follows:
LDA Order Weight(wi) = 1
log(i+ 1) if i > 0
= 1 otherwise
Topic coherence score based on LDA word order, denoted by WL, is then computed as the sum of LDA Order Weight of each word in the largest bucket. For TBuckets-SVD and TBucketsSVD-Reorg, another topic coherence score, denoted as EVS, can be computed using each word’s similarity with its corresponding eigenvector. For reporting results for TBuckets-SVD and TBuckets-SVD-Reorg techniques, we weight the scores (size, WL and EVS) with the square of the highest singular value.We evaluate TBuckets on 3 datasets - 20 NewsGroups (20NG), New York Times (NYT) and Genomics. Each dataset consists of a set of 100 topics where each topic is represented by its 10 most probable words. Moreover, each topic is associated with a real number between 1 and 3 indicating human judgement of its coherence. Detailed description of these datasets is provided in Roder et. al (2015).
For all our experiments, we use the 300 dimensional pre-trained word embeddings provided by the GloVe framework 1. These embeddings have
1http://nlp.stanford.edu/projects/ glove/
been trained on Wikipedia and Gigaword corpora.Topic models have been used for text classification with weak supervision in several previous approaches like Hingmire et al. (2013), Hingmire and Chakraborti (2014) and Pawar et al. (2016). The core idea used in these approaches is that instead of obtaining gold labels for documents, the human annotators can provide labels for the learned topics. This reduces the labelling effort drastically as it was reported that a small number of topics (usually twice the number of class labels) was sufficient to achieve good classification performance.
As learning topics from a text corpus is based on approximate inference, different topics are generated when the process of learning topics is run
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Setting/Scoring Metric NYT 20NG Genomics Roder et al. (2015) Best performing settings 0.806 0.859 0.773
TBuckets-Clustering
Single-linkage, size 0.779 (0.63) 0.832 (0.7) 0.774 (0.67) Single-linkage, WL 0.766 (0.63) 0.848 (0.72) 0.751 (0.69) Average-linkage, size 0.745 (0.81) 0.856 (0.79) 0.709 (0.77) Average-linkage, WL 0.73 (0.81) 0.863 (0.79) 0.721 (0.86) Complete-linkage, size 0.709 (0.91) 0.806 (0.86) 0.581 (0.81) Complete-linkage, WL 0.702 (0.91) 0.835 (0.86) 0.567 (0.81)
TBuckets-SVD
size 0.758 0.867 0.698 WL 0.772 0.868 0.694 EVS 0.762 0.854 0.693 WL*EVS 0.772 0.858 0.69
TBuckets-SVD-Reorg size 0.837 0.863 0.68 WL 0.837 0.866 0.685 EVS 0.828 0.868 0.687 WL*EVS 0.833 0.869 0.691
size: Size of the largest bucket WL: Word order based on generation probability from LDA
EVS: Eigenvector similarity
Table 1: Comparative performance of various techniques in terms of Pearson’s r correlation co-efficient
Figure 1: Effect of varying Max Dist for TBuckets-Clustering (size)
Dataset Buckets TBuckets-SVD-Reorg Human
20NG
Bucket 1: { gun, crime, firearm, weapon, handgun, law, criminal, control} Bucket 2: { rate} Bucket 3: { 000}
0.542 2.3125
20NG
Bucket 1: { convex} Bucket 2: { oracle, opinion, expressed} Bucket 3: { user} Bucket 4: { princeton, tamu} Bucket 5: { corporation} Bucket 6: { phil} Bucket 7: { phoenix}
0.028 1.23
NYT Bucket 1: { show, television, tv, network, cable, channel, series, news, fox} Bucket 2: { medium}
0.796 2.82
NYT
Bucket 1: { portugal, portuguese} Bucket 2: { dinosaur, fossil} Bucket 3: { apple} Bucket 4: { ant} Bucket 5: { rent, peru} Bucket 6: { sherman} Bucket 7: { evans}
0.071 1.25
TBuckets-SVD-Reorg coherence scores are normalized to lie between 0 and 1 Human assigned coherence scores lie between 1 and 3
Table 2: Examples of buckets created using TBuckets-SVD-Reorg
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
multiple times. Hence, the process of learning topics is repeated several times and the average of classification accuracies obtained over these runs is considered as the final accuracy. Labels by human annotators have to be obtained for the set of topics generated in each of these runs. As reported in (Pawar et al., 2016) the quality of topics can be different for various runs. Low quality topics are difficult for human annotators to annotate and incorrect labels by them results in poor classification performance. To overcome this problem, we propose to evaluate quality of topics using TBuckets and present only high quality topics for human annotation. For computing quality of set of topics, we simply add the coherence scores of individual topics in the set. As all the set of topics have same number of topics, these scores are comparable.
Here, we consider 4 subsets (PC vs MAC, MEDICAL vs SPACE, POLITICS vs SCIENCE and POLITICS vs RELIGION) of 20NG dataset for the experiments involving text classification using topic labelling. We compare the performance of LPA-TD classifier used in (Pawar et al., 2016) using two different strategies for generating and labelling topics (Table 3):
S1 (Without considering quality of topics) : Topic learning process is repeated for 5 times and human annotations were obtained. Reported classification accuracies are average of accuracies for all 5 runs.
S2 (Considering quality of topics) : Topic learning process is repeated for 10 times and quality of each set of topics was measured using TBuckets-SVDReorg. But human annotations were obtained only for the best 5 sets of topics. Reported classification accuracies are average of accuracies for the corresponding 5 runs with best coherence scores. To emphasize usefulness of our coherence scores, we also obtain human annotations for remaining 5 sets of topics with lowest coherence scores and report average classification performance for them also.
It can be observed from Table 3 that strategy S2 produces better accuracies than S1 in 3 out of 4 datasets. Also, except for the MED-SPACE dataset, there is wide difference in the accuracies in S2 when the highest 5 sets of topics as per coherence are considered as against the lowest 5 sets. This can be attributed to the fact that MED-SPACE is considered to be one of the easiest for classification amongst the 20NG datasets and hence coherent topics are generated more often than not.
Dataset S1 S2
5 Highest 5 Lowest Coh. sets Coh. sets
PC vs MAC 66.98 68.47 67.96 MEDICAL vs SPACE 94.67 94.83 95.37 POLITICS vs SCIENCE 95.95 96.19 91.09 POLITICS vs RELIGION 85.61 85.23 82.85
Table 3: Text Classification performance (macroF1) with and without considering quality of topics
For other datasets, especially PC-MAC which is considered one of the most difficult amongst the 20NG datasets, the large difference in accuracies (Highest 5 coherence sets vs Lowest 5 coherence sets) underlines the importance of measuring topic coherence before obtaining human annotations.","We use the same evaluation scheme used in (Röder et al., 2015). Each technique generates coherence scores for all the topics in a dataset. Pearson’s r correlation co-efficient is computed between the coherence scores based on human judgement and the coherence scores automatically generated by the technique. Higher the correlation with human scores, better is the performance of the technique at measuring coherence.
Table 1 shows the Pearson’s r values for our techniques - TBuckets-Clustering, TBuckets-SVD and TBuckets-SVD-Reorg. We compare their performance with the best performing correlation values as reported by Roder et al. (2015).
As observed in Table 1, our technique TBuckets-SVD-Reorg outperforms the state-ofthe-art on 2 out of 3 datasets, namely NYT and 20NG. This is significant considering the fact that both SVD based techniques are completely parameter less whereas the state-of-the-art requires considerable tuning of multiple parameters. This also is a sound validation of the TBuckets idea for measuring topic coherence. TBuckets-Clustering also performs at par with the state-of-the-art on the Genomics dataset. It requires only one parameter, namely Max Dist which is maximum allowable distance for merging in Agglomerative clustering. Figure 1 shows the variation in the performance of TBuckets-Clustering for various values of Max Dist. Table 2 shows some examples of topics and the buckets which were created for them using the TBuckets-SVD-Reorg technique.","Starting with the formalization of the notion of a topic as a probability distribution over words, probabilistic graphical models have been widely investigated for inferring the set of topics present in a document collection in an unsupervised manner (Blei et al., 2003). These models also infer the probability distribution over topics for documents in the collection. Since topics give a particular perspective on the structure of the document collection, topic modelling techniques have been applied on a variety of real-life document collections, such as scientific papers (Griffiths and Steyvers, 2004), (Blei, 2012) and newspapers archives (Yang et al., 2011). Topic models have also been used for im-
proving many traditional text-mining tasks, such as document classification (Hingmire et al., 2013), document summarization (Wang et al., 2009), sentiment analysis (Lin and He, 2009), word sense disambiguation (Boyd-Graber et al., 2007), corpus visualization (Newman et al., 2010a) etc. Several variations on topic models are also being researched; e.g., correlated topic models (e.g., a document having a topic environment is likely to include topics such as UN and politics but not sports) (David M. Blei, 2007), dynamic topic models evolving over time (Blei and Lafferty, 2006), (Wang et al., 2008) and supervised topic models (Mcauliffe and Blei, 2008), (Ramage et al., 2009).
Given this growing importance of topic modelling in text mining techniques and in practical applications, it is crucial to ensure that the inferred topics are of as high quality as possible. An attractive feature of the probabilistic topic models is that the inferred topics can be easily interpreted by humans, each topic being just a bag of probabilistically selected “prominent” words in that topic’s distribution. This has opened up a research area which explores using human expertise or designing automated techniques to measure the quality of topics and improve the topic modelling techniques by incorporating these measures. As an example, consider the following two topics inferred from a document collection: {loan, foreclosure, mortgage, home, property, lender, housing, bank, homeowner, claim} {horse, sullivan, business, secretariat, owner, get, truck, back, old, merchant}
The first topic is easily interpretable by humans whereas the second topic is much less coherent and hence less understandable.
One could evaluate a single topic or an entire set of topics (“topic model”) for quality. Several different approaches have been proposed in the
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
literature for measuring the quality of a particular topic or that of an entire topic model: word and topic intrusions (Chang et al., 2009), analysis of the topic word probability distributions (AlSumait et al., 2009), average pointwise mutual information (PMI) between topic words (Newman et al., 2010b), co-document frequencies of the topic words (Mimno et al., 2011), coverage and specificities of WordNet hierarchies for words in a topic (Musat et al., 2011), distributional semantics (distances between vectors for words in a topic) (Aletras and Stevenson, 2013), among many others.
In this paper, we propose a novel approach TBuckets which groups topic words into thematic groups (which we call buckets). The intuition is that if a single large bucket is obtained from a topic, then the topic carries a single coherent theme. Under TBuckets, we explore three techniques for creating buckets of words - i) clustering based, ii) using singular value decomposition (SVD) and iii) SVD with reorganization. We evaluate our techniques by correlating their estimated coherence scores with human annotated scores and compare with state-of-the-art results reported in Roder et al. (2015). The TBuckets approach not only outperforms the state-of-the-art, but its SVD-based techniques carry merit for being completely parameter free.
This rest of the paper is organized as follows. Section 2 briefly discusses the necessary background. Sections 3 describes the TBuckets approach in detail. Section 4 gives experimental evaluation of our techniques. Section 5 discusses the relevant related work and section 6 concludes with a discussion on future work.","LDA uses statistical relations between words like word co-occurrence while inferring topics and not semantic relations. Hence, topics inferred by LDA may not correlate well with human judgements even though they better optimize perplexity on held-out documents (Chang et al., 2009). (Chang et al., 2009) emphasize that quality of topics should depend on their human interpretability rather than purely statistical measures like perplexity.
Several authors (e.g. (Newman et al., 2010b; Mimno et al., 2011)) hypothesize that coherence of the most N probable words of a topic capture its semantic interpretability and proposed measures to estimate coherence of topics. (Newman et al., 2010b) used the set of N most probable words of a topic and computed its coherence (CUCI ) based on pointwise mutual information (PMI) between all possible word pairs of N words. CUCI of a topic t is computed as:
CUCI(t) = 2
N(N − 1) N−1∑ i=1 N∑ j=i+1 PMI(wi, wj)
where,
PMI(wi, wj) = log P (wi, wj)
P (wi)P (wj)
Where, P (wi, wj) is estimated based on the number of times words wi and wj co-occur in a sliding window of size 10 that moves over all the articles in Wikipedia. (Lau et al., 2014) propose a variant
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
of CUCI by using normalized PMI (NPMI) instead of PMI.
(Mimno et al., 2011) propose similar topic coherence measure (CUMASS) that uses log conditional probability (LCP) instead of PMI and uses the same corpus on which topics are inferred, to estimate LCP rather than Wikipedia. CUMASS for a topic t is computed as:
CUMASS(t) = 2
N(N − 1) N∑ i=2 i−1∑ j=1 log P (wi, wj) + 1 P (wj)
(Aletras and Stevenson, 2013) propose a topic coherence measure based on distributional similarity between the most N probable words of the topic. In this approach, a topic word (wi) is represented as a context vector (~vi) over the words that co-occur with wi in Wikipedia in a window of ±5 words, such that vi,j represents PMI (or NPMI) between words wi and wj . The word vectors of topic words are then used to find coherence (CUSheffield) of a topic (t) as follows:
CUSheffield(t) =
∑N i=1 sim(TC , ~vi)
N
where, TC represents the centroid of the word vectors of the topic and sim(~u,~v) is cosine similarity between vectors ~u and ~v.
(Aletras and Stevenson, 2013) observed that CUCI with NPMI correlates well with human judgements than CUMASS and CUCI with PMI.
(Aletras and Stevenson, 2013) also propose an alternative to CUSheffield where they represent a topic word wi as a context vector over the space of topic words only. They observed that CUSheffield with topic words only outperforms CUCI with NPMI.
Roder et al. (2015) propose a unifying framework that represents a coherence measure as a composition of parts, that can be freely combined to form a configuration space of coherence definitions. These parts can be grouped into four dimensions: 1) first dimension defines number of ways a word set can be divided into smaller pieces, 2) second dimension defines confirmation measures like PMI or NPMI to measure the agreement of a given word pair, 3) third dimension defines different ways to estimate word probabilities (P (wi) and P (wi, wj)), 4) fourth dimension defines methods to aggregate scalar values computed by the
confirmation measure. This framework spans over a large number of configuration space of coherence measures, hence it becomes computationally expensive to find appropriate coherence measure for a set of topics.",,"We proposed a novel approach TBuckets to measure quality of Latent Dirichlet Allocation (LDA) based topics, based on grouping of topic words into buckets. TBuckets uses 3 different techniques for creating buckets of words - TBucketsClustering which performs agglomerative clustering of words, TBuckets-SVD which uses singular value decomposition (SVD) to discover important sub-themes in topic words and lastly TBucketsSVD-Reorg which reorganized buckets obtained from TBuckets-SVD. We evaluated our techniques on three publicly available datasets by correlating the estimated coherence scores with human annotated scores and demonstrated better performance than the state-of-the-art results. Moreover, as compared to the state-of-the-art technique which needs to tune multiple parameters, our techniques TBuckets-SVD and TBuckets-SVD-Reorg require absolutely no parameter tuning. We also highlighted the utility of TBuckets for the task of weakly supervised text classification.
In future, we plan to devise better ways to compute word similarities which would be more suitable for specific domains like Genomics. Furthermore, we plan to devise a unified framework to evaluate and employ various scoring metrics. Also, we wish to explore usefulness of our techniques for applications other than text classification."
17,"Measuring topic quality is essential for scoring the learned topics and their subsequent use in Information Retrieval and Text classification. To measure quality of Latent Dirichlet Allocation (LDA) based topics learned from text, we propose a novel approach based on grouping of topic words into buckets (TBuckets). A single large bucket signifies a single coherent theme, in turn indicating high topic coherence. TBuckets represents topic words using their word embeddings and employs 3 different techniques for creating buckets of words - i) clustering based, ii) using singular value decomposition (SVD) and iii) SVD with reorganization. The TBuckets approach outperforms the state-of-the-art techniques when evaluated using three publicly available datasets. Further, we demonstrate the usefulness of TBuckets for the task of weakly supervised text classification.",Measuring Topic Quality using Word Buckets,103,"This paper proposes a new method for the evaluation of topic models that
partitions the top n words of each topic into clusters or ""buckets"" based on
cosine similarity of their associated word embeddings. In the simplest setup,
the words are considered one by one, and each is either put into an existing
""bucket"" â if its cosine similarity to the other words in the bucket is below
a certain threshold â or a new bucket is created for the word. Two more
complicated methods based on eigenvectors and reorganisation are also
suggested. The method is evaluated on three standard data sets and in a  weakly
supervised text classification setting. It outperforms or is en par with the
state of the art (RÃ¶der et al., 2015).

The basic idea behind the paper is rather simple and has a certain ad
hoc-flavour. The authors do not offer any new explanations for why topic
quality should be measurable in terms of wordâword similarity. It is not
obvious to me why this should be so, given that topics and word embeddings are
defined with respect to two rather different notions of context (document vs.
sequential context). At the same time, the proposed method seems to work quite
well. (I would like to see some significance tests for Table 1 though.)

Overall the paper is clearly written, even though there are some language
issues. Also, I found the description of the techniques in Section 3 a bit hard
to follow; I believe that this is mostly due to the authors using passive voice
(""the threshold is computed as"") in places were they were actually making a
design choice. I find that the authors should try to explain the different
methods more clearly, with one subsection per method. There seems to be some
space for that: The authors did not completely fill the 8 pages of content, and
they could easily downsize the rather uninformative ""trace"" of the method on
page 3.

One question that I had was how sensitive the proposed technique was to
different word embeddings. For example, how would the scores be if the authors
had used word2vec instead of GloVe?",,3,4,Oral Presentation,4,4,3,3,4,5,3,3,2016,"Word embeddings are vector representations of words which have become popular recently. Some efficient approaches to learn these vector representations from large unlabeled corpora are by Mikolov et al. (2013) and Pennington et al. (2014). Each word is mapped to a real-valued vector in d dimensions, such that vectors of semantically similar words lie close to each other. The cosine similarity of word vectors in this space is hence, a good estimation of semantic similarity between the corresponding words.Through SVD, a rectangular matrix A can be factorized to three components U , S and V where the matrix U contains eigenvectors of AAT , the matrix V contains the eigenvectors of ATA and the matrix S is a diagonal matrix containing the singular values of A, which are effectively square roots of eigenvalues of AAT and ATA. Intuitively, the SVD of a rectangular matrix, allows to express the matrix as a combination of three geometrical operations - rotation through U , scaling through S and another rotation through V . More clearly, for an m × n matrix A, where m entities are represented by their n features, the U matrix helps to identify important dimensions among the entities and the V matrix does so for important dimensions among the entities in terms of their features. In the paper, we focus particularly on the V matrix allowing us to obtain eigenvectors of the matrix ATA, which captures feature-feature interactions of the m entities.The TBuckets idea is based on how we humans generally observe a topic and conclude on its coherence. Assuming a topic with words ordered (descending) by their probability of getting generated from the topic, the general procedure involves observing the topic words one by one and putting them in some form of logical groups (or buckets, as we call them). Starting with a fresh bucket for the first word, every new word is put in an already created bucket if the word is semantically similar or semantically associated with the words in the bucket; otherwise the word is put in a new bucket. On completion of this exercise, all topic words would be distributed in various buckets. A distribution with a single large bucket and few small buckets would signify better coherence. On the other hand, a distribution with multiple medium sized buckets would indicate lower coherence.
To see an example dry run of the above procedure, lets consider a coherent and a non-coherent topic. The topic {storm, weather, wind, temperature, rain, snow, air, high, cold, northern} is quite coherent and deals with weather and its factors. The bucket procedure for this topic executes as follows:
1. Word Seen: storm. Bucket-1: {storm}
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
2. Word Seen: weather. Bucket-1: {storm, weather}
3. Word Seen: wind. Bucket-1: {storm, weather, wind}
4. Word Seen: temperature. Bucket-1: {storm, weather, wind, temperature}
5. Word Seen: rain. Bucket-1: {storm, weather, wind, temperature, rain}.
6. Word Seen: snow. Bucket-1: {storm, weather, wind, temperature, rain, snow}.
7. Word Seen: air. Bucket-1: {storm, weather, wind, temperature, rain, snow, air}.
8. Word Seen: high. Bucket-1: {storm, weather, wind, temperature, rain, snow, air}; Bucket-2: {high}
9. Word Seen: cold. Bucket-1: {storm, weather, wind, temperature, rain, snow, air, cold}; Bucket-2: {high}
10. Word Seen: northern. Bucket-1: {storm, weather, wind, temperature, rain, snow, air, cold}; Bucket-2: {high}; Bucket-3: {northern}
Another topic {karzai, afghan, miner, official, mine, assange, government, kabul, afghanistan, wikileaks} is not coherent and deals with multiple areas like afghanistan, wikileaks and mining. The bucket procedure for this topic executes as follows:
1. Word Seen: karzai. Bucket-1: {karzai}
2. Word Seen: afghan. Bucket-1: {karzai, afghan}
3. Word Seen: miner. Bucket-1: {karzai, afghan}; Bucket-2: {miner}
4. Word Seen: official. Bucket-1: {storm, weather}; Bucket-2: {miner}; Bucket-3: {official}
5. Word Seen: mine. Bucket-1: {karzai, afghan}; Bucket-2: {miner, mine}; Bucket-3: {official}
6. Word Seen: assange. Bucket-1: {karzai, afghan}; Bucket-2: {miner, mine}; Bucket-3: {official}; Bucket-4: {assange}
7. Word Seen: government. Bucket-1: {karzai, afghan}; Bucket-2: {miner, mine}; Bucket-3: {official, government}; Bucket-4: {assange}
8. Word Seen: kabul. Bucket-1: {karzai, afghan, kabul}; Bucket-2: {miner, mine}; Bucket-3: {official, government}; Bucket-4: {assange}
9. Word Seen: afghanistan. Bucket-1: {karzai, afghan, kabul, afghanistan}; Bucket-2: {miner, mine}; Bucket-3: {official, government}; Bucket-4: {assange}
10. Word Seen: wikileaks. Bucket-1: {karzai, afghan, kabul, afghanistan}; Bucket-2: {miner, mine}; Bucket-3: {official, government}; Bucket-4: {assange, wikileaks}
It is evident from the above iterations that final distributions of topic words into buckets, reflects the coherence of a topic quite closely. Based on this idea, we devise approaches to carry out topic word distribution into buckets, automatically. We consider properties of the finally formed buckets to compute a coherence score for a topic.
The TBuckets approach powers us to do this bucketing automatically and generates a coherence score for a topic. It mainly requires two resources namely the word embeddings of topic words and the topic model (i.e. complete set of topics with words ordered according to their generation probability). These resources are not difficult to obtain as the topic model is available de facto and word embeddings of a large set of words, trained on various corpora, are now available publicly. The approach can be characterized through three techniques.
The first technique (TBuckets-Clustering) is based on a the idea of clustering which arises intuitively when we think of forming related groups among a set of items (words here). The TBucketsClustering technique involves representing words by their word embeddings and clustering them using agglomerative clustering. We use a maximum distance threshold as an input to the clustering for facilitating the agglomeration. Further we try with all single, average and complete linkage configurations. The technique proves to be competent involving only a single parameter.
The notion for the SVD-based technique (TBuckets-SVD) is to find the various orthogonal
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
dimensions or sub-themes inside a topic and attach words to those sub-themes. The technique starts by constructing a matrix of d dimensional word embeddings of n words of a topic leading to a n × d rectangular matrix A. An SVD operation on A provides the orthonormal bases in U and V and singular values in S. We focus particularly on the V matrix, as that records the eigenvectors of ATA, which are in turn the orthogonal d dimensional directions we are interested in. This is intuitive as a d dimensional vector would represent some definite concept in the space of word embeddings and can become a distinct bucket identifier. Further, the V matrix, provides eigenvectors ordered according to most significant singular values first. We use the first n eigenvectors as bucket identifiers to attach words to. The attachment is simple - the word goes to the bucket represented by the word’s most similar eigenvector.Observation of buckets getting formed by TBuckets-SVD revealed that some words in spite of being similar/associated with words in the largest bucket, were being put to another bucket due to high similarity with that eigenvector. An example of such a topic is - {show, television, tv, news, network, medium, fox, cable, channel, series}. The output of TBuckets-SVD reveals the distribution - Bucket 1: {show, television, tv, network, cable, channel}; Bucket 2: {medium}; Bucket 3: {series}; Bucket 4: {news}; Bucket 5: {fox}. We can clearly observe that words news, fox and series belonging to other buckets should be a part of the largest bucket (Bucket 1). Many such similar examples were observed and hence the need for a reorganization step over TBuckets-SVD was desirable. Moreover, there were examples of topics where some words in the largest bucket were a better fit to other buckets than the current one and hence needed to be evicted for better coherence of the largest bucket. An example is a topic where TBuckets-SVD generates the largest bucket as {cocktail, glass, drink, girl}, where it is important to shift the word girl out from the bucket.
We collectively denote both the reorganization steps i.e. into and from the largest bucket, as the TBuckets-SVD-Reorg technique. The methodol-
ogy for automatic reorganization is based on four important parameters for each word:
• Native Bucket Pull - Words (NBPW): Maximum semantic similarity of the word with all other words in the word’s native bucket.
• Native Bucket Pull - EigenVector (NBPE): Similarity of the word with the eigenvector representing the word’s native bucket.
• Largest Bucket Pull - Words (LBPW): Maximum semantic similarity of the word with all words in the largest bucket.
• Other Bucket Pull - Words (OBPW): Maximum semantic similarity of the word with all words in a bucket other than the native bucket.
Now, it is intuitive that for a word in the nonlargest bucket, a condition where LBPW > NBPE, is a indication for carrying out the into step and for a word in the largest bucket, a condition where OBPW > max(NBPW, NBPE) is an indication for carrying out the from step. However, considering the into case, even a marginal difference (say of the order 0.01 or less) between LBPW and NBPE would lead to the shift. A similar argument holds for the from case. Hence, an additive threshold to the Right Hand Side (RHS) in both conditions would be necessary. We do not decide the threshold empirically, instead devise it based on the topic under consideration and hence removing possibility of any parameter tuning. The dynamic threshold depends on two values namely average word pair similarity (AWPS) among the topic words and average word eigenvector similarity (AWES). The former value is computed as an average of semantic similarity among all pairs of words in the topic. The latter value is obtained after the buckets are formed using TBuckets-SVD and the value is computed as an average of similarities between each word and its corresponding bucket’s eigenvector. The threshold is computed as AWES − AWPSAWES . The ratio in the threshold denotes flexibility among words for reorganization. A higher value of the ratio indicates better word-word similarities and lenient word-eigenvector similarities making reorganization meaningful. The subtraction from AWES manages the gap in the ranges of word pair similarities and word eigenvector similarities.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499For all techniques we consider the properties of words in the largest bucket to compute the coherence score. This scoring is in line with the important idea - more topic words belonging to a common theme signifies higher coherence. The coherence is computed based on various word and bucket properties like length of the largest bucket, function of word order in the topic, similarity to the eigenvector and most significant singular value.
Consider the largest bucket obtained from a topic, Bucket 1: {w0, · · ·wk−1}. A basic coherence score is the size of Bucket 1 which is k in this case. We also consider the order of words in a topic which is based on their generation probability. A weight is computed for each ith word in a topic as follows:
LDA Order Weight(wi) = 1
log(i+ 1) if i > 0
= 1 otherwise
Topic coherence score based on LDA word order, denoted by WL, is then computed as the sum of LDA Order Weight of each word in the largest bucket. For TBuckets-SVD and TBucketsSVD-Reorg, another topic coherence score, denoted as EVS, can be computed using each word’s similarity with its corresponding eigenvector. For reporting results for TBuckets-SVD and TBuckets-SVD-Reorg techniques, we weight the scores (size, WL and EVS) with the square of the highest singular value.We evaluate TBuckets on 3 datasets - 20 NewsGroups (20NG), New York Times (NYT) and Genomics. Each dataset consists of a set of 100 topics where each topic is represented by its 10 most probable words. Moreover, each topic is associated with a real number between 1 and 3 indicating human judgement of its coherence. Detailed description of these datasets is provided in Roder et. al (2015).
For all our experiments, we use the 300 dimensional pre-trained word embeddings provided by the GloVe framework 1. These embeddings have
1http://nlp.stanford.edu/projects/ glove/
been trained on Wikipedia and Gigaword corpora.Topic models have been used for text classification with weak supervision in several previous approaches like Hingmire et al. (2013), Hingmire and Chakraborti (2014) and Pawar et al. (2016). The core idea used in these approaches is that instead of obtaining gold labels for documents, the human annotators can provide labels for the learned topics. This reduces the labelling effort drastically as it was reported that a small number of topics (usually twice the number of class labels) was sufficient to achieve good classification performance.
As learning topics from a text corpus is based on approximate inference, different topics are generated when the process of learning topics is run
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Setting/Scoring Metric NYT 20NG Genomics Roder et al. (2015) Best performing settings 0.806 0.859 0.773
TBuckets-Clustering
Single-linkage, size 0.779 (0.63) 0.832 (0.7) 0.774 (0.67) Single-linkage, WL 0.766 (0.63) 0.848 (0.72) 0.751 (0.69) Average-linkage, size 0.745 (0.81) 0.856 (0.79) 0.709 (0.77) Average-linkage, WL 0.73 (0.81) 0.863 (0.79) 0.721 (0.86) Complete-linkage, size 0.709 (0.91) 0.806 (0.86) 0.581 (0.81) Complete-linkage, WL 0.702 (0.91) 0.835 (0.86) 0.567 (0.81)
TBuckets-SVD
size 0.758 0.867 0.698 WL 0.772 0.868 0.694 EVS 0.762 0.854 0.693 WL*EVS 0.772 0.858 0.69
TBuckets-SVD-Reorg size 0.837 0.863 0.68 WL 0.837 0.866 0.685 EVS 0.828 0.868 0.687 WL*EVS 0.833 0.869 0.691
size: Size of the largest bucket WL: Word order based on generation probability from LDA
EVS: Eigenvector similarity
Table 1: Comparative performance of various techniques in terms of Pearson’s r correlation co-efficient
Figure 1: Effect of varying Max Dist for TBuckets-Clustering (size)
Dataset Buckets TBuckets-SVD-Reorg Human
20NG
Bucket 1: { gun, crime, firearm, weapon, handgun, law, criminal, control} Bucket 2: { rate} Bucket 3: { 000}
0.542 2.3125
20NG
Bucket 1: { convex} Bucket 2: { oracle, opinion, expressed} Bucket 3: { user} Bucket 4: { princeton, tamu} Bucket 5: { corporation} Bucket 6: { phil} Bucket 7: { phoenix}
0.028 1.23
NYT Bucket 1: { show, television, tv, network, cable, channel, series, news, fox} Bucket 2: { medium}
0.796 2.82
NYT
Bucket 1: { portugal, portuguese} Bucket 2: { dinosaur, fossil} Bucket 3: { apple} Bucket 4: { ant} Bucket 5: { rent, peru} Bucket 6: { sherman} Bucket 7: { evans}
0.071 1.25
TBuckets-SVD-Reorg coherence scores are normalized to lie between 0 and 1 Human assigned coherence scores lie between 1 and 3
Table 2: Examples of buckets created using TBuckets-SVD-Reorg
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
multiple times. Hence, the process of learning topics is repeated several times and the average of classification accuracies obtained over these runs is considered as the final accuracy. Labels by human annotators have to be obtained for the set of topics generated in each of these runs. As reported in (Pawar et al., 2016) the quality of topics can be different for various runs. Low quality topics are difficult for human annotators to annotate and incorrect labels by them results in poor classification performance. To overcome this problem, we propose to evaluate quality of topics using TBuckets and present only high quality topics for human annotation. For computing quality of set of topics, we simply add the coherence scores of individual topics in the set. As all the set of topics have same number of topics, these scores are comparable.
Here, we consider 4 subsets (PC vs MAC, MEDICAL vs SPACE, POLITICS vs SCIENCE and POLITICS vs RELIGION) of 20NG dataset for the experiments involving text classification using topic labelling. We compare the performance of LPA-TD classifier used in (Pawar et al., 2016) using two different strategies for generating and labelling topics (Table 3):
S1 (Without considering quality of topics) : Topic learning process is repeated for 5 times and human annotations were obtained. Reported classification accuracies are average of accuracies for all 5 runs.
S2 (Considering quality of topics) : Topic learning process is repeated for 10 times and quality of each set of topics was measured using TBuckets-SVDReorg. But human annotations were obtained only for the best 5 sets of topics. Reported classification accuracies are average of accuracies for the corresponding 5 runs with best coherence scores. To emphasize usefulness of our coherence scores, we also obtain human annotations for remaining 5 sets of topics with lowest coherence scores and report average classification performance for them also.
It can be observed from Table 3 that strategy S2 produces better accuracies than S1 in 3 out of 4 datasets. Also, except for the MED-SPACE dataset, there is wide difference in the accuracies in S2 when the highest 5 sets of topics as per coherence are considered as against the lowest 5 sets. This can be attributed to the fact that MED-SPACE is considered to be one of the easiest for classification amongst the 20NG datasets and hence coherent topics are generated more often than not.
Dataset S1 S2
5 Highest 5 Lowest Coh. sets Coh. sets
PC vs MAC 66.98 68.47 67.96 MEDICAL vs SPACE 94.67 94.83 95.37 POLITICS vs SCIENCE 95.95 96.19 91.09 POLITICS vs RELIGION 85.61 85.23 82.85
Table 3: Text Classification performance (macroF1) with and without considering quality of topics
For other datasets, especially PC-MAC which is considered one of the most difficult amongst the 20NG datasets, the large difference in accuracies (Highest 5 coherence sets vs Lowest 5 coherence sets) underlines the importance of measuring topic coherence before obtaining human annotations.","We use the same evaluation scheme used in (Röder et al., 2015). Each technique generates coherence scores for all the topics in a dataset. Pearson’s r correlation co-efficient is computed between the coherence scores based on human judgement and the coherence scores automatically generated by the technique. Higher the correlation with human scores, better is the performance of the technique at measuring coherence.
Table 1 shows the Pearson’s r values for our techniques - TBuckets-Clustering, TBuckets-SVD and TBuckets-SVD-Reorg. We compare their performance with the best performing correlation values as reported by Roder et al. (2015).
As observed in Table 1, our technique TBuckets-SVD-Reorg outperforms the state-ofthe-art on 2 out of 3 datasets, namely NYT and 20NG. This is significant considering the fact that both SVD based techniques are completely parameter less whereas the state-of-the-art requires considerable tuning of multiple parameters. This also is a sound validation of the TBuckets idea for measuring topic coherence. TBuckets-Clustering also performs at par with the state-of-the-art on the Genomics dataset. It requires only one parameter, namely Max Dist which is maximum allowable distance for merging in Agglomerative clustering. Figure 1 shows the variation in the performance of TBuckets-Clustering for various values of Max Dist. Table 2 shows some examples of topics and the buckets which were created for them using the TBuckets-SVD-Reorg technique.","Starting with the formalization of the notion of a topic as a probability distribution over words, probabilistic graphical models have been widely investigated for inferring the set of topics present in a document collection in an unsupervised manner (Blei et al., 2003). These models also infer the probability distribution over topics for documents in the collection. Since topics give a particular perspective on the structure of the document collection, topic modelling techniques have been applied on a variety of real-life document collections, such as scientific papers (Griffiths and Steyvers, 2004), (Blei, 2012) and newspapers archives (Yang et al., 2011). Topic models have also been used for im-
proving many traditional text-mining tasks, such as document classification (Hingmire et al., 2013), document summarization (Wang et al., 2009), sentiment analysis (Lin and He, 2009), word sense disambiguation (Boyd-Graber et al., 2007), corpus visualization (Newman et al., 2010a) etc. Several variations on topic models are also being researched; e.g., correlated topic models (e.g., a document having a topic environment is likely to include topics such as UN and politics but not sports) (David M. Blei, 2007), dynamic topic models evolving over time (Blei and Lafferty, 2006), (Wang et al., 2008) and supervised topic models (Mcauliffe and Blei, 2008), (Ramage et al., 2009).
Given this growing importance of topic modelling in text mining techniques and in practical applications, it is crucial to ensure that the inferred topics are of as high quality as possible. An attractive feature of the probabilistic topic models is that the inferred topics can be easily interpreted by humans, each topic being just a bag of probabilistically selected “prominent” words in that topic’s distribution. This has opened up a research area which explores using human expertise or designing automated techniques to measure the quality of topics and improve the topic modelling techniques by incorporating these measures. As an example, consider the following two topics inferred from a document collection: {loan, foreclosure, mortgage, home, property, lender, housing, bank, homeowner, claim} {horse, sullivan, business, secretariat, owner, get, truck, back, old, merchant}
The first topic is easily interpretable by humans whereas the second topic is much less coherent and hence less understandable.
One could evaluate a single topic or an entire set of topics (“topic model”) for quality. Several different approaches have been proposed in the
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
literature for measuring the quality of a particular topic or that of an entire topic model: word and topic intrusions (Chang et al., 2009), analysis of the topic word probability distributions (AlSumait et al., 2009), average pointwise mutual information (PMI) between topic words (Newman et al., 2010b), co-document frequencies of the topic words (Mimno et al., 2011), coverage and specificities of WordNet hierarchies for words in a topic (Musat et al., 2011), distributional semantics (distances between vectors for words in a topic) (Aletras and Stevenson, 2013), among many others.
In this paper, we propose a novel approach TBuckets which groups topic words into thematic groups (which we call buckets). The intuition is that if a single large bucket is obtained from a topic, then the topic carries a single coherent theme. Under TBuckets, we explore three techniques for creating buckets of words - i) clustering based, ii) using singular value decomposition (SVD) and iii) SVD with reorganization. We evaluate our techniques by correlating their estimated coherence scores with human annotated scores and compare with state-of-the-art results reported in Roder et al. (2015). The TBuckets approach not only outperforms the state-of-the-art, but its SVD-based techniques carry merit for being completely parameter free.
This rest of the paper is organized as follows. Section 2 briefly discusses the necessary background. Sections 3 describes the TBuckets approach in detail. Section 4 gives experimental evaluation of our techniques. Section 5 discusses the relevant related work and section 6 concludes with a discussion on future work.","LDA uses statistical relations between words like word co-occurrence while inferring topics and not semantic relations. Hence, topics inferred by LDA may not correlate well with human judgements even though they better optimize perplexity on held-out documents (Chang et al., 2009). (Chang et al., 2009) emphasize that quality of topics should depend on their human interpretability rather than purely statistical measures like perplexity.
Several authors (e.g. (Newman et al., 2010b; Mimno et al., 2011)) hypothesize that coherence of the most N probable words of a topic capture its semantic interpretability and proposed measures to estimate coherence of topics. (Newman et al., 2010b) used the set of N most probable words of a topic and computed its coherence (CUCI ) based on pointwise mutual information (PMI) between all possible word pairs of N words. CUCI of a topic t is computed as:
CUCI(t) = 2
N(N − 1) N−1∑ i=1 N∑ j=i+1 PMI(wi, wj)
where,
PMI(wi, wj) = log P (wi, wj)
P (wi)P (wj)
Where, P (wi, wj) is estimated based on the number of times words wi and wj co-occur in a sliding window of size 10 that moves over all the articles in Wikipedia. (Lau et al., 2014) propose a variant
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
of CUCI by using normalized PMI (NPMI) instead of PMI.
(Mimno et al., 2011) propose similar topic coherence measure (CUMASS) that uses log conditional probability (LCP) instead of PMI and uses the same corpus on which topics are inferred, to estimate LCP rather than Wikipedia. CUMASS for a topic t is computed as:
CUMASS(t) = 2
N(N − 1) N∑ i=2 i−1∑ j=1 log P (wi, wj) + 1 P (wj)
(Aletras and Stevenson, 2013) propose a topic coherence measure based on distributional similarity between the most N probable words of the topic. In this approach, a topic word (wi) is represented as a context vector (~vi) over the words that co-occur with wi in Wikipedia in a window of ±5 words, such that vi,j represents PMI (or NPMI) between words wi and wj . The word vectors of topic words are then used to find coherence (CUSheffield) of a topic (t) as follows:
CUSheffield(t) =
∑N i=1 sim(TC , ~vi)
N
where, TC represents the centroid of the word vectors of the topic and sim(~u,~v) is cosine similarity between vectors ~u and ~v.
(Aletras and Stevenson, 2013) observed that CUCI with NPMI correlates well with human judgements than CUMASS and CUCI with PMI.
(Aletras and Stevenson, 2013) also propose an alternative to CUSheffield where they represent a topic word wi as a context vector over the space of topic words only. They observed that CUSheffield with topic words only outperforms CUCI with NPMI.
Roder et al. (2015) propose a unifying framework that represents a coherence measure as a composition of parts, that can be freely combined to form a configuration space of coherence definitions. These parts can be grouped into four dimensions: 1) first dimension defines number of ways a word set can be divided into smaller pieces, 2) second dimension defines confirmation measures like PMI or NPMI to measure the agreement of a given word pair, 3) third dimension defines different ways to estimate word probabilities (P (wi) and P (wi, wj)), 4) fourth dimension defines methods to aggregate scalar values computed by the
confirmation measure. This framework spans over a large number of configuration space of coherence measures, hence it becomes computationally expensive to find appropriate coherence measure for a set of topics.",,"We proposed a novel approach TBuckets to measure quality of Latent Dirichlet Allocation (LDA) based topics, based on grouping of topic words into buckets. TBuckets uses 3 different techniques for creating buckets of words - TBucketsClustering which performs agglomerative clustering of words, TBuckets-SVD which uses singular value decomposition (SVD) to discover important sub-themes in topic words and lastly TBucketsSVD-Reorg which reorganized buckets obtained from TBuckets-SVD. We evaluated our techniques on three publicly available datasets by correlating the estimated coherence scores with human annotated scores and demonstrated better performance than the state-of-the-art results. Moreover, as compared to the state-of-the-art technique which needs to tune multiple parameters, our techniques TBuckets-SVD and TBuckets-SVD-Reorg require absolutely no parameter tuning. We also highlighted the utility of TBuckets for the task of weakly supervised text classification.
In future, we plan to devise better ways to compute word similarities which would be more suitable for specific domains like Genomics. Furthermore, we plan to devise a unified framework to evaluate and employ various scoring metrics. Also, we wish to explore usefulness of our techniques for applications other than text classification."
18,"Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers.Statistical classification using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features.  We perform feature significance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment analyzers to handle complex constructs.",Leveraging Cognitive Features for Sentiment Analysis,124,"This paper proposed a very interesting idea of using cognitive features for
sentiment analysis and sarcasm detection. More specifically, the eye-movement
patterns of human annotators are recorded to derive a new set of features. The
authors claim that this is the first work to include cognitive features into
the NLP community. 

Strength: 
1. The paper is generally well written and easy to follow
2. Very interesting idea which may inspire research in other NLP tasks.

Weakness:
1. The motivation of using cognitive features for sentiment analysis is not
very well justified. I can imagine these features may help reflect the reading
ease, but I don't see why they are helpful in detecting sentiment polarities.
2. The improvement is marginal after considering cognitive features by
comparing Sn+Sr+Gz with Sn+Sr.
3. Although the authors discussed about the feasibility of the approach in
Section 7, but I'm not convinced, especially about the example given in section
7.2, I don't see why this technique is helpful in such a scenario.",,4,2,Poster,4,4,4,3,4,5,3,4,2016,"Sentiment analyzers face the following three challenges at the lexical level: (1) Data Sparsity, i.e., handling the presence of unseen words/phrases. (e.g., The movie is messy, uncouth, incomprehensible, vicious and absurd) (2) Lexical Ambiguity,
e.g., finding appropriate senses of a word given the context (e.g., His face fell when he was dropped from the team vs The boy fell from the bicycle, where the verb “fell” has to be disambiguated) (3) Domain Dependency, tackling words that change polarity across domains. (e.g., the word unpredictable being positive in case of unpredictable movie in movie domain and negative in case of unpredictable steering in car domain). Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).Difficulty at the syntax level arises when the given text follows a complex phrasal structure and, phrase attachments are expected to be resolved before performing SA. For instance, the sentence A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race. requires processing at the syntactic level, before analyzing the sentiment. Approaches leveraging syntactic properties of text include generating dependency based rules for SA (Poria et al., 2014) and leveraging local dependency (Li et al., 2010).This corresponds to the difficulties arising in the higher layers of NLP, i.e., semantic and pragmatic layers. Challenges in these layers include handling: (a) Sentiment expressed implicitly (e.g., Guy gets girl, guy loses girl, audience falls asleep.) (b) Presence of sarcasm and other
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.).
Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone.We use two publicly available datasets for our experiments. Dataset 1 has been released by Mishra et al. (2016) which they use for the task of sarcasm understandability prediction. Dataset 2 has been used by Joshi et al. (2014) for the task of sentiment annotation complexity prediction. These datasets contain many instances with higher level nuances like presence of implicit sentiment, sarcasm and thwarting. We describe the datasets below.It contains 994 text snippets with 383 positive and 611 negative examples. Out of this, 350 are sarcastic or have other forms of irony. The snippets are a collection of reviews, normalized-tweets and quotes. Each snippet is annotated by seven participants with binary positive/negative polarity labels. Their eye-movement patterns are recorded with a high quality SR-Research Eyelink-1000 eyetracker (sampling rate 500Hz). The annotation accuracy varies from 70%-90% with a Fleiss kappa inter-rater agreement of 0.62.This dataset consists of 1059 snippets comprising movie reviews and normalized tweets. Each snippet is annotated by five participants with positive, negative and objective labels. Eye-tracking is done using a low quality Tobii T120 eye-tracker (sampling rate 120Hz). The annotation accuracy varies from 75%-85% with a Fleiss kappa interrater agreement of 0.68. We rule out the objective ones and consider 843 snippets out of which 443 are positive and 400 are negative.It is essential to check whether our selected datasets really pose challenges to existing sentiment analyzers or not. For this, we implement two statistical classifiers and a rule based classifier to check the test accuracy of Dataset 1 and Dataset 2. The statistical classifiers are based on Support Vector Machine (SVM) and Näive Bayes (NB) implemented using Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http:// www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as explained by Jia et al. (2009) and intensifiers as explained by Dragut and Fellbaum (2014).
Table 1 presents the accuracy of the three systems. The F-scores are not very high for all the systems (especially for dataset 1 that contains more sarcastic/ironic texts), possibly indicating that the snippets in our dataset pose challenges for
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
existing sentiment analyzers. Hence, the selected datasets are ideal for our current experimentation that involves cognitive features.Our feature-set into four categories viz. (1) Sentiment features (2) Sarcasm, Irony and Thwarting related Features (3) Cognitive features from eyemovement (4) Textual features related to reading difficulty. We describe our feature-set below.We consider a series of textual features that have been extensively used in sentiment literature (Liu and Zhang, 2012). The features are described below. Each feature is represented by a unique abbreviated form, which are used in the subsequent discussions.
1. Presence of Unigrams (NGRAM˙PCA) i.e. Presence of unigrams appearing in each sentence that also appear in the vocabulary obtained from the training corpus. To avoid overfitting (since our training data size is less), we reduce the dimension to 500 using Principal Component Analysis.
2. Subjective words (Positive words, Negative words) i.e. Presence of positive and negative words computed against MPQA lexicon (Wilson et al., 2005), a popular lexicon used for sentiment analysis.
3. Subjective scores (PosScore, NegScore) i.e. Scores of positive subjectivity and negative subjectivity using SentiWordNet (Esuli and Sebastiani, 2006).
4. Sentiment flip count (FLIP) i.e. Number of times words polarity changes in the text. Word polarity is determined using MPQA lexicon.
5. Part of Speech ratios (VERB, NOUN, ADJ, ADV) i.e. Ratios (proportions) of verbs, nouns, adjectives and adverbs in the text. This is computed using NLTK1.
6. Count of Named Entities (NE) i.e. Number of named entity mentions in the text. This is computed using NLTK.
1http://www.nltk.org/
7. Discourse connectors (DC) i.e. Number of discourse connectors in the text computed using an in-house list of discourse connectors (like however, although etc.)To handle complex texts containing constructs irony, sarcasm and thwarted expectations as explained earlier, we consider the following features. The features are taken from Riloff et al. (2013), Ramteke et al. (2013) and Joshi et al. (2015).
1. Implicit incongruity (IMPLICIT PCA) i.e. Presence of positive phrases followed by negative situational phrase (computed using bootstrapping technique suggested by Riloff et al. (2013)). We consider the top 500 principal components of these phrases to reduce dimension, in order to avoid overfitting.
2. Punctuation marks (PUNC) i.e. Count of punctuation marks in the text.
3. Largest pos/neg subsequence (LAR) i.e. Length of the largest series of words with polarities unchanged. Word polarity is determined using MPQA lexicon.
4. Lexical polarity (LP) i.e. Sentence polarity found by supervised logistic regression using the dataset used by Joshi et al. (2015).Eye-movement patterns are characterized by two basic attributes: (1) Fixations, corresponding to a longer stay of gaze on a visual object (like characters, words etc. in text) (2) Saccades, corresponding to the transition of eyes between two fixations. Moreover, a saccade is called a Regressive Saccade or simply, Regression if it represents a phenomenon of going back to a pre-visited segment. A portion of a text is said to be skipped if it does not have any fixation. Figure 1 shows eye-movement behavior during annotation of the given sentence in dataset-1. The circles represent fixation and the line connecting the circles represent saccades. Our cognition driven features are derived from these basic eye-movement attributes. We divide our features in two sets as explained ahead.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
Figure 1: Snapshot of eye-movement behavior during annotation of an opinionated text. The circles represent fixations and lines connecting the circles represent saccades. Boxes represent Areas of Interest (AoI) which are words of the sentence in our case.Readers’ eye-movement behavior, characterized by fixations, forward saccades, skips and regressions, can be directly quantified by simple statistical aggregation (i.e., computing features for individual participants and then averaging). Since these behaviors intuitively relate to the cognitive process of the readers (Rayner and Sereno, 1994), we consider simple statistical properties of these factors as features to our model. Some of these features have been reported by Mishra et al. (2016) for modeling sarcasm understandability of readers. However, as far as we know, these features are being introduced in NLP tasks like sentiment analysis for the first time.
1. Average First-Fixation Duration per word (FDUR) i.e. Sum of first-fixation duration divided by word count. First fixations are fixations occurring during the first pass reading. Intuitively, an increased first fixation duration is associated to more time spent on the words, which accounts for lexical complexity. This is motivated by Rayner and Duffy (1986).
2. Average Fixation Count (FC) i.e. Sum of fixation counts divided by word count. If the reader reads fast, the first fixation duration may not be high even if the lexical complexity is more. But the number of fixations may increase on the text. So, fixation count may help capture lexical complexity in such cases.
3. Average Saccade Length (SL) i.e. Sum of saccade lengths (measured by number of words) divided by word count. Intuitively, lengthy saccades represent the text being structurally/syntactically complex. This is also supported by von der Malsburg and Vasishth (2011).
4. Regression Count (REG) i.e. Total number of gaze regressions. Regressions correspond to both lexical and syntactic reanalysis (Malsburg et al., 2015). Intuitively,
regression count should be useful in capturing both syntactic and semantic difficulties.
5. Skip count (SKIP) i.e. Number of words skipped divided by total word count. Intuitively, higher skip count should correspond lesser semantic processing requirement (assuming that skipping is not done intentionally).
6. Count of regressions from second half to first half of the sentence (RSF) i.e. Number of regressions from second half of the sentence to the first half of the sentence (given the sentence is divided into two equal half of words). Constructs like sarcasm, irony often have phrases that are incongruous (e.g. ”The book is so great that it can be used as a paperweight”- the incongruous phrases are ”book is so great” and ”used as a paperweight”.. Intuitively, when a reader encounters such incongruous phrases, the second phrases often cause a surprisal resulting in a long regression to the first part of the text. Hence, this feature is considered.
7. Largest Regression Position (LREG) i.e. Ratio of the absolute position of the word from which a regression with the largest amplitude (in terms of number of characters) is observed, to the total word count of sentence. This is chosen under the assumption that regression with the maximum amplitude may occur from the portion of the text which causes maximum surprisal (in order to get more information about the portion causing maximum surprisal). The relative starting position of such portion, captured by LREG, may help distinguish between sentences with different linguistic subtleties.We propose a graph structure constructed from the gaze data to derive more complex gaze features. We term the graph as gaze-saliency graphs.
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599original mis-conception I had of you
Figure 2: Saliency graph of a human annotator for the sentence I will always cherish the original misconception I had of you.
A gaze-saliency graph for a sentence S for a reader R, represented as G = (V,E), is a graph with vertices (V ) and edges (E) where each vertex v ∈ V corresponds to a word in S (may not be unique) and there exists an edge e ∈ E between vertices v1 and v2 if R performs at least one saccade between the words corresponding to v1 and v2. Figure 2 shows an example of such a graph.
1. Edge density of the saliency gaze graph (ED) i.e. Ratio of number of edges in the gaze saliency graph and total number of possible links ((|V |×|V |−1|)/2) in the saliency graph. As, Edge Density of a saliency graph increases with the number of distinct saccades, it is supposed to increase if the text is semantically more difficult.
2. Fixation Duration at Left/Source (F1H, F1S) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the fixation duration on the word of node i of edge Eij as edge weight.
3. Fixation Duration at Right/Target (F2H, F2S) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the fixation duration of the word of node i of edge Eij as edge weight.
4. Forward Saccade Word Count of Source (PSH, PSS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of forward saccades between nodes i and j of an edge Eij as edge weight..
5. Forward Saccade Word Count of Destination (PSDH, PSDS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the total distance (word count) of forward saccades between nodes i and j of an edge Eij as edge weight.
6. Regressive Saccade Word Count of Source (RSH, RSS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of regressive saccades between nodes i and j of an edge Eij as edge weight.
7. Regressive Saccade Word Count of Destination (RSDH,RSDS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of regressive saccades between nodes i and j of an edge Eij as edge weight.
The ”highest and second highest degree” based gaze features derived from saliency graphs are motivated by our qualitative observations from the gaze data. Intuitively, the highest weighted degree of a graph is expected to be higher if some phrases have complex semantic relationships with others.Eye-movement during reading text with sentiment related nuances (like sarcasm) can be similar to text with other forms of difficulties. To address the effect of sentence length, word length and syllable count that affect reading behavior, we consider the following features.
1. Readability Ease (RED) i.e. Flesch Readability Ease score of the text (Kincaid et al., 1975). Higher the score, easier is the text to comprehend.
2. Sentence Length (LEN) i.e. Number of words in the sentence.
We now explain our experimental setup and results.To study whether the cognitive features actually help in classifying complex output as hypothesized earlier, we repeat the experiment on a heldout dataset, randomly derived from Dataset-1. It has 294 text snippets out of which 131 contain complex constructs like irony/sarcasm and rest of the snippets are relatively simpler. We choose
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Irony Non-Irony Sn 58.2 75.5
Sn+Sr 60.1 75.9 Gz+Sn+Sr 64.3 77.6
Table 4: F-scores on held-out dataset for Complex Constructs (Irony), Simple Constructs (Nonirony)
SVM, our best performing classifier, with similar configuration as explained in section 5. As seen in Table 4, the relative improvement of F-score, when gaze features are included, is 6.1% for complex texts and is 2.1% for simple texts (all the values are statistically significant with p < 0.05 for McNemar test, except Sn and Sn + Sr for Nonirony case.). This demonstrates the efficacy of the gaze based features.Errors committed by our system arise from multiple factors starting from limitations of the eyetracker hardware to errors committed by linguistic tools and resources. Moreover, aggregating various eye-tracking parameters to extract the cognitive features may have caused loss of information. For example, the graph based features are computed for each participant and eventually averaged to get the graph features for a sentence, thereby not leveraging the power of individual eye-movement patterns.Since our method requires gaze data from human readers to be available, the methods practicability becomes questionable. We present our views on this below.Availability of inexpensive embedded eye-trackers on hand-held devices has come close to reality now. This opens avenues to get eye-tracking data from inexpensive mobile devices from a huge population of online readers non-intrusively, and derive cognitive features to be used in predictive frameworks like ours. For instance, Cogisen: (http://www.sencogi.com) has a patent (ID: EP2833308-A1) on “eye-tracking using inexpensive mobile web-cams”.We believe, mobile eye-tracking modules could be a part of mobile applications built for e-commerce, online learning, gaming etc. where automatic analysis of online reviews calls for better solutions to detect and handle linguistic nuances in sentiment analysis setting. To give an example, let’s say a book gets different reviews on Amazon. Our system could watch how readers read the review using mobile eye-trackers, and thereby, decide the polarity of opinion, especially when sentiment is not expressed explicitly (e.g., using strong polar words) in the text. Such an application can horizontally scale across the web, helping to improve automatic classification of online reviews.Eye-tracking technology has already been utilized by leading mobile technology developers (like Samsung) to facilitate richer user experiences through services like Smart-scroll (where a user’s eye movement determines whether a page has to be scrolled or not) and Smart-lock (where user’s gaze position decided whether to lock the screen or not). The growing interest of users in using such services takes us to a promising situation where getting users’ consent to record eyemovement patterns will not be difficult, though it is yet not the current state of affairs.","We test the effectiveness of the enhanced featureset by implementing three classifiers viz., SVM (with linear kernel), NB and Multi-layered Neural Network. These systems are implemented using the Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. Several classifier hyperparameters are kept to the default values given in Weka. We separately perform a 10-fold cross validation on both Dataset 1 and 2 using different sets of feature combinations. The average F-scores for the class-frequency based random classifier are 33% and 46.93% for dataset 1 and dataset 2 respectively.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Classifier Näive Bayes SVM Multi-layer NN Dataset 1
P R F P R F P R F Uni 58.5 57.3 57.9 67.8 68.5 68.14 65.4 65.3 65.34 Sn 58.7 57.4 58.0 69.6 70.2 69.8 67.5 67.4 67.5
Sn + Sr 63.0 59.4 61.14 72.8 73.2 72.9 69.0 69.2 69.1 Gz 61.8 58.4 60.05 54.3 52.6 53.4 59.1 60.8 60
Sn+Gz 60.2 58.8 59.2 69.5 70.1 69.6 70.3 70.5 70.4 Sn+ Sr+Gz 63.4 59.6 61.4 73.3 73.6 73.5 70.5 70.7 70.6
Dataset 2 Uni 51.2 50.3 50.74 57.8 57.9 57.8 53.8 53.9 53.8 Sn 51.1 50.3 50.7 62.5 62.5 62.5 58.0 58.1 58.0 Sn+Sr 50.7 50.1 50.39 70.3 70.3 70.3 66.8 66.8 66.8 Gz 49.9 50.9 50.39 48.9 48.9 48.9 53.6 54.0 53.3 Sn+Gz 49.9 50.9 48.5 48.9 48.9 48.9 53.6 54.0 53.8 Sn+ Sr+Gz 50.2 49.7 50 71.9 71.8 71.8 69.1 69.2 69.1
Table 2: Results for different feature combinations. (P,R,F)→ Precision, Recall, F-score. Feature labels Uni→Unigram features, Sn→Sentiment features, Sr→Sarcasm features and Gz→Gaze features along with features related to reading difficulty
The classification accuracy is reported in Table 2. We observe the maximum accuracy with the complete feature-set comprising Sentiment, Sarcasm and Thwarting, and Cognitive features derived from gaze data. For this combination, SVM outperforms the other classifiers. The novelty of our feature design lies in (a) First augmenting sarcasm and thwarting based features (Sr) with sentiment features (Sn), which shoots up the accuracy by 3.1% for Dataset1 and 7.8% for Dataset2 (b) Augmenting gaze features with Sn+Sr, which further increases the accuracy by 0.6% and 1.5%for Dataset 1 and 2 respectively, amounting to an overall improvement of 3.7% and 9.3% respectively.
Since the best and the second best features are close in terms of accuracy for dataset 1 (difference of 0.5%), we perform a statistical significance test using McNemar test (α = 0.05). The difference in the F-scores turns out to be significant with p = 0.0001.
We also perform a chi-squared test based feature significance analysis, shown in Table 3. For dataset 1, 10 out of the top 20 ranked features are gaze-based features and for dataset 2, 7 out of top 20 features are gaze-based, as shown in bold letters.","We empower our systems by augmenting cognitive features along with traditional linguistic features used for general sentiment analysis, thwarting and sarcasm detection. Cognitive features are derived from the eye-movement patterns of human annotators recorded while they annotate short-text with sentiment labels. Our hypothesis is that cognitive processes in the brain are related to eye-movement activities (Parasuraman and Rizzo, 2006). Hence, considering readers’ eye-movement patterns while they read sentiment bearing texts may help tackle linguistic nuances better. We perform statistical classification using various classifiers and different feature combinations. With our augmented feature-set, we observe a significant improvement of accuracy across all classifiers for two different datasets. Experiments on a carefully curated held-out dataset indicate a significant improvement in sentiment polarity detection over the state of the art, specifically text with complex constructs like irony and sarcasm. Through feature significance analysis, we show that cognitive features indeed empower sentiment analyzers to handle complex constructs like irony and sarcasm. Our approach is the first of its kind to the best of our knowledge.
The rest of the paper is organized as follows. Section 2 presents a summary of past work done
in traditional SA and SA from a psycholinguistic point of view. Section 3 describes the available datasets we have taken for our analysis. Section 4 presents an our features that comprise both traditional textual features, used for sentiment analysis and cognitive features derived from annotators’ eye-movement patterns. In section 5, we discuss the results for various sentiment classification techniques under different combinations of textual and cognitive features, showing the effectiveness of cognitive features. In section 7, we discuss on the feasibility of our approach before concluding the paper in section 8.","Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task.
Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators (Ikeda et al., 2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches.
Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. (2013) proposed a studied the cognitive aspects if Word Sense Disambiguation (WSD) through eye-
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
NB SVM RB P R F P R F P R F
D1 66.15 66 66.15 64.5 65.3 64.9 56.8 60.9 53.5 D2 74.5 74.2 74.3 77.1 76.5 76.8 75.9 53.9 63.02
Table 1: Classification results for different SA systems for dataset 1 (D1) and dataset 2 (D2). P→ Precision, R→ Recall, F→ F˙score
tracking. Earlier, Mishra et al. (2013) measured translation annotation difficulty of a given sentence based on gaze input of translators used to label training data. The recent advancements in the literature discussed above, motivate us to explore gaze-based cognition for sentiment analysis.
We acknowledge that some of the well performing sentiment analyzers use Deep Learning techniques (like Convolutional Neural Network based approach by Maas et al. (2011) and Recursive Neural Network based approach by dos Santos and Gatti (2014)). In these, the features are automatically learned from the input text. Since our approach is feature based, we do not consider these approaches for our current experimentation. Taking inputs from gaze data and using them in a deep learning setting sounds intriguing, though, it is beyond the scope of this work.",,"We combined traditional sentiment features with (a) different textual features used for sarcasm and thwarting detection, and (b) cognitive features derived from readers’ eye movement behavior. The combined feature set improves the overall accuracy over the traditional feature set based SA by a margin of 3.6% and 9.3% respectively for Datasets 1 and 2. It is significantly effective for text with complex constructs, leading to an improvement of 6.1% on our held-out data. In future, we propose to explore (a) devising deeper gaze-based features and (b) multi-view classification using independent learning from linguistics and cognitive data. We also plan to explore deeper graph and gaze features, and models to learn complex gaze feature representation. Our general approach may be useful in other problems like emotion analysis, text summarization and question answering, where textual clues alone do not prove to be sufficient.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
19,"Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers.Statistical classification using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features.  We perform feature significance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment analyzers to handle complex constructs.",Leveraging Cognitive Features for Sentiment Analysis,124,"This paper is about introducing eye-tracking features for sentiment analysis as
a type of cognitive feature.  I think that the idea of introducing eye-tracking
features as a proxy for cognitive load for sentiment analysis is an interesting
one.  

I think the discussion on the features and comparison of feature sets is clear
and very helpful.  I also like that the feasibility of the approach is
addressed in section 7.

I wonder if it would help the evaluation if the datasets didn't conflate
different domains, e.g., the movie review corpus and the tweet corpus.             
For one
it might improve the prediction of movie review (resp. tweets) if the tweets
(resp. movie reviews) weren't in the training.              It would also make the
results
easier to interpret.  The results in Table 2 would seem rather low compared to
state-of-the art results for the Pang and Lee data, but look much better if
compared to results for Twitter data.

In Section 3.3, there are no overlapping snippets in the training data and
testing data of datasets 1 and 2, right?  Even if they come from the same
sources (e.g., Pang & Lee and Sentiment 140).

Minor: some of the extra use of bold is distracting (or maybe it's just me);",,4,4,Oral Presentation,5,5,4,2,5,5,4,4,2016,"Sentiment analyzers face the following three challenges at the lexical level: (1) Data Sparsity, i.e., handling the presence of unseen words/phrases. (e.g., The movie is messy, uncouth, incomprehensible, vicious and absurd) (2) Lexical Ambiguity,
e.g., finding appropriate senses of a word given the context (e.g., His face fell when he was dropped from the team vs The boy fell from the bicycle, where the verb “fell” has to be disambiguated) (3) Domain Dependency, tackling words that change polarity across domains. (e.g., the word unpredictable being positive in case of unpredictable movie in movie domain and negative in case of unpredictable steering in car domain). Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).Difficulty at the syntax level arises when the given text follows a complex phrasal structure and, phrase attachments are expected to be resolved before performing SA. For instance, the sentence A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race. requires processing at the syntactic level, before analyzing the sentiment. Approaches leveraging syntactic properties of text include generating dependency based rules for SA (Poria et al., 2014) and leveraging local dependency (Li et al., 2010).This corresponds to the difficulties arising in the higher layers of NLP, i.e., semantic and pragmatic layers. Challenges in these layers include handling: (a) Sentiment expressed implicitly (e.g., Guy gets girl, guy loses girl, audience falls asleep.) (b) Presence of sarcasm and other
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.).
Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone.We use two publicly available datasets for our experiments. Dataset 1 has been released by Mishra et al. (2016) which they use for the task of sarcasm understandability prediction. Dataset 2 has been used by Joshi et al. (2014) for the task of sentiment annotation complexity prediction. These datasets contain many instances with higher level nuances like presence of implicit sentiment, sarcasm and thwarting. We describe the datasets below.It contains 994 text snippets with 383 positive and 611 negative examples. Out of this, 350 are sarcastic or have other forms of irony. The snippets are a collection of reviews, normalized-tweets and quotes. Each snippet is annotated by seven participants with binary positive/negative polarity labels. Their eye-movement patterns are recorded with a high quality SR-Research Eyelink-1000 eyetracker (sampling rate 500Hz). The annotation accuracy varies from 70%-90% with a Fleiss kappa inter-rater agreement of 0.62.This dataset consists of 1059 snippets comprising movie reviews and normalized tweets. Each snippet is annotated by five participants with positive, negative and objective labels. Eye-tracking is done using a low quality Tobii T120 eye-tracker (sampling rate 120Hz). The annotation accuracy varies from 75%-85% with a Fleiss kappa interrater agreement of 0.68. We rule out the objective ones and consider 843 snippets out of which 443 are positive and 400 are negative.It is essential to check whether our selected datasets really pose challenges to existing sentiment analyzers or not. For this, we implement two statistical classifiers and a rule based classifier to check the test accuracy of Dataset 1 and Dataset 2. The statistical classifiers are based on Support Vector Machine (SVM) and Näive Bayes (NB) implemented using Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http:// www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as explained by Jia et al. (2009) and intensifiers as explained by Dragut and Fellbaum (2014).
Table 1 presents the accuracy of the three systems. The F-scores are not very high for all the systems (especially for dataset 1 that contains more sarcastic/ironic texts), possibly indicating that the snippets in our dataset pose challenges for
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
existing sentiment analyzers. Hence, the selected datasets are ideal for our current experimentation that involves cognitive features.Our feature-set into four categories viz. (1) Sentiment features (2) Sarcasm, Irony and Thwarting related Features (3) Cognitive features from eyemovement (4) Textual features related to reading difficulty. We describe our feature-set below.We consider a series of textual features that have been extensively used in sentiment literature (Liu and Zhang, 2012). The features are described below. Each feature is represented by a unique abbreviated form, which are used in the subsequent discussions.
1. Presence of Unigrams (NGRAM˙PCA) i.e. Presence of unigrams appearing in each sentence that also appear in the vocabulary obtained from the training corpus. To avoid overfitting (since our training data size is less), we reduce the dimension to 500 using Principal Component Analysis.
2. Subjective words (Positive words, Negative words) i.e. Presence of positive and negative words computed against MPQA lexicon (Wilson et al., 2005), a popular lexicon used for sentiment analysis.
3. Subjective scores (PosScore, NegScore) i.e. Scores of positive subjectivity and negative subjectivity using SentiWordNet (Esuli and Sebastiani, 2006).
4. Sentiment flip count (FLIP) i.e. Number of times words polarity changes in the text. Word polarity is determined using MPQA lexicon.
5. Part of Speech ratios (VERB, NOUN, ADJ, ADV) i.e. Ratios (proportions) of verbs, nouns, adjectives and adverbs in the text. This is computed using NLTK1.
6. Count of Named Entities (NE) i.e. Number of named entity mentions in the text. This is computed using NLTK.
1http://www.nltk.org/
7. Discourse connectors (DC) i.e. Number of discourse connectors in the text computed using an in-house list of discourse connectors (like however, although etc.)To handle complex texts containing constructs irony, sarcasm and thwarted expectations as explained earlier, we consider the following features. The features are taken from Riloff et al. (2013), Ramteke et al. (2013) and Joshi et al. (2015).
1. Implicit incongruity (IMPLICIT PCA) i.e. Presence of positive phrases followed by negative situational phrase (computed using bootstrapping technique suggested by Riloff et al. (2013)). We consider the top 500 principal components of these phrases to reduce dimension, in order to avoid overfitting.
2. Punctuation marks (PUNC) i.e. Count of punctuation marks in the text.
3. Largest pos/neg subsequence (LAR) i.e. Length of the largest series of words with polarities unchanged. Word polarity is determined using MPQA lexicon.
4. Lexical polarity (LP) i.e. Sentence polarity found by supervised logistic regression using the dataset used by Joshi et al. (2015).Eye-movement patterns are characterized by two basic attributes: (1) Fixations, corresponding to a longer stay of gaze on a visual object (like characters, words etc. in text) (2) Saccades, corresponding to the transition of eyes between two fixations. Moreover, a saccade is called a Regressive Saccade or simply, Regression if it represents a phenomenon of going back to a pre-visited segment. A portion of a text is said to be skipped if it does not have any fixation. Figure 1 shows eye-movement behavior during annotation of the given sentence in dataset-1. The circles represent fixation and the line connecting the circles represent saccades. Our cognition driven features are derived from these basic eye-movement attributes. We divide our features in two sets as explained ahead.
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
Figure 1: Snapshot of eye-movement behavior during annotation of an opinionated text. The circles represent fixations and lines connecting the circles represent saccades. Boxes represent Areas of Interest (AoI) which are words of the sentence in our case.Readers’ eye-movement behavior, characterized by fixations, forward saccades, skips and regressions, can be directly quantified by simple statistical aggregation (i.e., computing features for individual participants and then averaging). Since these behaviors intuitively relate to the cognitive process of the readers (Rayner and Sereno, 1994), we consider simple statistical properties of these factors as features to our model. Some of these features have been reported by Mishra et al. (2016) for modeling sarcasm understandability of readers. However, as far as we know, these features are being introduced in NLP tasks like sentiment analysis for the first time.
1. Average First-Fixation Duration per word (FDUR) i.e. Sum of first-fixation duration divided by word count. First fixations are fixations occurring during the first pass reading. Intuitively, an increased first fixation duration is associated to more time spent on the words, which accounts for lexical complexity. This is motivated by Rayner and Duffy (1986).
2. Average Fixation Count (FC) i.e. Sum of fixation counts divided by word count. If the reader reads fast, the first fixation duration may not be high even if the lexical complexity is more. But the number of fixations may increase on the text. So, fixation count may help capture lexical complexity in such cases.
3. Average Saccade Length (SL) i.e. Sum of saccade lengths (measured by number of words) divided by word count. Intuitively, lengthy saccades represent the text being structurally/syntactically complex. This is also supported by von der Malsburg and Vasishth (2011).
4. Regression Count (REG) i.e. Total number of gaze regressions. Regressions correspond to both lexical and syntactic reanalysis (Malsburg et al., 2015). Intuitively,
regression count should be useful in capturing both syntactic and semantic difficulties.
5. Skip count (SKIP) i.e. Number of words skipped divided by total word count. Intuitively, higher skip count should correspond lesser semantic processing requirement (assuming that skipping is not done intentionally).
6. Count of regressions from second half to first half of the sentence (RSF) i.e. Number of regressions from second half of the sentence to the first half of the sentence (given the sentence is divided into two equal half of words). Constructs like sarcasm, irony often have phrases that are incongruous (e.g. ”The book is so great that it can be used as a paperweight”- the incongruous phrases are ”book is so great” and ”used as a paperweight”.. Intuitively, when a reader encounters such incongruous phrases, the second phrases often cause a surprisal resulting in a long regression to the first part of the text. Hence, this feature is considered.
7. Largest Regression Position (LREG) i.e. Ratio of the absolute position of the word from which a regression with the largest amplitude (in terms of number of characters) is observed, to the total word count of sentence. This is chosen under the assumption that regression with the maximum amplitude may occur from the portion of the text which causes maximum surprisal (in order to get more information about the portion causing maximum surprisal). The relative starting position of such portion, captured by LREG, may help distinguish between sentences with different linguistic subtleties.We propose a graph structure constructed from the gaze data to derive more complex gaze features. We term the graph as gaze-saliency graphs.
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599original mis-conception I had of you
Figure 2: Saliency graph of a human annotator for the sentence I will always cherish the original misconception I had of you.
A gaze-saliency graph for a sentence S for a reader R, represented as G = (V,E), is a graph with vertices (V ) and edges (E) where each vertex v ∈ V corresponds to a word in S (may not be unique) and there exists an edge e ∈ E between vertices v1 and v2 if R performs at least one saccade between the words corresponding to v1 and v2. Figure 2 shows an example of such a graph.
1. Edge density of the saliency gaze graph (ED) i.e. Ratio of number of edges in the gaze saliency graph and total number of possible links ((|V |×|V |−1|)/2) in the saliency graph. As, Edge Density of a saliency graph increases with the number of distinct saccades, it is supposed to increase if the text is semantically more difficult.
2. Fixation Duration at Left/Source (F1H, F1S) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the fixation duration on the word of node i of edge Eij as edge weight.
3. Fixation Duration at Right/Target (F2H, F2S) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the fixation duration of the word of node i of edge Eij as edge weight.
4. Forward Saccade Word Count of Source (PSH, PSS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of forward saccades between nodes i and j of an edge Eij as edge weight..
5. Forward Saccade Word Count of Destination (PSDH, PSDS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the total distance (word count) of forward saccades between nodes i and j of an edge Eij as edge weight.
6. Regressive Saccade Word Count of Source (RSH, RSS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of regressive saccades between nodes i and j of an edge Eij as edge weight.
7. Regressive Saccade Word Count of Destination (RSDH,RSDS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of regressive saccades between nodes i and j of an edge Eij as edge weight.
The ”highest and second highest degree” based gaze features derived from saliency graphs are motivated by our qualitative observations from the gaze data. Intuitively, the highest weighted degree of a graph is expected to be higher if some phrases have complex semantic relationships with others.Eye-movement during reading text with sentiment related nuances (like sarcasm) can be similar to text with other forms of difficulties. To address the effect of sentence length, word length and syllable count that affect reading behavior, we consider the following features.
1. Readability Ease (RED) i.e. Flesch Readability Ease score of the text (Kincaid et al., 1975). Higher the score, easier is the text to comprehend.
2. Sentence Length (LEN) i.e. Number of words in the sentence.
We now explain our experimental setup and results.To study whether the cognitive features actually help in classifying complex output as hypothesized earlier, we repeat the experiment on a heldout dataset, randomly derived from Dataset-1. It has 294 text snippets out of which 131 contain complex constructs like irony/sarcasm and rest of the snippets are relatively simpler. We choose
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Irony Non-Irony Sn 58.2 75.5
Sn+Sr 60.1 75.9 Gz+Sn+Sr 64.3 77.6
Table 4: F-scores on held-out dataset for Complex Constructs (Irony), Simple Constructs (Nonirony)
SVM, our best performing classifier, with similar configuration as explained in section 5. As seen in Table 4, the relative improvement of F-score, when gaze features are included, is 6.1% for complex texts and is 2.1% for simple texts (all the values are statistically significant with p < 0.05 for McNemar test, except Sn and Sn + Sr for Nonirony case.). This demonstrates the efficacy of the gaze based features.Errors committed by our system arise from multiple factors starting from limitations of the eyetracker hardware to errors committed by linguistic tools and resources. Moreover, aggregating various eye-tracking parameters to extract the cognitive features may have caused loss of information. For example, the graph based features are computed for each participant and eventually averaged to get the graph features for a sentence, thereby not leveraging the power of individual eye-movement patterns.Since our method requires gaze data from human readers to be available, the methods practicability becomes questionable. We present our views on this below.Availability of inexpensive embedded eye-trackers on hand-held devices has come close to reality now. This opens avenues to get eye-tracking data from inexpensive mobile devices from a huge population of online readers non-intrusively, and derive cognitive features to be used in predictive frameworks like ours. For instance, Cogisen: (http://www.sencogi.com) has a patent (ID: EP2833308-A1) on “eye-tracking using inexpensive mobile web-cams”.We believe, mobile eye-tracking modules could be a part of mobile applications built for e-commerce, online learning, gaming etc. where automatic analysis of online reviews calls for better solutions to detect and handle linguistic nuances in sentiment analysis setting. To give an example, let’s say a book gets different reviews on Amazon. Our system could watch how readers read the review using mobile eye-trackers, and thereby, decide the polarity of opinion, especially when sentiment is not expressed explicitly (e.g., using strong polar words) in the text. Such an application can horizontally scale across the web, helping to improve automatic classification of online reviews.Eye-tracking technology has already been utilized by leading mobile technology developers (like Samsung) to facilitate richer user experiences through services like Smart-scroll (where a user’s eye movement determines whether a page has to be scrolled or not) and Smart-lock (where user’s gaze position decided whether to lock the screen or not). The growing interest of users in using such services takes us to a promising situation where getting users’ consent to record eyemovement patterns will not be difficult, though it is yet not the current state of affairs.","We test the effectiveness of the enhanced featureset by implementing three classifiers viz., SVM (with linear kernel), NB and Multi-layered Neural Network. These systems are implemented using the Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. Several classifier hyperparameters are kept to the default values given in Weka. We separately perform a 10-fold cross validation on both Dataset 1 and 2 using different sets of feature combinations. The average F-scores for the class-frequency based random classifier are 33% and 46.93% for dataset 1 and dataset 2 respectively.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Classifier Näive Bayes SVM Multi-layer NN Dataset 1
P R F P R F P R F Uni 58.5 57.3 57.9 67.8 68.5 68.14 65.4 65.3 65.34 Sn 58.7 57.4 58.0 69.6 70.2 69.8 67.5 67.4 67.5
Sn + Sr 63.0 59.4 61.14 72.8 73.2 72.9 69.0 69.2 69.1 Gz 61.8 58.4 60.05 54.3 52.6 53.4 59.1 60.8 60
Sn+Gz 60.2 58.8 59.2 69.5 70.1 69.6 70.3 70.5 70.4 Sn+ Sr+Gz 63.4 59.6 61.4 73.3 73.6 73.5 70.5 70.7 70.6
Dataset 2 Uni 51.2 50.3 50.74 57.8 57.9 57.8 53.8 53.9 53.8 Sn 51.1 50.3 50.7 62.5 62.5 62.5 58.0 58.1 58.0 Sn+Sr 50.7 50.1 50.39 70.3 70.3 70.3 66.8 66.8 66.8 Gz 49.9 50.9 50.39 48.9 48.9 48.9 53.6 54.0 53.3 Sn+Gz 49.9 50.9 48.5 48.9 48.9 48.9 53.6 54.0 53.8 Sn+ Sr+Gz 50.2 49.7 50 71.9 71.8 71.8 69.1 69.2 69.1
Table 2: Results for different feature combinations. (P,R,F)→ Precision, Recall, F-score. Feature labels Uni→Unigram features, Sn→Sentiment features, Sr→Sarcasm features and Gz→Gaze features along with features related to reading difficulty
The classification accuracy is reported in Table 2. We observe the maximum accuracy with the complete feature-set comprising Sentiment, Sarcasm and Thwarting, and Cognitive features derived from gaze data. For this combination, SVM outperforms the other classifiers. The novelty of our feature design lies in (a) First augmenting sarcasm and thwarting based features (Sr) with sentiment features (Sn), which shoots up the accuracy by 3.1% for Dataset1 and 7.8% for Dataset2 (b) Augmenting gaze features with Sn+Sr, which further increases the accuracy by 0.6% and 1.5%for Dataset 1 and 2 respectively, amounting to an overall improvement of 3.7% and 9.3% respectively.
Since the best and the second best features are close in terms of accuracy for dataset 1 (difference of 0.5%), we perform a statistical significance test using McNemar test (α = 0.05). The difference in the F-scores turns out to be significant with p = 0.0001.
We also perform a chi-squared test based feature significance analysis, shown in Table 3. For dataset 1, 10 out of the top 20 ranked features are gaze-based features and for dataset 2, 7 out of top 20 features are gaze-based, as shown in bold letters.","We empower our systems by augmenting cognitive features along with traditional linguistic features used for general sentiment analysis, thwarting and sarcasm detection. Cognitive features are derived from the eye-movement patterns of human annotators recorded while they annotate short-text with sentiment labels. Our hypothesis is that cognitive processes in the brain are related to eye-movement activities (Parasuraman and Rizzo, 2006). Hence, considering readers’ eye-movement patterns while they read sentiment bearing texts may help tackle linguistic nuances better. We perform statistical classification using various classifiers and different feature combinations. With our augmented feature-set, we observe a significant improvement of accuracy across all classifiers for two different datasets. Experiments on a carefully curated held-out dataset indicate a significant improvement in sentiment polarity detection over the state of the art, specifically text with complex constructs like irony and sarcasm. Through feature significance analysis, we show that cognitive features indeed empower sentiment analyzers to handle complex constructs like irony and sarcasm. Our approach is the first of its kind to the best of our knowledge.
The rest of the paper is organized as follows. Section 2 presents a summary of past work done
in traditional SA and SA from a psycholinguistic point of view. Section 3 describes the available datasets we have taken for our analysis. Section 4 presents an our features that comprise both traditional textual features, used for sentiment analysis and cognitive features derived from annotators’ eye-movement patterns. In section 5, we discuss the results for various sentiment classification techniques under different combinations of textual and cognitive features, showing the effectiveness of cognitive features. In section 7, we discuss on the feasibility of our approach before concluding the paper in section 8.","Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task.
Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators (Ikeda et al., 2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches.
Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. (2013) proposed a studied the cognitive aspects if Word Sense Disambiguation (WSD) through eye-
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
NB SVM RB P R F P R F P R F
D1 66.15 66 66.15 64.5 65.3 64.9 56.8 60.9 53.5 D2 74.5 74.2 74.3 77.1 76.5 76.8 75.9 53.9 63.02
Table 1: Classification results for different SA systems for dataset 1 (D1) and dataset 2 (D2). P→ Precision, R→ Recall, F→ F˙score
tracking. Earlier, Mishra et al. (2013) measured translation annotation difficulty of a given sentence based on gaze input of translators used to label training data. The recent advancements in the literature discussed above, motivate us to explore gaze-based cognition for sentiment analysis.
We acknowledge that some of the well performing sentiment analyzers use Deep Learning techniques (like Convolutional Neural Network based approach by Maas et al. (2011) and Recursive Neural Network based approach by dos Santos and Gatti (2014)). In these, the features are automatically learned from the input text. Since our approach is feature based, we do not consider these approaches for our current experimentation. Taking inputs from gaze data and using them in a deep learning setting sounds intriguing, though, it is beyond the scope of this work.",,"We combined traditional sentiment features with (a) different textual features used for sarcasm and thwarting detection, and (b) cognitive features derived from readers’ eye movement behavior. The combined feature set improves the overall accuracy over the traditional feature set based SA by a margin of 3.6% and 9.3% respectively for Datasets 1 and 2. It is significantly effective for text with complex constructs, leading to an improvement of 6.1% on our held-out data. In future, we propose to explore (a) devising deeper gaze-based features and (b) multi-view classification using independent learning from linguistics and cognitive data. We also plan to explore deeper graph and gaze features, and models to learn complex gaze feature representation. Our general approach may be useful in other problems like emotion analysis, text summarization and question answering, where textual clues alone do not prove to be sufficient.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
20,"In this paper, we propose a method which uses semi-supervised convolutional neural networks (CNNs) to select in-domain training data for statistical machine translation. This approach is particularly effective when only tiny amounts of in-domain data are available. The in-domain data and randomly sampled general-domain data are used to train a data selection model with semi-supervised CNN, then this model computes domain relevance scores for all the sentences in the general-domain data set. The sentence pairs with top scores are selected to train the system. We carry out experiments on 4 language directions with three test domains. Compared with strong baseline systems trained with large amount of data,   this method can improve the performance up to 3.1 BLEU.  Its performances are significant better than three state-of-the-art language model based data selection methods. We also show that the in-domain data used to train the selection model could be as few as 100 sentences, which makes fine-grained topic-dependent translation adaptation possible.",Semi-supervised Convolutional Networks for Translation Adaptation with Tiny Amount of In-domain Data,129,"The paper describes an MT training data selection approach that scores and
ranks general-domain sentences using a CNN classifier. Comparison to prior work
using continuous or n-gram based language models is well done, even though  it
is not clear of the paper also compared against bilingual data selection (e.g.
sum of difference of cross-entropies).
The motivation to use a CNN instead of an RNN/LSTM was first unclear to me, but
it is a strength of the paper to argue that certain sections of a text/sentence
are more important than others and this is achieved by a CNN. However, the
paper does not experimentally show whether a BOW or SEQ (or the combination of
both( representation is more important and why.
The textual description of the CNN (one-hot or semi-supervised using
pre-trained embeddings) 
is clear, detailed, and points out the important aspects. However, a picture of
the layers showing how inputs are combined would be worth a thousand words.

The paper is overall well written, but some parentheses for citations are not
necessary (\citet vs. \citep) (e.g line 385).

Experiments and evaluation support the claims of the paper, but I am a little
bit concerned about the method of determining the number of selected in-domain
sentences (line 443) based on a separate validation set:
- What validation data is used here? It is also not clear on what data
hyperparameters of the CNN models are chosen. How sensitive are the models to
this?
- Table 2 should really compare scores of different approaches with the same
number of sentences selected. As Figure 1 shows, the approach of the paper
still seems to outperform the baselines in this case. 

Other comments:
- I would be interested in an experiment that compares the technique of the
paper against baselines when more in-domain data is available, not just the
development set.
- The results or discussion section could feature some example sentences
selected by the different methods to support the claims made in section 5.4.
- In regards to the argument of abstracting away from surface forms in 5.4:
Another baseline to compare against could have been the work of Axelrod, 2015,
who replace some words with POS tags to reduce LM data sparsity to see whether
the word2vec embeddings provide an additional advantage over this.
- Using the sum of source and target classification scores is very similar to
source & target Lewis-Moore LM data selection: sum of difference of
cross-entropies. A reference to this work around line 435 would be reasonable.

Finally, I wonder if you could learn weights for the sum of both source &
target classification scores by extending the CNN model to the
bilingual/parallel setting.",,4,4,Poster,3,4,4,4,4,5,4,3,2016,"Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general-domain data, then it is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data for language model or translation model training. Some early data selection approaches (Zhao et al., 2004; Lü et al., 2007; Moore and Lewis, 2010) use in-domain monolingual data to select monolingual or bilingual data that are similar to the indomain data according to some criterion. By contrast, (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on
3
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
in-domain and out-domain data, respectively. Data weighting approaches weight each data item according to its relevance to the in-domain data. Mixture model adaptation (Foster and Kuhn, 2007; Foster et al., 2010; Sennrich, 2012; Foster et al., 2013) assumes that the general-domain data can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it can be applied to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010). If in-domain comparable data are available, (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) propose mining translations from the comparable data to translate out-of-vocabulary (OOV) words and capture new senses for the new test domains. (Dou and Knight, 2012; Zhang and Zong, 2013) learn bilingual lexical or phrase tables from in-domain monolingual data with a decipherment method, then incorporate them into the SMT system. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper.In a text classification task, key phrases (or ngrams) can help in determining the class of the text, regardless of their locations in the text. For example, the word “desktop” in a sentence may indicate this sentence has computers as its topic; the phrase “not satisfactory” may indicate that the sentiment of the entire sentence is negative. This kind of strong local information about the class of a text can appear in different regions in the input. Convolutional neural networks (CNNs) are useful for text classification because convolutional and
pooling layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Schütze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015), etc. Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned for a specific task. (Johnson and Zhang, 2015b) extends their “one-hot” CNN in (Johnson and Zhang, 2015a) to take region embeddings trained on unlabeled data as CNN input. CNNs that input word embeddings trained on unlabeled data are considered to be semi-supervised CNNs.A CNN is a feed-forward network consisting of convolutional and pooling layers. Each neuron in the convolutional layer of a CNN processes a segment of input signals, which could be a region in an image or a window of words in a sentence. The convolution layer consists of a set of kernels that compute the dot product between different segments of the input. The kernel associated with the l-th segment of the input x computes:
σ(W · wl(x) + b), (1)
where wl(x) ∈ Rq is the input window vector that represents the l-th segment of data. Weight matrix W ∈ Rm×q and bias vector b ∈ Rm are shared by all the kernels in the same layer, and are learned during the training process. Because the convolution kernel allows interaction between different parts of the input, it reduces
4
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
the requirement to select features by hand. Important features in a sentence are automatically selected with pooling, which is a form of non-linear down-sampling. It takes the maximum or the average value observed in each of the d dimension vectors over different windows. As a result, information from multiple d dimension vectors is kept in a single d dimensional vector. At training time, both the weight vectors and the bias vectors are learned with stochastic gradient ascent.When applying CNN to NLP tasks, the first layer of the network takes word embeddings as input. Word embeddings can be pre-trained using tools such as word2vec (Mikolov et al., 2013) or GloV e (Pennington et al., 2014), in which case a table lookup is enough. Alternatively, these vectors can be learned from scratch as a step in the network training process. When there are enough in-domain data, training in-domain word embeddings is meaningful. However, when the in-domain data are limited, the word embeddings learned from these data are unreliable. In this case, the input sentence x can be represented with one-hot vectors where each vector’s length is the vocabulary size, value 1 at index i indicates word i appears in the sentence, and 0 indicates its absence. A CNN with one-hot vector input is called “one-hot CNN” (Johnson and Zhang, 2015a). wl(x) can be either a concatenation of one-hot vectors, in which the order of concatenation is the same as the word order in the sentence, or it can be a bag-of-word/n-gram vector. The bag-of-word (BOW) representation loses word order information but is more robust to data sparsity. In (Johnson and Zhang, 2015a), a CNN whose input being BOW representation is called bow-CNN while input with concatenation of vectors is called seq-CNN. The window size and stride (distance between the window centers) are meta-parameters. σ in Equation 1 is a component-wise non-linear function such as ReLU. Thus, each kernel generates anm-dimensional vector wherem is the number of weight vectors or neurons. These vectors from all the windows of each sentence are aggregated by the pooling layer, by either componentwise maximum (max pooling) or average (average pooling), then used by the top layer as features for classification.Although the size of the in-domain data is normally small, the unlabeled data from general domains are much larger and easier to obtain. To exploit large amounts of unlabeled data, we adopt a semi-supervised learning framework similar to (Johnson and Zhang, 2015b). It first learns word embedding from unlabeled data, then generates the text segment embedding based on these unsupervised word embeddings. Both the one-hot vectors from the labeled data and the segment embeddings from unlabeled data are combined to train the CNN classifier. The word embeddings map each word to a realvalued, dense vector (Bengio et al., 2003). Word embeddings are often learned with an unsupervised learning paradigm: each dimension of the continuous word embeddings aims at capturing a latent feature, reflecting certain syntactic and semantic meanings of the word. A widely used approach for generating useful word embeddings was developed by (Mikolov et al., 2013). This method learns the word embeddings such that the likelihood of generating a word based on its contexts (or generating the context of a given word, aka “skip-gram” model) is maximized. It speeds up the training with the hierarchical softmax strategy and a simplified learning objective, which scales very well to very large training corpora. We adopt the skip-gram model, which intuitively learns a classifier that predicts words conditioned on the central word’s vector representation. An advantage of such distributed representations is that words that have similar contexts, and therefore similar syntactic and semantic properties, will tend to be near one another in the low-dimensional vector space. Given the word embeddings trained from unlabeled data, a sentence is represented as a sequence of d-dimensional vectors, which is the input to a convolution network that generates feature vectors for each text segment. The segment vectors and one-hot vectors are fed into another convolution layer, which outputs the classification labels. The second network is trained with the labeled indomain/out-domain data. Therefore, Equation 1 is replaced with:
σ(W · wl(x) + V · ul(x) + b), (2)
where wl(x) is the one-hot vector obtained from segment l in a sentence, and ul(x) is the embed-
5
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
ding learned from the unlabeled data (general domain training data), applied to the same segment. We train this model with the labeled data. We update the weights W, V, bias b, and the top-layer parameters so that the designated loss function is minimized on the labeled training data.We use the in-domain data from the translation task as positive samples, and randomly select the same number of sentences from the general domain data as negative samples. We train the CNN model on the positive and negative samples with the one-hot CNN or semi-supervised CNN described previously. The trained CNN model is then used to classify the sentence pairs in the general domain data. The sentence pairs with high in-domain scores are selected to train the machine translation system. We classify the source sentence and target sentences separately. The CNN model computes two scores for each sentence pair. The sentence pairs are selected based on the source scores, target scores or the sum of source and target scores. Experiments show that selection based on the sum of the source and target scores achieves the best performance. We empirically determine the number of selected in-domain sentences for each MT system based on experimental results on a separate validation set. When selecting the negative samples, we either randomly sample from the whole data pool, or select from the sentences which have been labeled as negative in the first round classification. Additional experiments show that the results from these two methods are very similar, so we sample the negative samples from the whole general domain for simplicity.Our goal is to adapt the MT system when only a tiny amount of in-domain data is available. So in our experiments, we did not consider any domain information about the training data, such as the source of each corpus. What we have is a small development set (dev) and one or more test sets (test) which are in the same domain.We carried out experiments in four different data settings. All four have large amounts of bilin-
gual training data: 9-15M sentences. The first two involve translation into English (en) from Chinese (zh) and Arabic (ar), while the last two involve translation from English to Spanish (es) and Chinese. The training data are all publicly available, either from LDC1 and transcriptions of TED talks2, where the data are the mixture of newswire, web crawl, UN proceedings and TED talks, etc., or from WMT3, where the data are the mixture of Europarl, web crawl, news-commentary, and UN proceedings, etc. The dev and test sets are “short messages (sms)” for the first task, which are also available from LDC; “tweets” for the second task; publicly available “Facebook post” for the remaining two tasks. The last three tasks are from social media - an intriguing new area of application for MT - where in-domain parallel training data are seldom publicly available. Table 1 summarizes the statistics of the training, dev, and test data for all the test sets.We experiment with two CNN-based data selection strategies:
1. ohcnn: Data selection by supervised one-hot CNN (Section 3.1)
2. sscnn: Data selection by semi-supervised CNN (Section 3.2)
We employ the dev set as in-domain data. All the supervised CNN models are trained with the in-domain dev data as positive examples and an equal number of randomly selected generaldomain sentences as negative examples. All the meta-parameters of the CNN are tuned on heldout data; we generate both bow-regions and seqregions and input them to the CNN. We set the region size to 5 and stride size to 1. The nonlinear function we chose is “ReLU”, the number of weight vectors or neurons is 500. The pooling method is component-wise maximum (max pooling). We use the online available CNN toolkit conText4. To train the general domain word embedding, we used word2vec5. The size of the vector was set to 300. We also generate wordembedding-based bow-regions and seq-regions as additional input to the CNN.
1https://catalog.ldc.upenn.edu/ 2https://wit3.fbk.eu/ 3http://statmt.org/wmt15/ 4http://riejohnson.com/cnn download.html 5https://code.google.com/archive/p/word2vec/
6
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
language zh2en ar2en en2es en2zh test domain sms tweets facebook facebook train origin LDC&TED LDC&TED WMT LDC&TED train size 12.20M 8.97M 15.23M 12.20M dev size 6,016 1,000 800 650 test size 3,282 1,500 3,378 3,343
Table 1: Summary of the data. “sms” means “short message”. “facebook” means “Facebook post”. Data is given as the number of sentence pairs, “M” represents “million”. The tasks “zh2en” and “en2zh” use the same training data.
We compared with four baselines for each task. The first baseline SMT system is trained using all general-domain data. The other three systems are trained with data selected with different LM-based data selection methods as same as in (Duh et al., 2013)6. The four baselines are:
1. alldata: All general-domain data.
2. ngram: Data selection by 3-gram LMs with Witten-Bell 7 smoothing (Axelrod et al., 2011)
3. rnnlm: Data selection by recurrent neural network LM, with the RNNLM Toolkit (Duh et al., 2013)
4. comblm: Data selection by the combined LM using ngram & rnnlm (equal weight) (Duh et al., 2013).
All systems are trained with a standard phrasebased SMT system with standard settings, i.e., GIZA++ alignment, phrase table Kneser-Ney smoothing, hierarchical reordering models, target side 4-gram language model, and “gigaword” 5- gram language model for systems with English as the target language, etc.","We evaluated the system using BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. Table 2 summarizes the results and numbers of the selected sentences for each task. First, we can see that all the data selection methods improved the performance over the baseline “alldata” with much less
6The code and scripts for the three baselines are available at http://cl.naist.jp/ kevinduh/a/acl2013/
7For small amounts of data, Witten-Bell smoothing had performed better than Kneser-Ney smoothing in our experiments
training data (only around 2.5% to 10% of the whole training data). Consistent with (Duh et al., 2013), the three LM based data selection all got improvements, where “rnnlm” obtained better performance than the “ngram” on average. It is not clear that combining the two language model methods (“comblm”) yields further improvement. While the one-hot CNN method “ohcnn” obtained similar improvement as the three LM-based methods on average. The semi-supervised CNN (sscnn) achieved the best performance for all the tasks: its improvements over the “alldata” baseline are 3.1, 1.4, 0.7 and 1.4 BLEU score respectively. It beats “ohcnn” by about 0.5 BLEU point on average. There are two results worth noticing. First, task 1 (zh2en sms task) obtained very high BLEU improvement through data selection. This is because in this task, there is a 120K in-domain subset within the general-domain data. If we train a system on this in-domain data set, we get 25.7 BLEU on the test set. The LM-based methods did not beat this “in-domain data only (indata)” baseline, while the semi-supervised CNN method performed significantly better than this baseline at p < 0.05 level. Second, for the other three tasks, there is no in-domain data component in the general-domain data (that we know of). Even in this case, we achieved up to 1.4 BLEU improvement, which demonstrates the effectiveness of our method: it can select highly suitable in-domain sentences, even when the in-domain data is very limited. In our second experiment, we examine how many labeled samples are needed to train a strong CNN classifier to select theMT training in-domain data. Fixing the number of MT training sentence pairs to 300K that will be selected by the CNN, we reduce the CNN training data from 6,000 down to 100 sentence pairs in steps. The performance of the resulting MT systems for all five data selection
7
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
zh2en ar2en en2es en2zh #sent BLEU #sent BLEU #sent BLEU #sent BLEU
alldata 12.2M 22.9 8.9M 17.6 15.2M 26.8 12.2M 10.0 ngram 300K 25.3** 800K 18.2** 1600K 26.9 400K 10.5* rnnlm 300K 25.6** 800K 18.4** 1600K 27.0 400K 10.5*
comblm 400K 25.7** 800K 18.4** 1400K 27.0 500K 10.4* ohcnn 300K 25.3** 700K 18.2* 1200K 27.1* 400K 11.0**+ sscnn 300K 26.0**+ 700K 19.0**++ 1300K 27.5**++ 300K 11.4**++
Table 2: Summary of the results. Data size is given as number of sentence pairs. The number of selected in-domain sentences is determined by the performance on held-out data. “M” represents million, “K” represents thousand. */** means result is significantly better than the “alldata” baseline at p < 0.05 or p < 0.01 level, respectively. +/++ means result is significantly better than the best LM-based method at p < 0.05 or p < 0.01 level, respectively.
Figure 1: The performance on zh2en sms task with different numbers of in-domain sentences to train LM-based vs. CNN-based classifiers, which are then used to select 300K sentence pairs for MT system training. X-axis is the number of in-domain sentences, Y-axis is BLEU score.
8
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
methods is shown in Figure 1. From Figure 1, we can see that all the data selection methods obtained improvement compared to the “alldata” baseline. When the in-domain training sample is more than 1600 sentence pairs, all the data selection methods obtain reasonable and comparable improvement, while “sscnn” is better than the best LM-based method by 0.3-0.5 BLEU. However, when the in-domain training sample is less than 800 sentence pairs, the difference between the “sscnn” and other methods gets bigger, and CNN-based methods get more stable results than the LM-based methods get. For instance, when the in-domain set increases from 400 to 800, the LM-based methods did not get an improvement; “ngram” and “comblm” even got a small loss on BLEU score. When the in-domain sample is reduced to 100 sentence pairs, the LM-based methods only get a small improvement over the baseline, while the “ohcnn” got a 1.2 BLEU score improvement over the baseline and “sscnn” got a 2.1 BLEU improvement over the baseline. Thus, even if we have no domain knowledge about the training data, when we have only 100 sentences in the test domain, the semi-supervised CNN classifier can still select a good in-domain subset and achieve good performance. We obtained 2.1 BLEU improvement even when we randomly select only 100 in-domain sentence pairs to train the classification model. Is this just because we luckily sampled a good part of the in-domain data? We repeated the “100 in-domain sentence pairs experiment” three times for our most effective method - “sscnn” - by sampling three different in-domain sets from the whole 6,016-sentence dev set. The average BLEU score we got is 25.03, and the standard deviation is 0.12. This means that our algorithm is quite stable even when the in-domain set is very small.","Statistical machine translation (SMT) systems are trained on bilingual parallel and monolingual data. The training corpora typically come from different sources, and vary across topics, genres, dialects, authors’ written styles, etc., which are usually referred as “general domain” training data. Here the word “domain” is often used to indicate some combination of all above and other possible hidden factors (Chen et al., 2013). At run time, the
content to be translated may come from a different domain. Due to the mismatch in “domains”, it is possible to achieve better performance by adapting the SMT system to the test domain (in-domain). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; Lü et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). To train the in-domain language model, a reasonable size in-domain data set, which typically includes several thousands of sentences, is required. In (Axelrod et al., 2011; Duh et al., 2013), the sizes of the in-domain data sets are 30K and over 100K sentences respectively. However, we do not always have access to large or even medium amounts of in-domain data. With the growth of social media, new domains have emerged which need machine translation but
2
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
which have very limited in-domain data, maybe just a few hundred sentence pairs. What’s more, if one wishes to build a large scale topic-specific MT system with hundreds of topics, it is prohibitively expensive to collect tens of thousands of in-domain sentences for each topic.
In this paper, we try to address this challenge, i.e., domain adaptation with very limited amounts of in-domain data. Inspired by the success of convolutional neural networks (CNNs) applied to image and text classification (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), we propose to use CNN to classify training sentence pairs as in-domain or out-of-domain sentences. To overcome the problem of limited in-domain data, we propose to augment the original model with semi-supervised convolutional neural networks for domain classification.
Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolution layers; each computation unit processes a small region of the input data. CNN has been very successful on image classification. When applying it to text input, the convolution layers process small regions of a document, i.e., a sequence of sentences or words. CNN has been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Schütze, 2015; Wang et al., 2015).
In many of these studies, the first layer of the network converts words to word embeddings using table lookup. The word embeddings are either trained as part of CNN training, or pre-trained (thus fixed during model training time) on an additional unlabled corpus. The later is termed semi-supervised CNN. Given tiny amounts of indomain data, the information learned in these pretrained word embeddings is very helpful.
We use a small amount of in-domain data, such as the development set, as the positive sample and randomly select the same number of sentences from the general-domain training data as the negative sample to form the training sample for training the CNN classification model. This is a typical supervised learning setting. To compensate the limit of in-domain data size, we use word2vec (Mikolov et al., 2013) to learn the word embedding from a large amount of general-domain data.
Together with the labeled data, these word embeddings are fed to the convolution layer as additional input to train the final classification model. This is a semi-supervised framework. The learned models are then used to classify each sentence in the general-domain training data based on their domain relevance score. The top N sentence pairs are selected to train the SMT system. We carry out experiments on 4 different language directions with 9-15M sentence pairs in each direction. The test domains include short message (sms), tweets, and Facebook posts, etc. The experimental results show that our method is able to select a small amount of training data that is used to create a system which outperforms baseline systems trained with all the general-domain data. For example, we obtain over 3.1 BLEU improvement on the Chinese-to-English sms task with around 3% of the whole training data. Experiments also show that we can reduce the size of the in-domain sample to around 100 sentences and still obtain a 2.1 BLEU improvement.",,"Why do semi-supervised convolutional neural networks perform so well for data selection? We think there are two main reasons. The first one is that convolution captures the important domain information of the words in the window, and the max-pooling operation combines the vectors which, as a result, focuses on the most important “features” in the sentence. Even a highly domainspecific sentence normally contains both domainspecific words and general-domain words. For
example, in “I have a Dell desktop and a Macbook laptop”, the words “Dell, laptop, Macbook, laptop” are from the computer domain, while the words “I, have, a, and” are general. However, the topic of this sentence is decided by the domain specific words, not the general-domain words. If the properties of the words “Dell, laptop, Macbook, laptop” are kept and highlighted, classification will be more accurate for this sentence. The second reason is the use of word embedding learned from the whole general-domain data. A very important advantage of word embedding is that words that have similar meaning will tend to be grouped together in the vector space. If the word “Lenovo” in the test sentence is not seen in the labeled data, it would be difficult for LMbased models to classify sentences like “I prefer choosing a Lenovo machine” as computer-domain sentence. However, the word embeddings learned from much larger unlabeled data ensure that the word embedding of “Lenovo” is close to that of “Dell”. According to the domain of its neighbor words, the CNN model can still label this sentence as belonging to the computer domain. This property is particularly useful for fast or fine grained adaptation, where obtaining large amount of indomain samples may be slow or too expensive.","Domain adaptation with only a tiny amount of in-domain data is a hard problem. In this paper, we proposed to use a semi-supervised convolutional neural network (CNN) to train the domain classification model, then use the CNN to select the data which is most similar to the test domain. Experiments on large data condition SMT tasks showed that this outperforms state-of-the-art language-model-based data selection methods significantly. Particularly when the size of the indomain data is small, semi-supervised CNN classifier can still select in-domain bilingual sentences to train an adapted SMT system. In future work, we plan to 1) apply this approach to select the data from large size target language corpus, such as the “Gigaword” corpus, for language model training; 2) use the source sentences of the test set to select the data for online dynamic adaptation."
21,"In this paper, we propose a method which uses semi-supervised convolutional neural networks (CNNs) to select in-domain training data for statistical machine translation. This approach is particularly effective when only tiny amounts of in-domain data are available. The in-domain data and randomly sampled general-domain data are used to train a data selection model with semi-supervised CNN, then this model computes domain relevance scores for all the sentences in the general-domain data set. The sentence pairs with top scores are selected to train the system. We carry out experiments on 4 language directions with three test domains. Compared with strong baseline systems trained with large amount of data,   this method can improve the performance up to 3.1 BLEU.  Its performances are significant better than three state-of-the-art language model based data selection methods. We also show that the in-domain data used to train the selection model could be as few as 100 sentences, which makes fine-grained topic-dependent translation adaptation possible.",Semi-supervised Convolutional Networks for Translation Adaptation with Tiny Amount of In-domain Data,129,"The paper describes a method for in-domain data selection for SMT with a
convolutional neural network classifier, applying the same framework as Johnson
and Zhang, 2015. The method performs about 0.5 BLEU points better than language
model based data selection, and, unlike the other methods, is robust even if
only a very small in-domain data set is provided. 

The paper claims improvements of 3.1 BLEU points. However, from the results we
see that improvements of this magnitude are only achieved if there are
in-domain data in the training set - training only on the in-domain data
already produces +2.8 BLEU. It might be interesting to also compare this to a
system which interpolates separate in- and out-domain models. 

The more impressive result, in my opinion, comes from the second experiment,
which demonstrates that the CNN classifier is still effective if there is very
little in-domain data. However, the second experiment is only run on the zh2en
task which includes actual in-domain data in the training set, possibly making
selection easier. Would the result also hold for the other tasks, where there
is no in-domain data in the training set? The results for the en2es and en2zh
task already point in this direction, since the development sets only contain a
few hundred sentence pairs. I think the claim would be better supported if
results were reported for all tasks when only 100 sentence pairs are used for
training.  

When translating social media text one often has to face very different
problems from other domains, the most striking being a high OOV rate due to
non-conventional spelling (for Latin scripts, at least). The texts can also
contain special character sequences such as usernames, hashtags or emoticons.
Was there any special preprocessing or filtering step applied to the data?  
Since data selection cannot address the OOV problem, it would be interesting to
know in more detail what kinds of improvements are made through adaptation via
data selection, maybe by providing examples.   

The following remarks concern specific sections:

Section 3.2:
- It could be made clearer how the different vectors (word embeddings, segment
vectors and one-hot vectors) are combined in the model. An illustration of the
architecture would be very helpful. 
- What was the ""designated loss function""?

Section 5.2:
For completeness' sake, it could be mentioned how the system weights were
tuned.",,4,3,Poster,4,5,4,3,4,5,3,3,2016,"Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general-domain data, then it is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data for language model or translation model training. Some early data selection approaches (Zhao et al., 2004; Lü et al., 2007; Moore and Lewis, 2010) use in-domain monolingual data to select monolingual or bilingual data that are similar to the indomain data according to some criterion. By contrast, (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on
3
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
in-domain and out-domain data, respectively. Data weighting approaches weight each data item according to its relevance to the in-domain data. Mixture model adaptation (Foster and Kuhn, 2007; Foster et al., 2010; Sennrich, 2012; Foster et al., 2013) assumes that the general-domain data can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it can be applied to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010). If in-domain comparable data are available, (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) propose mining translations from the comparable data to translate out-of-vocabulary (OOV) words and capture new senses for the new test domains. (Dou and Knight, 2012; Zhang and Zong, 2013) learn bilingual lexical or phrase tables from in-domain monolingual data with a decipherment method, then incorporate them into the SMT system. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper.In a text classification task, key phrases (or ngrams) can help in determining the class of the text, regardless of their locations in the text. For example, the word “desktop” in a sentence may indicate this sentence has computers as its topic; the phrase “not satisfactory” may indicate that the sentiment of the entire sentence is negative. This kind of strong local information about the class of a text can appear in different regions in the input. Convolutional neural networks (CNNs) are useful for text classification because convolutional and
pooling layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Schütze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015), etc. Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned for a specific task. (Johnson and Zhang, 2015b) extends their “one-hot” CNN in (Johnson and Zhang, 2015a) to take region embeddings trained on unlabeled data as CNN input. CNNs that input word embeddings trained on unlabeled data are considered to be semi-supervised CNNs.A CNN is a feed-forward network consisting of convolutional and pooling layers. Each neuron in the convolutional layer of a CNN processes a segment of input signals, which could be a region in an image or a window of words in a sentence. The convolution layer consists of a set of kernels that compute the dot product between different segments of the input. The kernel associated with the l-th segment of the input x computes:
σ(W · wl(x) + b), (1)
where wl(x) ∈ Rq is the input window vector that represents the l-th segment of data. Weight matrix W ∈ Rm×q and bias vector b ∈ Rm are shared by all the kernels in the same layer, and are learned during the training process. Because the convolution kernel allows interaction between different parts of the input, it reduces
4
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
the requirement to select features by hand. Important features in a sentence are automatically selected with pooling, which is a form of non-linear down-sampling. It takes the maximum or the average value observed in each of the d dimension vectors over different windows. As a result, information from multiple d dimension vectors is kept in a single d dimensional vector. At training time, both the weight vectors and the bias vectors are learned with stochastic gradient ascent.When applying CNN to NLP tasks, the first layer of the network takes word embeddings as input. Word embeddings can be pre-trained using tools such as word2vec (Mikolov et al., 2013) or GloV e (Pennington et al., 2014), in which case a table lookup is enough. Alternatively, these vectors can be learned from scratch as a step in the network training process. When there are enough in-domain data, training in-domain word embeddings is meaningful. However, when the in-domain data are limited, the word embeddings learned from these data are unreliable. In this case, the input sentence x can be represented with one-hot vectors where each vector’s length is the vocabulary size, value 1 at index i indicates word i appears in the sentence, and 0 indicates its absence. A CNN with one-hot vector input is called “one-hot CNN” (Johnson and Zhang, 2015a). wl(x) can be either a concatenation of one-hot vectors, in which the order of concatenation is the same as the word order in the sentence, or it can be a bag-of-word/n-gram vector. The bag-of-word (BOW) representation loses word order information but is more robust to data sparsity. In (Johnson and Zhang, 2015a), a CNN whose input being BOW representation is called bow-CNN while input with concatenation of vectors is called seq-CNN. The window size and stride (distance between the window centers) are meta-parameters. σ in Equation 1 is a component-wise non-linear function such as ReLU. Thus, each kernel generates anm-dimensional vector wherem is the number of weight vectors or neurons. These vectors from all the windows of each sentence are aggregated by the pooling layer, by either componentwise maximum (max pooling) or average (average pooling), then used by the top layer as features for classification.Although the size of the in-domain data is normally small, the unlabeled data from general domains are much larger and easier to obtain. To exploit large amounts of unlabeled data, we adopt a semi-supervised learning framework similar to (Johnson and Zhang, 2015b). It first learns word embedding from unlabeled data, then generates the text segment embedding based on these unsupervised word embeddings. Both the one-hot vectors from the labeled data and the segment embeddings from unlabeled data are combined to train the CNN classifier. The word embeddings map each word to a realvalued, dense vector (Bengio et al., 2003). Word embeddings are often learned with an unsupervised learning paradigm: each dimension of the continuous word embeddings aims at capturing a latent feature, reflecting certain syntactic and semantic meanings of the word. A widely used approach for generating useful word embeddings was developed by (Mikolov et al., 2013). This method learns the word embeddings such that the likelihood of generating a word based on its contexts (or generating the context of a given word, aka “skip-gram” model) is maximized. It speeds up the training with the hierarchical softmax strategy and a simplified learning objective, which scales very well to very large training corpora. We adopt the skip-gram model, which intuitively learns a classifier that predicts words conditioned on the central word’s vector representation. An advantage of such distributed representations is that words that have similar contexts, and therefore similar syntactic and semantic properties, will tend to be near one another in the low-dimensional vector space. Given the word embeddings trained from unlabeled data, a sentence is represented as a sequence of d-dimensional vectors, which is the input to a convolution network that generates feature vectors for each text segment. The segment vectors and one-hot vectors are fed into another convolution layer, which outputs the classification labels. The second network is trained with the labeled indomain/out-domain data. Therefore, Equation 1 is replaced with:
σ(W · wl(x) + V · ul(x) + b), (2)
where wl(x) is the one-hot vector obtained from segment l in a sentence, and ul(x) is the embed-
5
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
ding learned from the unlabeled data (general domain training data), applied to the same segment. We train this model with the labeled data. We update the weights W, V, bias b, and the top-layer parameters so that the designated loss function is minimized on the labeled training data.We use the in-domain data from the translation task as positive samples, and randomly select the same number of sentences from the general domain data as negative samples. We train the CNN model on the positive and negative samples with the one-hot CNN or semi-supervised CNN described previously. The trained CNN model is then used to classify the sentence pairs in the general domain data. The sentence pairs with high in-domain scores are selected to train the machine translation system. We classify the source sentence and target sentences separately. The CNN model computes two scores for each sentence pair. The sentence pairs are selected based on the source scores, target scores or the sum of source and target scores. Experiments show that selection based on the sum of the source and target scores achieves the best performance. We empirically determine the number of selected in-domain sentences for each MT system based on experimental results on a separate validation set. When selecting the negative samples, we either randomly sample from the whole data pool, or select from the sentences which have been labeled as negative in the first round classification. Additional experiments show that the results from these two methods are very similar, so we sample the negative samples from the whole general domain for simplicity.Our goal is to adapt the MT system when only a tiny amount of in-domain data is available. So in our experiments, we did not consider any domain information about the training data, such as the source of each corpus. What we have is a small development set (dev) and one or more test sets (test) which are in the same domain.We carried out experiments in four different data settings. All four have large amounts of bilin-
gual training data: 9-15M sentences. The first two involve translation into English (en) from Chinese (zh) and Arabic (ar), while the last two involve translation from English to Spanish (es) and Chinese. The training data are all publicly available, either from LDC1 and transcriptions of TED talks2, where the data are the mixture of newswire, web crawl, UN proceedings and TED talks, etc., or from WMT3, where the data are the mixture of Europarl, web crawl, news-commentary, and UN proceedings, etc. The dev and test sets are “short messages (sms)” for the first task, which are also available from LDC; “tweets” for the second task; publicly available “Facebook post” for the remaining two tasks. The last three tasks are from social media - an intriguing new area of application for MT - where in-domain parallel training data are seldom publicly available. Table 1 summarizes the statistics of the training, dev, and test data for all the test sets.We experiment with two CNN-based data selection strategies:
1. ohcnn: Data selection by supervised one-hot CNN (Section 3.1)
2. sscnn: Data selection by semi-supervised CNN (Section 3.2)
We employ the dev set as in-domain data. All the supervised CNN models are trained with the in-domain dev data as positive examples and an equal number of randomly selected generaldomain sentences as negative examples. All the meta-parameters of the CNN are tuned on heldout data; we generate both bow-regions and seqregions and input them to the CNN. We set the region size to 5 and stride size to 1. The nonlinear function we chose is “ReLU”, the number of weight vectors or neurons is 500. The pooling method is component-wise maximum (max pooling). We use the online available CNN toolkit conText4. To train the general domain word embedding, we used word2vec5. The size of the vector was set to 300. We also generate wordembedding-based bow-regions and seq-regions as additional input to the CNN.
1https://catalog.ldc.upenn.edu/ 2https://wit3.fbk.eu/ 3http://statmt.org/wmt15/ 4http://riejohnson.com/cnn download.html 5https://code.google.com/archive/p/word2vec/
6
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
language zh2en ar2en en2es en2zh test domain sms tweets facebook facebook train origin LDC&TED LDC&TED WMT LDC&TED train size 12.20M 8.97M 15.23M 12.20M dev size 6,016 1,000 800 650 test size 3,282 1,500 3,378 3,343
Table 1: Summary of the data. “sms” means “short message”. “facebook” means “Facebook post”. Data is given as the number of sentence pairs, “M” represents “million”. The tasks “zh2en” and “en2zh” use the same training data.
We compared with four baselines for each task. The first baseline SMT system is trained using all general-domain data. The other three systems are trained with data selected with different LM-based data selection methods as same as in (Duh et al., 2013)6. The four baselines are:
1. alldata: All general-domain data.
2. ngram: Data selection by 3-gram LMs with Witten-Bell 7 smoothing (Axelrod et al., 2011)
3. rnnlm: Data selection by recurrent neural network LM, with the RNNLM Toolkit (Duh et al., 2013)
4. comblm: Data selection by the combined LM using ngram & rnnlm (equal weight) (Duh et al., 2013).
All systems are trained with a standard phrasebased SMT system with standard settings, i.e., GIZA++ alignment, phrase table Kneser-Ney smoothing, hierarchical reordering models, target side 4-gram language model, and “gigaword” 5- gram language model for systems with English as the target language, etc.","We evaluated the system using BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. Table 2 summarizes the results and numbers of the selected sentences for each task. First, we can see that all the data selection methods improved the performance over the baseline “alldata” with much less
6The code and scripts for the three baselines are available at http://cl.naist.jp/ kevinduh/a/acl2013/
7For small amounts of data, Witten-Bell smoothing had performed better than Kneser-Ney smoothing in our experiments
training data (only around 2.5% to 10% of the whole training data). Consistent with (Duh et al., 2013), the three LM based data selection all got improvements, where “rnnlm” obtained better performance than the “ngram” on average. It is not clear that combining the two language model methods (“comblm”) yields further improvement. While the one-hot CNN method “ohcnn” obtained similar improvement as the three LM-based methods on average. The semi-supervised CNN (sscnn) achieved the best performance for all the tasks: its improvements over the “alldata” baseline are 3.1, 1.4, 0.7 and 1.4 BLEU score respectively. It beats “ohcnn” by about 0.5 BLEU point on average. There are two results worth noticing. First, task 1 (zh2en sms task) obtained very high BLEU improvement through data selection. This is because in this task, there is a 120K in-domain subset within the general-domain data. If we train a system on this in-domain data set, we get 25.7 BLEU on the test set. The LM-based methods did not beat this “in-domain data only (indata)” baseline, while the semi-supervised CNN method performed significantly better than this baseline at p < 0.05 level. Second, for the other three tasks, there is no in-domain data component in the general-domain data (that we know of). Even in this case, we achieved up to 1.4 BLEU improvement, which demonstrates the effectiveness of our method: it can select highly suitable in-domain sentences, even when the in-domain data is very limited. In our second experiment, we examine how many labeled samples are needed to train a strong CNN classifier to select theMT training in-domain data. Fixing the number of MT training sentence pairs to 300K that will be selected by the CNN, we reduce the CNN training data from 6,000 down to 100 sentence pairs in steps. The performance of the resulting MT systems for all five data selection
7
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
zh2en ar2en en2es en2zh #sent BLEU #sent BLEU #sent BLEU #sent BLEU
alldata 12.2M 22.9 8.9M 17.6 15.2M 26.8 12.2M 10.0 ngram 300K 25.3** 800K 18.2** 1600K 26.9 400K 10.5* rnnlm 300K 25.6** 800K 18.4** 1600K 27.0 400K 10.5*
comblm 400K 25.7** 800K 18.4** 1400K 27.0 500K 10.4* ohcnn 300K 25.3** 700K 18.2* 1200K 27.1* 400K 11.0**+ sscnn 300K 26.0**+ 700K 19.0**++ 1300K 27.5**++ 300K 11.4**++
Table 2: Summary of the results. Data size is given as number of sentence pairs. The number of selected in-domain sentences is determined by the performance on held-out data. “M” represents million, “K” represents thousand. */** means result is significantly better than the “alldata” baseline at p < 0.05 or p < 0.01 level, respectively. +/++ means result is significantly better than the best LM-based method at p < 0.05 or p < 0.01 level, respectively.
Figure 1: The performance on zh2en sms task with different numbers of in-domain sentences to train LM-based vs. CNN-based classifiers, which are then used to select 300K sentence pairs for MT system training. X-axis is the number of in-domain sentences, Y-axis is BLEU score.
8
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
methods is shown in Figure 1. From Figure 1, we can see that all the data selection methods obtained improvement compared to the “alldata” baseline. When the in-domain training sample is more than 1600 sentence pairs, all the data selection methods obtain reasonable and comparable improvement, while “sscnn” is better than the best LM-based method by 0.3-0.5 BLEU. However, when the in-domain training sample is less than 800 sentence pairs, the difference between the “sscnn” and other methods gets bigger, and CNN-based methods get more stable results than the LM-based methods get. For instance, when the in-domain set increases from 400 to 800, the LM-based methods did not get an improvement; “ngram” and “comblm” even got a small loss on BLEU score. When the in-domain sample is reduced to 100 sentence pairs, the LM-based methods only get a small improvement over the baseline, while the “ohcnn” got a 1.2 BLEU score improvement over the baseline and “sscnn” got a 2.1 BLEU improvement over the baseline. Thus, even if we have no domain knowledge about the training data, when we have only 100 sentences in the test domain, the semi-supervised CNN classifier can still select a good in-domain subset and achieve good performance. We obtained 2.1 BLEU improvement even when we randomly select only 100 in-domain sentence pairs to train the classification model. Is this just because we luckily sampled a good part of the in-domain data? We repeated the “100 in-domain sentence pairs experiment” three times for our most effective method - “sscnn” - by sampling three different in-domain sets from the whole 6,016-sentence dev set. The average BLEU score we got is 25.03, and the standard deviation is 0.12. This means that our algorithm is quite stable even when the in-domain set is very small.","Statistical machine translation (SMT) systems are trained on bilingual parallel and monolingual data. The training corpora typically come from different sources, and vary across topics, genres, dialects, authors’ written styles, etc., which are usually referred as “general domain” training data. Here the word “domain” is often used to indicate some combination of all above and other possible hidden factors (Chen et al., 2013). At run time, the
content to be translated may come from a different domain. Due to the mismatch in “domains”, it is possible to achieve better performance by adapting the SMT system to the test domain (in-domain). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; Lü et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). To train the in-domain language model, a reasonable size in-domain data set, which typically includes several thousands of sentences, is required. In (Axelrod et al., 2011; Duh et al., 2013), the sizes of the in-domain data sets are 30K and over 100K sentences respectively. However, we do not always have access to large or even medium amounts of in-domain data. With the growth of social media, new domains have emerged which need machine translation but
2
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
which have very limited in-domain data, maybe just a few hundred sentence pairs. What’s more, if one wishes to build a large scale topic-specific MT system with hundreds of topics, it is prohibitively expensive to collect tens of thousands of in-domain sentences for each topic.
In this paper, we try to address this challenge, i.e., domain adaptation with very limited amounts of in-domain data. Inspired by the success of convolutional neural networks (CNNs) applied to image and text classification (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), we propose to use CNN to classify training sentence pairs as in-domain or out-of-domain sentences. To overcome the problem of limited in-domain data, we propose to augment the original model with semi-supervised convolutional neural networks for domain classification.
Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolution layers; each computation unit processes a small region of the input data. CNN has been very successful on image classification. When applying it to text input, the convolution layers process small regions of a document, i.e., a sequence of sentences or words. CNN has been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Schütze, 2015; Wang et al., 2015).
In many of these studies, the first layer of the network converts words to word embeddings using table lookup. The word embeddings are either trained as part of CNN training, or pre-trained (thus fixed during model training time) on an additional unlabled corpus. The later is termed semi-supervised CNN. Given tiny amounts of indomain data, the information learned in these pretrained word embeddings is very helpful.
We use a small amount of in-domain data, such as the development set, as the positive sample and randomly select the same number of sentences from the general-domain training data as the negative sample to form the training sample for training the CNN classification model. This is a typical supervised learning setting. To compensate the limit of in-domain data size, we use word2vec (Mikolov et al., 2013) to learn the word embedding from a large amount of general-domain data.
Together with the labeled data, these word embeddings are fed to the convolution layer as additional input to train the final classification model. This is a semi-supervised framework. The learned models are then used to classify each sentence in the general-domain training data based on their domain relevance score. The top N sentence pairs are selected to train the SMT system. We carry out experiments on 4 different language directions with 9-15M sentence pairs in each direction. The test domains include short message (sms), tweets, and Facebook posts, etc. The experimental results show that our method is able to select a small amount of training data that is used to create a system which outperforms baseline systems trained with all the general-domain data. For example, we obtain over 3.1 BLEU improvement on the Chinese-to-English sms task with around 3% of the whole training data. Experiments also show that we can reduce the size of the in-domain sample to around 100 sentences and still obtain a 2.1 BLEU improvement.",,"Why do semi-supervised convolutional neural networks perform so well for data selection? We think there are two main reasons. The first one is that convolution captures the important domain information of the words in the window, and the max-pooling operation combines the vectors which, as a result, focuses on the most important “features” in the sentence. Even a highly domainspecific sentence normally contains both domainspecific words and general-domain words. For
example, in “I have a Dell desktop and a Macbook laptop”, the words “Dell, laptop, Macbook, laptop” are from the computer domain, while the words “I, have, a, and” are general. However, the topic of this sentence is decided by the domain specific words, not the general-domain words. If the properties of the words “Dell, laptop, Macbook, laptop” are kept and highlighted, classification will be more accurate for this sentence. The second reason is the use of word embedding learned from the whole general-domain data. A very important advantage of word embedding is that words that have similar meaning will tend to be grouped together in the vector space. If the word “Lenovo” in the test sentence is not seen in the labeled data, it would be difficult for LMbased models to classify sentences like “I prefer choosing a Lenovo machine” as computer-domain sentence. However, the word embeddings learned from much larger unlabeled data ensure that the word embedding of “Lenovo” is close to that of “Dell”. According to the domain of its neighbor words, the CNN model can still label this sentence as belonging to the computer domain. This property is particularly useful for fast or fine grained adaptation, where obtaining large amount of indomain samples may be slow or too expensive.","Domain adaptation with only a tiny amount of in-domain data is a hard problem. In this paper, we proposed to use a semi-supervised convolutional neural network (CNN) to train the domain classification model, then use the CNN to select the data which is most similar to the test domain. Experiments on large data condition SMT tasks showed that this outperforms state-of-the-art language-model-based data selection methods significantly. Particularly when the size of the indomain data is small, semi-supervised CNN classifier can still select in-domain bilingual sentences to train an adapted SMT system. In future work, we plan to 1) apply this approach to select the data from large size target language corpus, such as the “Gigaword” corpus, for language model training; 2) use the source sentences of the test set to select the data for online dynamic adaptation."
22,"Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors themselves and linear relationships between them.",Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec,132,"A combination of word2vec and LDA could be potentially interesting. The main
problem with the current paper is that the technical details are
incomprehensible. Section 2 needs a complete rewrite so that a reader familiar
with word2vec and LDA could relatively easily get a high-level picture of how
the models are being combined. The current presentation doesn't achieve that.

More detailed comments:

The third paragraph of the introduction makes no sense to me. ""requires
deriving a new approximation"" - approximation of what? why is it time consuming
to develop prototypes? Why is it easier to evaluate features?

Why use the same word vectors for pivot and target (unlike in word2vec)? What's
the motivation for that decision?

what does it mean to separate words from a marginal distribution?

what's co-adaptation?

""If we only included structure up to this point"" - what kind of structure?

""it's similarity"" -> its

Footnote 1 breaks anonymity.

There doesn't appear to be any evaluation. The days when it was ok to just give
some example clusters are long gone in NLP. Figure 2 looks like it might be a
quantitative evaluation, but it's only described in the overly long caption.

The statement in the conclusion that the model solves word analogies is
overstating what was shown, which was just a few cherry-picked examples of king
+ queen etc. sort.

The Chang ref has the conference/journal name as ""Advances in ..."" You'd like
me to guess the venue?",,2,3,Poster,1,5,4,4,2,5,3,3,2016,"This section describes the model for lda2vec. We are interested in modifying the Skipgram Negative-Sampling (SGNS) objective in (Mikolov et al., 2013) to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors. The network architecture is shown in Figure 1.
The total loss term L in (1) is the sum of the Skipgram Negative Sampling Loss (SGNS) Lnegij with the addition of a Dirichlet-likelihood term over document weights, Ld that will be discussed later. The loss is conducted using a context vector, ~cj , pivot word vector ~wj , target word vector ~wi, and negatively-sampled word vector ~wl.
L = Ld + ΣijLnegij (1) Lnegij = log σ(~cj · ~wi) + Σ n l=0 log σ(−~cj · ~wl)
(2)As in Mikolov et al. (2013), pairs of pivot and target words (j, i) are extracted when they co-
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
occur in a moving window scanning across the corpus. In our experiments, the window contains five tokens before and after the pivot token. For every pivot-target pair of words the pivot word is used to predict the nearby target word. Each word is represented with a fixedlength dense distributed-representation vector, but unlike Mikolov et al. (2013) the same word vectors are used in both the pivot and target representations. The SGNS loss shown in (2) attempts to discriminate context-word pairs that appear in the corpus from those randomly sampled from a ‘negative’ pool of words. This loss is minimized when the observed words are completely separated from the marginal distribution. The distribution from which tokens are drawn is uβ , where u denotes the overall word frequency normalized by the total corpus size. Unless stated otherwise, the negative sampling power beta is set to 3/4 and the number of negative samples is fixed to n = 15 as in Mikolov et al. (2013). Note that a distribution of u0.0 would draw negative tokens from the vocabulary with no notion of popularity while a distribution proportional with u1.0 draws from the empirical unigram distribution. Compared to the unigram distribution, the choice of u3/4 slightly emphasizes choosing infrequent words for negative samples. In contrast to optimizing the softmax cross entropy, which requires modelling the overall popularity of each token, negative sampling focuses on learning word vectors conditional on a context by drawing negative samples from each token’s marginal popularity in the corpus.lda2vec embeds both words and document vectors into the same space and trains both representations simultaneously. By adding the pivot and document vectors together, both spaces are effectively joined. Mikolov et al. (2013) provide the intuition that word vectors can be summed together to form a semantically meaningful combination of both words. For example, the vector representation for Germany + airline is similar to the vector for Lufthansa. We would like to exploit the additive property of word vectors to construct a meaningful sum of word and document vectors. For example, if as lda2vec is scanning a document the jth word is Germany, then neighboring words are predicted to be similar such as France, Spain, and Austria. But if the document is specifically
about airlines, then we would like to construct a document vector similar to the word vector for airline. Then instead of predicting tokens similar to Germany alone, predictions similar to both the document and the pivot word can be made such as: Lufthansa, Condor F lugdienst, and Aero Lloyd. Motivated by the meaningful sums of words vectors, in lda2vec the context vector is explicitly designed to be the sum of a document vector and a word vector as in (3):
~cj = ~wj + ~dj (3)
This models document-wide relationships by preserving ~dj for all word-context pairs in a document, while still leveraging local inter-word relationships stemming from the interaction between the pivot word vector ~wj and target word ~wi. The document and word vectors are summed together to form a context vector that intuitively captures long- and short-term themes, respectively. In order to prevent co-adaptation, we also perform dropout on both the unnormalized document vector ~dj and the pivot word vector ~wj (Hinton et al., 2012).If we only included structure up to this point, the model would produce a dense vector for every document. However, lda2vec strives to form interpretable representations and to do so an additional constraint is imposed such that the document representations are similar to those in traditional LDA models. We aim to generate a document vector from a mixture of topic vectors and to do so, we begin by constraining the document vector ~dj to project onto a set of latent topic vectors ~t0, ~t1, ..., ~tk:
~dj = pj0·~t0+pj1·~t1+...+pjk·~tk+...+pjn·~tn (4)
Each weight 0 ≤ pjk ≤ 1 is a fraction that denotes the membership of document j in the topic k. For example, the Twenty Newsgroups model described later has 11313 documents and n = 20 topics so j = 0...11312, k = 0...19. When the word vector dimension is set to 300, it is assumed that the document vectors ~dj , word vectors ~wi and topic vectors ~tk all have dimensionality 300. Note that the topics ~tk are shared and are a common component to all documents but whose strengths are modulated by document weights pjk that are
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
unique to each document. To aid interpretability, the document memberships are designed to be non-negative, and to sum to unity. To achieve this constraint, a softmax transform maps latent vectors initialized in R300 onto the simplex defined by pjk. The softmax transform naturally enforces the constraint that Σkpjk = 1 and allows us interpret memberships as percentages rather than unbounded weights.
Formulating the mixture in (4) as a sum ensures that topic vectors ~tk, document vectors ~dj and word vectors ~wi, operate in the same space. As a result, what words ~wi are most similar to any given topic vector ~tk can be directly calculated. While each topic is not literally a token present in the corpus, it’s similarity to other tokens is meaningful and can be measured. Furthermore, by examining the list of most similar words one can attempt to interpret what the topic represents. For example, by calculating the most similar token to any topic vector (e.g. argmaxi(~t0 · ~wi)) one may discover that the first topic vector ~t0 is similar to the tokens pitching, catcher, and Braves while the second topic vector ~t1 may be similar to Jesus, God, and faith. This provides us the option to interpret the first topic as baseball topic, and as a result the first component in every document proportion pj0 indicates how much document j is in the baseball topic. Similarly, the second topic may be interpreted as Christianity and the second component of any document proportion pj1 indicates the membership of that document in the Christianity topic.Finally, the document weights pij are sparsified by optimizing the document weights with respect to a Dirichlet likelihood with a low concentration parameter α:
Ld = λΣjk (α− 1) log pjk (5)
The overall objective in (5) measures the likelihood of document j in topic k summed over all available documents. The strength of this term is modulated by the tuning parameter λ. This simple likelihood encourages the document proportions coupling in each topic to be sparse when α < 1 and homogeneous when α > 1. To drive interpretability, we are interested in finding sparse memberships and so set α = n−1 where n is the number of topics. We also find that setting
the overall strength of the Dirichlet optimization to λ = 200 works well. Document proportions are initialized to be relatively homogeneous, but as time progresses, the Ld encourages document proportions vectors to become more concentrated (e.g. sparser) over time. In experiments without this sparsity-inducing term (or equivalently when α = 1) the document weights pij tend to have probability mass spread out among all elements. Without any sparsity inducing terms the existence of so many non-zero weights makes interpreting the document vectors difficult. Furthermore, we find that the topic basis are also strongly affected, and the topics become incoherent.The objective in (1) is trained in individual minibatches at a time while using the Adam optimizer (Kingma and Ba, 2014) for two hundred epochs across the dataset. The Dirichlet likelihood term Ld is typically computed over all documents, so in modifying the objective to minibatches we adjust the loss of the term to be proportional to the minibatch size divided by the size of the total corpus. Our software is open source, available online, documented and unit tested1. Finally, the top ten most likely words in a given topic are submitted to the online Palmetto2 topic quality measuring tool and the coherence measure Cv is recorded. After evaluating multiple alternatives, Cv is the recommended coherence metric in Röder et al. (2015). This measure averages the Normalized Pointwise Mutual Information (NPMI) for every pair of words within a sliding window of size 110 on an external corpus and returns mean of the NPMI for the submitted set of words. Token-toword similarity is evaluated using the 3COSMUL measure (Levy and Goldberg, 2014b).This section details experiments in discovering the salient topics in the Twenty Newsgroups dataset, a popular corpus for machine learning on text. Each document in the corpus was posted to one of twenty possible newsgroups. While the text of each post is available to lda2vec, each of the
1The code for lda2vec is available online at https:// github.com/cemoody/lda2vec
2The online evaluation tool can be accessed at http:// palmetto.aksw.org/palmetto-webapp/
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
# of topics β Topic Coherences 20 0.75 0.567 30 0.75 0.555 40 0.75 0.553 50 0.75 0.547 20 1.00 0.563 30 1.00 0.564 40 1.00 0.552 50 1.00 0.558
Figure 2: Average topic coherences found by lda2vec in the Twenty Newsgroups dataset are given. The topic coherence has been demonstrated to correlate with human evaluations of topic models (Röder et al., 2015). The number of topics chosen is given, as well as the negative sampling exponent parameter β. Compared to β = 1.00, β = 0.75 draws more rare words as negative samples. The best topic coherences are found in models n = 20 topics and a β = 0.75.
newsgroup partitions is not revealed to the algorithm but is nevertheless useful for post-hoc qualitative evaluations of the discovered topics. The corpus is preprocessed using the data loader available in Scikit-learn (Pedregosa et al., 2012) and tokens are identified using the SpaCy parser (Honnibal and Johnson, 2015). Words are lemmatized to group multiple inflections into single tokens. Tokens that occur fewer than ten times in the corpus are removed, as are tokens that appear to be URLs, numbers or contain special symbols within their orthographic forms. After preprocessing, the dataset contains 1.8 million observations of 8,946 unique tokens in 11,313 documents. Word vectors are initialized to the pretrained values found in Mikolov et al. (2013) but otherwise updates are allowed to these vectors at training time.
A range of lda2vec parameters are evaluated by varying the number of topics n ∈ 20, 30, 40, 50 and the negative sampling exponent β ∈ 0.75, 1.0. The best topic coherences were achieved with n = 20 topics and with negative sampling power β = 0.75 as summarized in Figure 2. We briefly experimented with variations on dropout ratios but we did not observe any substantial differences.
Figure 3 lists four example topics discovered in the Twenty Newsgroups dataset. Each topic is associated with a topic vector that lives in the same space as the trained word vectors and listed are the most similar words to each topic vector. The
first topic shown has high similarity to the tokens astronomical, Astronomy, satellite, planetary, and telescope and is thus likely a ‘Space’-related topic similar to the ‘sci.space’ newsgroup. The second example topic is similar to words semantically related to ‘Encryption’, such as Clipper and encrypt, and is likely related to the ‘sci.crypt’ newsgroup. The third and four example topics are ‘X Windows’ and ‘Middle East’ which likely belong to the ‘comp.windows.x’ and ‘talk.politics.mideast’ newsgroups.This section evaluates lda2vec on a very large corpus of Hacker News 3 comments. Hacker News is social content-voting website and community whose focus is largely on technology and entrepreneurship. In this corpus, a single document is composed of all of the words in all comments posted to a single article. Only stories with more than 10 comments are included, and only comments from users with more than 10 comments are included. We ignore other metadata such as votes, timestamps, and author identities. The raw dataset 4 is available for download online. The corpus is nearly fifty times the size of the Twenty Newsgroups corpus which is sufficient for learning a specialized vocabulary. To take advantage of this rich corpus, we use the SpaCy to tokenize whole noun phrases and entities at once (Honnibal and Johnson, 2015). The specific tokenization procedure5 is also available online, as are the preprocessed datasets 6 results. This allows us to capture phrases such as community policing measure and prominent figures such as Steve Jobs as single tokens. However, this tokenization procedure generates a vocabulary substantially different from the one available in the Palmetto topic coherence tool and so we do not report topic coherences on this corpus. After preprocessing, the corpus contains 75 million tokens in 66 thousand documents with 110 thousand unique tokens. Unlike the Twenty Newsgroups analysis, word vectors are initialized randomly instead of using a library of pretrained vectors.
3See https://news.ycombinator.com/ 4The raw dataset is freely available at https:// zenodo.org/record/45901 5The tokenization procedure is available online at https://github.com/cemoody/lda2vec/blob/ master/lda2vec/preprocess.py
6A tokenized dataset is freely available at https:// zenodo.org/record/49899
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Topic Label “Space” “Encryption” “X Windows” “Middle East” Top tokens astronomical encryption mydisplay Armenian
Astronomy wiretap xlib Lebanese satellite encrypt window Muslim planetary escrow cursor Turk telescope Clipper pixmap sy
Topic Coherence 0.712 0.675 0.472 0.615
Figure 3: Topics discovered by lda2vec in the Twenty Newsgroups dataset. The inferred topic label is shown in the first row. The tokens with highest similarity to the topic are shown immediately below. Note that the twenty newsgroups corpus contains corresponding newsgroups such as sci.space, sci.crypt, comp.windows.x and talk.politics.mideast.
“Housing Issues” “Internet Portals” “Bitcoin” “Compensation” “Gadget Hardware” more housing DDG. btc current salary the Surface Pro basic income Bing bitcoins more equity HDMI new housing Google+ Mt. Gox vesting glossy screens house prices DDG MtGox equity Mac Pro short-term rentals iGoogle Gox vesting schedule Thunderbolt
Figure 4: Topics discovered by lda2vec in the Hacker News comments dataset. The inferred topic label is shown in the first row. We form tokens from noun phrases to capture the unique vocabulary of this specialized corpus.
Artificial sweeteners
Black holes Comic Sans Functional Programming San Francisco
glucose particles typeface FP New York fructose consciousness Arial Haskell Palo Alto HFCS galaxies Helvetica OOP NYC sugars quantum mechanics Times New Roman functional languages New York City sugar universe font monads SF Soylent dark matter new logo Lisp Mountain View paleo diet Big Bang Anonymous Pro Clojure Seattle diet planets Baskerville category theory Los Angeles carbohydrates entanglement serif font OO Boston
Figure 5: Given an example token in the top row, the most similar words available in the Hacker News comments corpus are reported.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
We train an lda2vec model using 40 topics and 256 hidden units and report the learned topics that demonstrate the themes present in the corpus. Furthermore, we demonstrate that word vectors and semantic relationships specific to this corpus are learned.
In Figure 4 five example topics discovered by lda2vec in the Hacker News corpus are listed. These topics demonstrate that the major themes of the corpus are reproduced and represented in learned topic vectors in a similar fashion as in LDA (Blei et al., 2003). The first, which we hand-label Housing Issues has prominent tokens relating to housing policy issues such as housing supply (e.g. more housing), and costs (e.g. basic income and house prices). Another topic lists major internet portals, such as the privacyconscious search engine ‘Duck Duck Go’ (in the corpus abbreviated as DDG), as well as other major search engines (e.g. Bing), and home pages (e.g. Google+, and iGoogle). A third topic is that of the popular online curency and payment system Bitcoin, the abbreviated form of the currency btc, and the now-defunct Bitcoin trading platform Mt. Gox. A fourth topic considers salaries and compensation with tokens such as current salary, more equity and vesting, the process by which employees secure stock from their employers. A fifth example topic is that of technological hardware like HDMI and glossy screens and includes devices such as the Surface Pro and Mac Pro.
Figure 5 demonstrates that token similarities are learned in a similar fashion as in SGNS (Mikolov et al., 2013) but specialized to the Hacker News corpus. Tokens similar to the token Artificial sweeteners include other sugar-related tokens like fructose and food-related tokens such as paleo diet. Tokens similar to Black holes include physics-related concepts such as galaxies and dark matter. The Hacker News corpus devotes a substantial quantity of text to fonts and design, and the words most similar to Comic Sans are other popular fonts (e.g. Times New Roman and Helvetica) as well as font-related concepts such as typeface and serif font. Tokens similar to Functional Programming demonstrate similarity to other computer science-related tokens while tokens similar to San Francisco include other large American cities as well smaller cities located in the San Francisco Bay Area.
Figure 6 demonstrates that in addition to learn-
ing topics over documents and similarities to word tokens, linear regularities between tokens are also learned. The ‘Query’ column lists a selection of tokens that when combined yield a token vector closest to the token shown in the ‘Result’ column. The subtractions and additions of vectors are evaluated literally, but instead take advantage of the 3COSMUL objective (Levy and Goldberg, 2014b). The results show that relationships between tokens important to the Hacker News community exists between the token vectors. For example, the vector for Silicon Valley is similar to both California and technology, Bitcoin is indeed a digital currency, Node.js is a technology that enables running Javascript on servers instead of on clientside browsers, Jeff Bezos and Mark Zuckerberg are CEOs of Amazon and Facebook respectively, NLP and computer vision are fields of machine learning research primarily dealing with text and images respectively, Edward Snowden and Julian Assange are both whistleblowers who were primarily located in the United States and Sweden and finally the Kindle and the Surface Pro are both tablets manufactured by Amazon and Microsoft respectively. In the above examples semantic relationships between tokens encode for attributes and features including: location, currencies, server v.s. client, leadership figures, machine learning fields, political figures, nationalities, companies and hardware.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799",,"Topic models are popular for their ability to organize document collections into a smaller set of prominent themes. In contrast to dense distributed representations, these document and topic representations are generally accessible to humans and more easily lend themselves to being interpreted. This interpretability provides additional options to highlight the patterns and structures within our systems of documents. For example, using Latent Dirichlet Allocation (LDA) topic models can reveal cluster of words within documents (Blei et al., 2003), highlight temporal trends (Charlin et al., 2015), and infer networks of complementary products (McAuley et al., 2015). See Blei et al. (2010) for an overview of topic modelling in do-
mains as diverse as computer vision, genetic markers, survey data, and social network data.
Dense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al. (2015) build vectors that reconstruct the sentence sequences before and after a given sentence, and Ghosh et al. (2016) construct contextual LSTMs that predict proceeding sentence features. Probabilistic topic models tend to form documents as a sparse mixed-membership of topics while neural network models tend to model documents as dense vectors. By virtue of both their sparsity and low-dimensionality, representations from the former are simpler to inspect and more immediately yield high level intuitions about the underlying system (although not without hazards, see Chang et al. (2009)). This paper explores hybrid approaches mixing sparse document representations with dense word and topic vectors.
Unfortunately, crafting a new probabilistic topic model requires deriving a new approximation, a procedure which takes substantial expertise and must be customized to every model. As a result, prototypes are time-consuming to develop and changes to model architectures must be carefully considered. However, with modern automatic differentiation frameworks the practitioner can focus development time on the model design rather than the model approximations. This expedites the process of evaluating which model features are relevant. This work takes advantage of the Chainer (Tokui et al., 2015) framework to quickly develop models while also enabling us to utilize GPUs to dramatically improve computational speed.
Finally, traditional topic models over text do not take advantage of recent advances in distributed word representations which can capture semantically meaningful regularities between tokens. The
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
0.34 -0.1 0.17
41% 26% 34%
-1.4 -0.5 -1.4
-1.9-1.7 0.75
0.96-0.7 -1.9
-0.2-1.1 0.6
-1.2-1.2 -0.2
-0.7 -0.4 -0.7 -0.3 -0.3-1.9 0.85 -0.6 -0.3 -0.5
-2.6 0.45 -1.3 -0.6 -0.8
sally said the fox jumped over the
sally said the fox jumped over the
#topics
#topics
fox
# hidden units
#topics
#hidden units#hidden units
#hidden units
Skip grams from sentences
34% 32% 34%
t=0
Word vector
Negative sampling loss
Topic matrix
Document proportion
Document weight
Document vector
Context vector
Sparse document proportions
x
+
41% 26% 34%
t=10
99% 1% 0%
t=∞
time
Figure 1: lda2vec builds representations over both words and documents by mixing word2vec’s skipgram architecture with Dirichlet-optimized sparse topic mixtures. The various components and transformations present in the diagram are described in the text.
examination of word co-occurrences has proven to be a fruitful research paradigm. For example, Mikolov et al. (2013) utilize Skipgram NegativeSampling (SGNS) to train word embeddings using word-context pairs formed from windows moving across a text corpus. These vector representations ultimately encode remarkable linearities such as king − man + woman = queen. In fact, Levy and Goldberg (2014c) demonstrate that this is implicitly factorizing a variant of the Pointwise Mutual Information (PMI) matrix that emphasizes predicting frequent co-occurrences over rare ones. Closely related to the PMI matrix, Pennington et al. (2014) factorize a large global word count co-occurrence matrix to yield more efficient and slightly more performant computed embeddings than SGNS. Once created, these representations are then useful for information retrieval (Manning et al., 2009) and parsing tasks (Levy and Goldberg, 2014a). In this work, we will take advantage of word-level representations to build document-level abstractions.
This paper extends distributed word representations by including interpretable document representations and demonstrate that model inference can be performed and extended within the framework of automatic differentiation.",,,"This work demonstrates a simple model, lda2vec, that extends SGNS (Mikolov et al., 2013) to build unsupervised document representations that yield coherent topics. Word, topic, and document vectors are jointly trained and embedded in a common representation space that preserves semantic regularities between the learned word vectors while still yielding sparse and interpretable documentto-topic proportions in the style of LDA (Blei et al., 2003). Topics formed in the Twenty Newsgroups corpus yield high mean topic coherences which have been shown to correlate with human evaluations of topics (Röder et al., 2015). When applied to a Hacker News comments corpus, lda2vec discovers the salient topics within this community and learns linear relationships between words that allow it solve word analogies in the specialized vocabulary of this corpus. Finally, we note that our method is simple to implement in automatic differentiation frameworks and can lead to more readily interpretable unsupervised representations."
23,"Distributed dense word vectors have been shown to be effective at capturing token-level semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors themselves and linear relationships between them.",Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec,132,"This paper proposes a neural-styled topic model, extending the objective of
word2vec to also learn document embeddings, which it then constrains through
sparsification, hence mimicking the output of a topic model.

I really liked the model that the authors proposed, and found the examples
presented by the authors to be highly promising. What was really missing from
the paper, however, was any empirical evaluation of the model -- evaluation
entirely falls back on tables of examples, without any indication of how
representative the examples are, or any attempt to directly compare with
standard or neural topic models. Without empirical evaluation, it is
impossible to get a sense of the true worth of the model, making it very hard
to accept the paper. Some ideas of how the authors could have achieved this:
(1) use the topic representation of each document in a supervised document
categorisation setup to compare against a topic model with the same topic
cardinality (i.e. as an indirect evaluation of the quality of the
representation); or (2) through direct evaluation over a dataset with document
similarity annotations (based on pairwise comparison over topic vectors).

It's fantastic that you are releasing code, but you have compromised anonymity
in publishing the github link in the submitted version of the paper (strictly
speaking, this is sufficient for the paper to be rejected outright, but I
leave that up to the PCs)

Other issues:

- how did you select the examples in Figures 3-6? presenting a subset of the
  actual topics etc. potentially reeks of cherry picking.

- in Section 2.2.1 you discuss the possibility of calculating word
  representations for topics based on pairwise comparison with each word in
  the vocabulary, but this is going to be an extremely expensive process for a
  reasonable vocab size and number of topics; is this really feasible?

- you say that you identify ""tokens"" using SpaCy in Section 3.1 -- how? You
  extract noun chunks (but not any other chunk type), similarly to the Section
  3.2, or something else? Given that you go on to say that you use word2vec
  pre-trained embeddings (which include only small numbers of multiword
  terms), it wasn't clear what you were doing here.

- how does your model deal with OOV terms? Yes, in the experiments you report
  in the paper you appear to train the model over the entire document
  collection so it perhaps isn't an immediate problem, but there will be
  contexts where you want to apply the trained model to novel documents, in
  which case the updating of the word2vec token embeddings is going to mean
  that any non-updated (OOV, relative to the training collection) word2vec
  embeddings are not going to be directly comparable to the tuned embeddings.

- the finding that 20 topics worked best over the 20 Newsgroups corpus wasn't
  surprising given its composition. Possibly another (very simple) form of
  evaluation here could have been based on some information-theoretic
  comparison relative to the true document labels, where again you would have
  been able to perform a direct comparison with LDA etc.

- a couple of other neural topic models that you meed to compare yourself with
  are:

Cao, Ziqiang, Sujian Li, Yang Liu, Wenjie Li, and Heng Ji. ""A Novel Neural
Topic Model and Its Supervised Extension."" In AAAI, pp. 2210-2216. 2015.

Nguyen, Dat Quoc, Richard Billingsley, Lan Du, and Mark Johnson. ""Improving
Topic Models with Latent Feature Word Representations."" Transactions of the
Association for Computational Linguistics 3 (2015): 299-313.

Shamanta, Debakar, Sheikh Motahar Naim, Parang Saraf, Naren Ramakrishnan, and
M. Shahriar Hossain. ""Concurrent Inference of Topic Models and Distributed
Vector Representations."" In Machine Learning and Knowledge Discovery in
Databases, pp. 441-457. Springer International Publishing, 2015.

Low-level things:

line 315: ""it's similarity"" -> ""its similarity""

line 361: what does it mean for the ""topic basis"" to be affected (and the
""are"" is awkward here)

- in the caption of Figure 5, the examples should perhaps be ""terms"" rather
  than ""words""

- the reference formatting is all over the place, e.g. ""Advances in ..."",
  ""Advances in Neural ..."", Roder et al. is missing the conference name, etc.",,2,4,Oral Presentation,4,3,3,4,4,5,4,3,2016,"This section describes the model for lda2vec. We are interested in modifying the Skipgram Negative-Sampling (SGNS) objective in (Mikolov et al., 2013) to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors. The network architecture is shown in Figure 1.
The total loss term L in (1) is the sum of the Skipgram Negative Sampling Loss (SGNS) Lnegij with the addition of a Dirichlet-likelihood term over document weights, Ld that will be discussed later. The loss is conducted using a context vector, ~cj , pivot word vector ~wj , target word vector ~wi, and negatively-sampled word vector ~wl.
L = Ld + ΣijLnegij (1) Lnegij = log σ(~cj · ~wi) + Σ n l=0 log σ(−~cj · ~wl)
(2)As in Mikolov et al. (2013), pairs of pivot and target words (j, i) are extracted when they co-
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
occur in a moving window scanning across the corpus. In our experiments, the window contains five tokens before and after the pivot token. For every pivot-target pair of words the pivot word is used to predict the nearby target word. Each word is represented with a fixedlength dense distributed-representation vector, but unlike Mikolov et al. (2013) the same word vectors are used in both the pivot and target representations. The SGNS loss shown in (2) attempts to discriminate context-word pairs that appear in the corpus from those randomly sampled from a ‘negative’ pool of words. This loss is minimized when the observed words are completely separated from the marginal distribution. The distribution from which tokens are drawn is uβ , where u denotes the overall word frequency normalized by the total corpus size. Unless stated otherwise, the negative sampling power beta is set to 3/4 and the number of negative samples is fixed to n = 15 as in Mikolov et al. (2013). Note that a distribution of u0.0 would draw negative tokens from the vocabulary with no notion of popularity while a distribution proportional with u1.0 draws from the empirical unigram distribution. Compared to the unigram distribution, the choice of u3/4 slightly emphasizes choosing infrequent words for negative samples. In contrast to optimizing the softmax cross entropy, which requires modelling the overall popularity of each token, negative sampling focuses on learning word vectors conditional on a context by drawing negative samples from each token’s marginal popularity in the corpus.lda2vec embeds both words and document vectors into the same space and trains both representations simultaneously. By adding the pivot and document vectors together, both spaces are effectively joined. Mikolov et al. (2013) provide the intuition that word vectors can be summed together to form a semantically meaningful combination of both words. For example, the vector representation for Germany + airline is similar to the vector for Lufthansa. We would like to exploit the additive property of word vectors to construct a meaningful sum of word and document vectors. For example, if as lda2vec is scanning a document the jth word is Germany, then neighboring words are predicted to be similar such as France, Spain, and Austria. But if the document is specifically
about airlines, then we would like to construct a document vector similar to the word vector for airline. Then instead of predicting tokens similar to Germany alone, predictions similar to both the document and the pivot word can be made such as: Lufthansa, Condor F lugdienst, and Aero Lloyd. Motivated by the meaningful sums of words vectors, in lda2vec the context vector is explicitly designed to be the sum of a document vector and a word vector as in (3):
~cj = ~wj + ~dj (3)
This models document-wide relationships by preserving ~dj for all word-context pairs in a document, while still leveraging local inter-word relationships stemming from the interaction between the pivot word vector ~wj and target word ~wi. The document and word vectors are summed together to form a context vector that intuitively captures long- and short-term themes, respectively. In order to prevent co-adaptation, we also perform dropout on both the unnormalized document vector ~dj and the pivot word vector ~wj (Hinton et al., 2012).If we only included structure up to this point, the model would produce a dense vector for every document. However, lda2vec strives to form interpretable representations and to do so an additional constraint is imposed such that the document representations are similar to those in traditional LDA models. We aim to generate a document vector from a mixture of topic vectors and to do so, we begin by constraining the document vector ~dj to project onto a set of latent topic vectors ~t0, ~t1, ..., ~tk:
~dj = pj0·~t0+pj1·~t1+...+pjk·~tk+...+pjn·~tn (4)
Each weight 0 ≤ pjk ≤ 1 is a fraction that denotes the membership of document j in the topic k. For example, the Twenty Newsgroups model described later has 11313 documents and n = 20 topics so j = 0...11312, k = 0...19. When the word vector dimension is set to 300, it is assumed that the document vectors ~dj , word vectors ~wi and topic vectors ~tk all have dimensionality 300. Note that the topics ~tk are shared and are a common component to all documents but whose strengths are modulated by document weights pjk that are
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
unique to each document. To aid interpretability, the document memberships are designed to be non-negative, and to sum to unity. To achieve this constraint, a softmax transform maps latent vectors initialized in R300 onto the simplex defined by pjk. The softmax transform naturally enforces the constraint that Σkpjk = 1 and allows us interpret memberships as percentages rather than unbounded weights.
Formulating the mixture in (4) as a sum ensures that topic vectors ~tk, document vectors ~dj and word vectors ~wi, operate in the same space. As a result, what words ~wi are most similar to any given topic vector ~tk can be directly calculated. While each topic is not literally a token present in the corpus, it’s similarity to other tokens is meaningful and can be measured. Furthermore, by examining the list of most similar words one can attempt to interpret what the topic represents. For example, by calculating the most similar token to any topic vector (e.g. argmaxi(~t0 · ~wi)) one may discover that the first topic vector ~t0 is similar to the tokens pitching, catcher, and Braves while the second topic vector ~t1 may be similar to Jesus, God, and faith. This provides us the option to interpret the first topic as baseball topic, and as a result the first component in every document proportion pj0 indicates how much document j is in the baseball topic. Similarly, the second topic may be interpreted as Christianity and the second component of any document proportion pj1 indicates the membership of that document in the Christianity topic.Finally, the document weights pij are sparsified by optimizing the document weights with respect to a Dirichlet likelihood with a low concentration parameter α:
Ld = λΣjk (α− 1) log pjk (5)
The overall objective in (5) measures the likelihood of document j in topic k summed over all available documents. The strength of this term is modulated by the tuning parameter λ. This simple likelihood encourages the document proportions coupling in each topic to be sparse when α < 1 and homogeneous when α > 1. To drive interpretability, we are interested in finding sparse memberships and so set α = n−1 where n is the number of topics. We also find that setting
the overall strength of the Dirichlet optimization to λ = 200 works well. Document proportions are initialized to be relatively homogeneous, but as time progresses, the Ld encourages document proportions vectors to become more concentrated (e.g. sparser) over time. In experiments without this sparsity-inducing term (or equivalently when α = 1) the document weights pij tend to have probability mass spread out among all elements. Without any sparsity inducing terms the existence of so many non-zero weights makes interpreting the document vectors difficult. Furthermore, we find that the topic basis are also strongly affected, and the topics become incoherent.The objective in (1) is trained in individual minibatches at a time while using the Adam optimizer (Kingma and Ba, 2014) for two hundred epochs across the dataset. The Dirichlet likelihood term Ld is typically computed over all documents, so in modifying the objective to minibatches we adjust the loss of the term to be proportional to the minibatch size divided by the size of the total corpus. Our software is open source, available online, documented and unit tested1. Finally, the top ten most likely words in a given topic are submitted to the online Palmetto2 topic quality measuring tool and the coherence measure Cv is recorded. After evaluating multiple alternatives, Cv is the recommended coherence metric in Röder et al. (2015). This measure averages the Normalized Pointwise Mutual Information (NPMI) for every pair of words within a sliding window of size 110 on an external corpus and returns mean of the NPMI for the submitted set of words. Token-toword similarity is evaluated using the 3COSMUL measure (Levy and Goldberg, 2014b).This section details experiments in discovering the salient topics in the Twenty Newsgroups dataset, a popular corpus for machine learning on text. Each document in the corpus was posted to one of twenty possible newsgroups. While the text of each post is available to lda2vec, each of the
1The code for lda2vec is available online at https:// github.com/cemoody/lda2vec
2The online evaluation tool can be accessed at http:// palmetto.aksw.org/palmetto-webapp/
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
# of topics β Topic Coherences 20 0.75 0.567 30 0.75 0.555 40 0.75 0.553 50 0.75 0.547 20 1.00 0.563 30 1.00 0.564 40 1.00 0.552 50 1.00 0.558
Figure 2: Average topic coherences found by lda2vec in the Twenty Newsgroups dataset are given. The topic coherence has been demonstrated to correlate with human evaluations of topic models (Röder et al., 2015). The number of topics chosen is given, as well as the negative sampling exponent parameter β. Compared to β = 1.00, β = 0.75 draws more rare words as negative samples. The best topic coherences are found in models n = 20 topics and a β = 0.75.
newsgroup partitions is not revealed to the algorithm but is nevertheless useful for post-hoc qualitative evaluations of the discovered topics. The corpus is preprocessed using the data loader available in Scikit-learn (Pedregosa et al., 2012) and tokens are identified using the SpaCy parser (Honnibal and Johnson, 2015). Words are lemmatized to group multiple inflections into single tokens. Tokens that occur fewer than ten times in the corpus are removed, as are tokens that appear to be URLs, numbers or contain special symbols within their orthographic forms. After preprocessing, the dataset contains 1.8 million observations of 8,946 unique tokens in 11,313 documents. Word vectors are initialized to the pretrained values found in Mikolov et al. (2013) but otherwise updates are allowed to these vectors at training time.
A range of lda2vec parameters are evaluated by varying the number of topics n ∈ 20, 30, 40, 50 and the negative sampling exponent β ∈ 0.75, 1.0. The best topic coherences were achieved with n = 20 topics and with negative sampling power β = 0.75 as summarized in Figure 2. We briefly experimented with variations on dropout ratios but we did not observe any substantial differences.
Figure 3 lists four example topics discovered in the Twenty Newsgroups dataset. Each topic is associated with a topic vector that lives in the same space as the trained word vectors and listed are the most similar words to each topic vector. The
first topic shown has high similarity to the tokens astronomical, Astronomy, satellite, planetary, and telescope and is thus likely a ‘Space’-related topic similar to the ‘sci.space’ newsgroup. The second example topic is similar to words semantically related to ‘Encryption’, such as Clipper and encrypt, and is likely related to the ‘sci.crypt’ newsgroup. The third and four example topics are ‘X Windows’ and ‘Middle East’ which likely belong to the ‘comp.windows.x’ and ‘talk.politics.mideast’ newsgroups.This section evaluates lda2vec on a very large corpus of Hacker News 3 comments. Hacker News is social content-voting website and community whose focus is largely on technology and entrepreneurship. In this corpus, a single document is composed of all of the words in all comments posted to a single article. Only stories with more than 10 comments are included, and only comments from users with more than 10 comments are included. We ignore other metadata such as votes, timestamps, and author identities. The raw dataset 4 is available for download online. The corpus is nearly fifty times the size of the Twenty Newsgroups corpus which is sufficient for learning a specialized vocabulary. To take advantage of this rich corpus, we use the SpaCy to tokenize whole noun phrases and entities at once (Honnibal and Johnson, 2015). The specific tokenization procedure5 is also available online, as are the preprocessed datasets 6 results. This allows us to capture phrases such as community policing measure and prominent figures such as Steve Jobs as single tokens. However, this tokenization procedure generates a vocabulary substantially different from the one available in the Palmetto topic coherence tool and so we do not report topic coherences on this corpus. After preprocessing, the corpus contains 75 million tokens in 66 thousand documents with 110 thousand unique tokens. Unlike the Twenty Newsgroups analysis, word vectors are initialized randomly instead of using a library of pretrained vectors.
3See https://news.ycombinator.com/ 4The raw dataset is freely available at https:// zenodo.org/record/45901 5The tokenization procedure is available online at https://github.com/cemoody/lda2vec/blob/ master/lda2vec/preprocess.py
6A tokenized dataset is freely available at https:// zenodo.org/record/49899
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Topic Label “Space” “Encryption” “X Windows” “Middle East” Top tokens astronomical encryption mydisplay Armenian
Astronomy wiretap xlib Lebanese satellite encrypt window Muslim planetary escrow cursor Turk telescope Clipper pixmap sy
Topic Coherence 0.712 0.675 0.472 0.615
Figure 3: Topics discovered by lda2vec in the Twenty Newsgroups dataset. The inferred topic label is shown in the first row. The tokens with highest similarity to the topic are shown immediately below. Note that the twenty newsgroups corpus contains corresponding newsgroups such as sci.space, sci.crypt, comp.windows.x and talk.politics.mideast.
“Housing Issues” “Internet Portals” “Bitcoin” “Compensation” “Gadget Hardware” more housing DDG. btc current salary the Surface Pro basic income Bing bitcoins more equity HDMI new housing Google+ Mt. Gox vesting glossy screens house prices DDG MtGox equity Mac Pro short-term rentals iGoogle Gox vesting schedule Thunderbolt
Figure 4: Topics discovered by lda2vec in the Hacker News comments dataset. The inferred topic label is shown in the first row. We form tokens from noun phrases to capture the unique vocabulary of this specialized corpus.
Artificial sweeteners
Black holes Comic Sans Functional Programming San Francisco
glucose particles typeface FP New York fructose consciousness Arial Haskell Palo Alto HFCS galaxies Helvetica OOP NYC sugars quantum mechanics Times New Roman functional languages New York City sugar universe font monads SF Soylent dark matter new logo Lisp Mountain View paleo diet Big Bang Anonymous Pro Clojure Seattle diet planets Baskerville category theory Los Angeles carbohydrates entanglement serif font OO Boston
Figure 5: Given an example token in the top row, the most similar words available in the Hacker News comments corpus are reported.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
We train an lda2vec model using 40 topics and 256 hidden units and report the learned topics that demonstrate the themes present in the corpus. Furthermore, we demonstrate that word vectors and semantic relationships specific to this corpus are learned.
In Figure 4 five example topics discovered by lda2vec in the Hacker News corpus are listed. These topics demonstrate that the major themes of the corpus are reproduced and represented in learned topic vectors in a similar fashion as in LDA (Blei et al., 2003). The first, which we hand-label Housing Issues has prominent tokens relating to housing policy issues such as housing supply (e.g. more housing), and costs (e.g. basic income and house prices). Another topic lists major internet portals, such as the privacyconscious search engine ‘Duck Duck Go’ (in the corpus abbreviated as DDG), as well as other major search engines (e.g. Bing), and home pages (e.g. Google+, and iGoogle). A third topic is that of the popular online curency and payment system Bitcoin, the abbreviated form of the currency btc, and the now-defunct Bitcoin trading platform Mt. Gox. A fourth topic considers salaries and compensation with tokens such as current salary, more equity and vesting, the process by which employees secure stock from their employers. A fifth example topic is that of technological hardware like HDMI and glossy screens and includes devices such as the Surface Pro and Mac Pro.
Figure 5 demonstrates that token similarities are learned in a similar fashion as in SGNS (Mikolov et al., 2013) but specialized to the Hacker News corpus. Tokens similar to the token Artificial sweeteners include other sugar-related tokens like fructose and food-related tokens such as paleo diet. Tokens similar to Black holes include physics-related concepts such as galaxies and dark matter. The Hacker News corpus devotes a substantial quantity of text to fonts and design, and the words most similar to Comic Sans are other popular fonts (e.g. Times New Roman and Helvetica) as well as font-related concepts such as typeface and serif font. Tokens similar to Functional Programming demonstrate similarity to other computer science-related tokens while tokens similar to San Francisco include other large American cities as well smaller cities located in the San Francisco Bay Area.
Figure 6 demonstrates that in addition to learn-
ing topics over documents and similarities to word tokens, linear regularities between tokens are also learned. The ‘Query’ column lists a selection of tokens that when combined yield a token vector closest to the token shown in the ‘Result’ column. The subtractions and additions of vectors are evaluated literally, but instead take advantage of the 3COSMUL objective (Levy and Goldberg, 2014b). The results show that relationships between tokens important to the Hacker News community exists between the token vectors. For example, the vector for Silicon Valley is similar to both California and technology, Bitcoin is indeed a digital currency, Node.js is a technology that enables running Javascript on servers instead of on clientside browsers, Jeff Bezos and Mark Zuckerberg are CEOs of Amazon and Facebook respectively, NLP and computer vision are fields of machine learning research primarily dealing with text and images respectively, Edward Snowden and Julian Assange are both whistleblowers who were primarily located in the United States and Sweden and finally the Kindle and the Surface Pro are both tablets manufactured by Amazon and Microsoft respectively. In the above examples semantic relationships between tokens encode for attributes and features including: location, currencies, server v.s. client, leadership figures, machine learning fields, political figures, nationalities, companies and hardware.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799",,"Topic models are popular for their ability to organize document collections into a smaller set of prominent themes. In contrast to dense distributed representations, these document and topic representations are generally accessible to humans and more easily lend themselves to being interpreted. This interpretability provides additional options to highlight the patterns and structures within our systems of documents. For example, using Latent Dirichlet Allocation (LDA) topic models can reveal cluster of words within documents (Blei et al., 2003), highlight temporal trends (Charlin et al., 2015), and infer networks of complementary products (McAuley et al., 2015). See Blei et al. (2010) for an overview of topic modelling in do-
mains as diverse as computer vision, genetic markers, survey data, and social network data.
Dense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al. (2015) build vectors that reconstruct the sentence sequences before and after a given sentence, and Ghosh et al. (2016) construct contextual LSTMs that predict proceeding sentence features. Probabilistic topic models tend to form documents as a sparse mixed-membership of topics while neural network models tend to model documents as dense vectors. By virtue of both their sparsity and low-dimensionality, representations from the former are simpler to inspect and more immediately yield high level intuitions about the underlying system (although not without hazards, see Chang et al. (2009)). This paper explores hybrid approaches mixing sparse document representations with dense word and topic vectors.
Unfortunately, crafting a new probabilistic topic model requires deriving a new approximation, a procedure which takes substantial expertise and must be customized to every model. As a result, prototypes are time-consuming to develop and changes to model architectures must be carefully considered. However, with modern automatic differentiation frameworks the practitioner can focus development time on the model design rather than the model approximations. This expedites the process of evaluating which model features are relevant. This work takes advantage of the Chainer (Tokui et al., 2015) framework to quickly develop models while also enabling us to utilize GPUs to dramatically improve computational speed.
Finally, traditional topic models over text do not take advantage of recent advances in distributed word representations which can capture semantically meaningful regularities between tokens. The
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
0.34 -0.1 0.17
41% 26% 34%
-1.4 -0.5 -1.4
-1.9-1.7 0.75
0.96-0.7 -1.9
-0.2-1.1 0.6
-1.2-1.2 -0.2
-0.7 -0.4 -0.7 -0.3 -0.3-1.9 0.85 -0.6 -0.3 -0.5
-2.6 0.45 -1.3 -0.6 -0.8
sally said the fox jumped over the
sally said the fox jumped over the
#topics
#topics
fox
# hidden units
#topics
#hidden units#hidden units
#hidden units
Skip grams from sentences
34% 32% 34%
t=0
Word vector
Negative sampling loss
Topic matrix
Document proportion
Document weight
Document vector
Context vector
Sparse document proportions
x
+
41% 26% 34%
t=10
99% 1% 0%
t=∞
time
Figure 1: lda2vec builds representations over both words and documents by mixing word2vec’s skipgram architecture with Dirichlet-optimized sparse topic mixtures. The various components and transformations present in the diagram are described in the text.
examination of word co-occurrences has proven to be a fruitful research paradigm. For example, Mikolov et al. (2013) utilize Skipgram NegativeSampling (SGNS) to train word embeddings using word-context pairs formed from windows moving across a text corpus. These vector representations ultimately encode remarkable linearities such as king − man + woman = queen. In fact, Levy and Goldberg (2014c) demonstrate that this is implicitly factorizing a variant of the Pointwise Mutual Information (PMI) matrix that emphasizes predicting frequent co-occurrences over rare ones. Closely related to the PMI matrix, Pennington et al. (2014) factorize a large global word count co-occurrence matrix to yield more efficient and slightly more performant computed embeddings than SGNS. Once created, these representations are then useful for information retrieval (Manning et al., 2009) and parsing tasks (Levy and Goldberg, 2014a). In this work, we will take advantage of word-level representations to build document-level abstractions.
This paper extends distributed word representations by including interpretable document representations and demonstrate that model inference can be performed and extended within the framework of automatic differentiation.",,,"This work demonstrates a simple model, lda2vec, that extends SGNS (Mikolov et al., 2013) to build unsupervised document representations that yield coherent topics. Word, topic, and document vectors are jointly trained and embedded in a common representation space that preserves semantic regularities between the learned word vectors while still yielding sparse and interpretable documentto-topic proportions in the style of LDA (Blei et al., 2003). Topics formed in the Twenty Newsgroups corpus yield high mean topic coherences which have been shown to correlate with human evaluations of topics (Röder et al., 2015). When applied to a Hacker News comments corpus, lda2vec discovers the salient topics within this community and learns linear relationships between words that allow it solve word analogies in the specialized vocabulary of this corpus. Finally, we note that our method is simple to implement in automatic differentiation frameworks and can lead to more readily interpretable unsupervised representations."
24,"Non-compositional phrases such as \emph{red herring} and weakly compositional phrases such as \emph{spelling bee} are an integral part of natural language.  They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics.  Compositionality detection therefore provides a good testbed for compositional methods. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the ad-hoc compositional approach of applying simple composition operations to state-of-the-art neural embeddings.",When a Red Herring is Not a Red Herring: Using Compositional Methods to Improve the Detection of Non-Compositional Phrases,137,"General comments
=============================
The paper reports experiments on predicting the level of compositionality of
compounds in English. 
The dataset used is a previously existing set of 90 compounds, whose
compositionality was ranked from 1 to 5
(by a non specified number of judges).
The general form of each experiment is to compute a cosine similarity between
the vector of the compound (treated as one token) and a composition of the
vectors of the components.
Evaluation is performed using a Spearman correlation between the cosine
similarity and the human judgments.

The experiments vary
- for the vectors used: neural embeddings versus syntactic-context count
vectors
- and for the latter case, whether plain or ""aligned"" vectors should be used,
for the dependent component of the compound. The alignment tries to capture a
shift from the dependent to the head. Alignment were proposed in a previous
suppressed reference.

The results indicate that syntactic-context count vectors outperform
embeddings, and the use of aligned alone performs less well than non-modified
vectors, and a highly-tuned combination of aligned and unaligned vectors
provides a slight improvement.

Regarding the form of the paper, I found the introduction quite well written,
but other parts (like section 5.1) are difficult to read, although the
underlying notions are not very complicated. Rephrasing with running examples
could help.

Regarding the substance, I have several concerns:

- the innovation with respect to Reddy et al. seems to be the use of the
aligned vectors
but they have been published in a previous ""suppressed reference"" by the
authors.

- the dataset is small, and not enough described. In particular, ranges of
frequences are quite likely to impact the results. 
Since the improvements using aligned vectors are marginal, over a small
dataset, in which it is unclear how the choice of the compounds was performed,
I find that the findings in the paper are quite fragile.

More detailed comments/questions
================================

Section 3

I don't understand the need for the new name ""packed anchored tree"".
It seems to me a plain extraction of the paths between two lexical items in a
dependency tree,
namely a plain extension of what is traditionally done in syntactic
distributional representations of words
(which typically (as far as Lin 98) use paths of length one, or length 2, with
collapsed prepositions).

Further, why is it called a tree? what are ""elementary APTs"" (section 5.1) ?

Table 2 : didn't you forget to mention that you discard features of order more
than 3 
(and that's why for instance NMOD.overline(NSUBJ).DOBJ does not appear in
leftmost bottom cell of table 2
Or does it have to do with the elimination of some incompatible types you
mention
(for which an example should be provided, I did not find it very clear).

Section 4:

Since the Reddy et al. dataset is central to your work, it seems necessary to
explain how the 90 compounds were selected. What are the frequency ranges of
the compounds / the components etc... ? There is a lot of chance that results
vary depending on the frequency ranges.

How many judgments were provided for a given compound? Are there many compounds
with same final compositionality score? Isn't it a problem when ranking them to
compute the Spearman correlation ?

Apparently you use ""constituent"" for a component of the N N sequence. I would
suggest ""component"", as ""constituent"" also has the sense of ""phrase"" (syntagm).

""... the intuition that if a constituent is used literally within a phrase then
it is highly likely that the compound and the constituent share co-occurrences""
: note the intuition is certainly true if the constituent is the head of the
phrase, otherwise much less true (e.g. ""spelling bee"" does not have the
distribution of ""spelling"").

Section 5

""Note that the elementary representation for the constituent of a compound
phrase will not contain any of the contextual features associated with the
compound phrase token unless they occurred with the constituent in some other
context. ""
Please provide a running example in order to help the reader follow which
object you're talking about.
Does ""compound phrase token"" refer to the merged components of the compound?

Section 5.1

I guess that ""elementary APTs"" are a triplet target word w + dependency path r
+ other word w'?
I find the name confusing.

Clarify whether ""shifted PMI"" refer to PMI as defined in equation (3).

""Removing features which tend to go with lots of
 things (low positive PMI) means that these phrases
 appear to have been observed in a very small num-
 ber of (highly informative) contexts.""
Do ""these phrases"" co-refer with ""things"" here?
The whole sentence seems contradictory, please clarify.

""In general, we would expect there to be little 558
overlap between APTs which have not been prop-
erly aligned.""
What does ""not properly aligned"" means? You mean not aligned at all?

I don't understand paragraph 558 to 563.
Why should the potential overlap be considerable
in the particular case of the NMOD relation between the two components?

Paragraph 575 to 580 is quite puzzling.
Why does the whole paper make use of higher order dependency features
and then suddenly, at the critical point of actually measuring the crucial
metric
of similarity between composed and observed phrasal vectors, you use
first order features only?

Note 3 is supposed to provide an answer, but I don't understand the explanation
of why the 2nd order paths in the composed representations are not reliable,
please clarify.

Section 6

""Smoothing the PPMI calculation with a value of Î± = 0.75 generally has a 663
small positive effect.""
does not seem so obvious from table 3.

What are the optimal values for h and q in equation 8 and 9? They are important
in order to estimate
how much of ""hybridity"" provides the slight gains with respect to the unaligned
results.

It seems that in table 4 results correspond to using the add combination, it
could help to have this in the legend.
Also, couldn't you provide the results from the word2vec vectors for the
compound phrases?

I don't understand the intuition behind the FREQ baseline. Why would a frequent
compound tend to be compositional? This suggests maybe a bias in the dataset.",,3,4,Poster,3,4,2,4,4,5,2,2,2016,"Elsewhere (reference suppressed), we proposed a distributional compositional approach where distributional features are based on anchored packed trees (APTs). The essence of the APT approach is that distributional features should encode complete dependency paths from the target word to each context word and that these representations should be properly aligned before composition.
token 1st order 2nd order
grad/N ⟨NMOD, student⟩ ⟨NMOD.NSUBJ, fold⟩ student/N ⟨NMOD, grad⟩,
⟨NSUBJ, fold⟩ ⟨NSUBJ.DOBJ, clothes⟩
fold/V ⟨NSUBJ, student⟩, ⟨DOBJ, clothes⟩ ⟨NSUBJ.AMOD, grad⟩, ⟨DOBJ.NMOD, dry⟩ dry/J ⟨AMOD, clothes⟩ ⟨AMOD.DOBJ, fold⟩, ⟨AMOD.AMOD, clean⟩ clothes/N ⟨AMOD, dry⟩, ⟨DOBJ, fold⟩ ⟨DOBJ.NSUBJ, student⟩
Table 1: Typed higher order distributional features
For example, consider the sentence “The grad student folded the dry clothes.” Table 1 shows some of the features that this would generate in an APT lexicon for each of the content tokens1. Features are typed using the complete dependency
1We assume lemmatisation in these examples since in practice it reduces sparsity, but it is not necessary from a theoretical point of view. In these examples, we also omit the POS tags on the context words since they can be easily inferred by the reader.
path from the token to the context word. The inverse of dependency relation R is denoted by R. The length of the dependency path is referred to as the order of the distributional feature. For compactness, we only show features up to order 2.
From Table 1, it is clear, as noted by Weeds et al. (2014), that features of words with different parts of speech do not immediately sit in the same space. This is because words of different parts of speech play different roles in syntactic (and the associated semantic) relations. In this example, all of the observed first order features of adjectives are of type AMOD whereas the nouns have observed first order features of types AMOD, NMOD, NMOD, NSUBJ and DOBJ. The use of an intersective pointwise composition operation such as multiply would largely lead to zero vectors, which is obviously not the desired result of composition. A pointwise composition operation which performs a union of the features such as add would largely lead to a concatenation of the two vectors.
However, the second observation that we make based on Table 1 is that the 2nd order features of, say, adjectives, correspond to the 1st order features of nouns. Similarly, whilst not shown, the 3rd order features correspond to the 2nd order features and the 4th order features correspond to the 3rd order features. The only difference is that the features in the adjective space have the prefix AMOD which is the path which is traversed between the adjective and the noun in the parse tree.
Accordingly, to construct a vector for a phrase from the perspective of its head (i.e., so that it has features in the same type space as its head), we first offset all of the dependent vectors in accordance with δ, the path to the dependent from the head in the phrase or sentence being composed. During offsetting each feature type is prepended by the given path and reduction applied. The reduced co-occurrence type produced from τ is denoted ↓(τ), and defined as follows:
↓(τ) = ⎧⎪⎪⎪⎨⎪⎪⎪⎩ ↓(τ1τ2) if τ = τ1 r r τ2 or τ = τ1 r r τ2 for some r ∈ R
τ otherwise (1)
Reduction essentially means that a relation cancels with its inverse relation i.e., the occurrence of a relation adjacent to its inverse relation will be replaced by the empty relation. Note, as discussed in (reference suppressed), reduction does introduce
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
δ student/N clothes/N
⟨NMOD, grad⟩, ⟨NSUBJ, fold⟩, ⟨NSUBJ.DOBJ, clothes⟩ ⟨AMOD, dry⟩, ⟨DOBJ, fold⟩, ⟨DOBJ.NSUBJ, student⟩
NSUBJ ⟨ , fold⟩, ⟨DOBJ, clothes⟩, ⟨NSUBJ.NMOD, grad⟩
⟨NSUBJ.AMOD, dry⟩
NMOD ⟨NMOD.NMOD, grad⟩, ⟨NMOD.NSUBJ, fold⟩ ⟨NMOD.AMOD, dry⟩
Table 2: Features of student/N and clothes/N given different offsets δ.
zero’th order features (with type ) in both elementary and offset representations. Further, all cooccurrence types are required to have a tree-based interpretation (τ ∈ R∗R∗) which leads to the elimination of some incompatible co-occurrence types. Table 2 shows the result of offsetting the features of student/N and clothes/N in Table 1 with three different paths 1) , 2) NSUBJ and 3) NMOD as required in the compositions of the phrases lazy student, student submits and student halls respectively.
Having aligned the vector spaces, it is possible to carry out any pointwise composition operation, such as add or multiply. The overall result will be sensitive to the structure in the composed phrase because different structures lead to different alignments.
However, the focus of the current study is noun compounds, the majority of which are generally tagged and parsed as noun-noun compounds (e.g., grad student/NN). In this particular instance, it is less apparent that the vector spaces need to be aligned before composition. There will be other instances of the word grad/N where it is used as the object or subject of verbs. These contexts may well be good indicators of what make good contexts for grad student/N (particularly in this example since grad/N and grad student/NN are often used synonymously). Therefore, we also consider the typed dependency model where features are higher order dependency paths but alignment is not carried out before composition.Compositionality detection, as described in Reddy et al. (2011), involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal (simplex) meaning of its parts. Reddy et al. (2011) introduced a dataset consisting of 90
compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level. All judgments are given on a scale of 0 to 5, where 5 is high. For example, the phrase climate change is deemed to take its meaning literally from both constituents and also deemed to be a literal phrase. Conversely, the phrase gravy train has low scores for the literalness of the phrase and of the use of each constituent within the phrase. The phrase cocktail dress is deemed to be literal in its use of the second constituent but not the first whereas the phrase spelling bee is deemed to have high literalness in its use of the first constituent but not in its use of the second. Both of these examples are considered to have a medium level of literalness with respect to the whole phrase.
Reddy et al. (2011) further investigated a number of ways of detecting compositionality using vector based models of word meaning. They experimented with both constituent based models and compositionality function based models. In constituent based models, the compositionality of the phrase is considered to be a function of the similarity of each of the constituent’s vectors to the observed phrase vector. This is based on the intuition that if a constituent is used literally within a phrase then it is highly likely that the compound and the constituent share co-occurrences. In compositionality function based methods, the compositionality of the phrase is determined by first composing the constituents using some function and then measuring the similarity of the composed vector to the observed vector. The intuition here is that a good compositionality function is highly likely to return vectors which are similar to observed phrasal vectors for compositional phrases but much less likely to return similar vectors for non-compositional phrases. Reddy et al. (2011) carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). They used 3-fold cross-validation to estimate model parameters and found that using weighted addition outperformed multiplication as a compositionality function and that this also outperformed all of the constituent based models. With the optimal settings in their experiments they achieved a Spearman’s rank correlation coefficient of 0.714 with the human judgments.
We adapt the experiment described above to
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
look at the effectiveness of APT composition in predicting the first-order dependency features of compound nouns. When a phrase is compositional, we expect that the second- (and zero-) order features of the modifier composed with the firstorder features of the head noun will be a good predictor of the first-order features of the compound noun. For example, we expect the second-order dependency features of spelling, which would include evidence from other uses of spelling as a modifier (e.g. spelling test) to be more indicative of the co-occurrences of spelling bee than the firstorder dependency features of spelling.For consistency with the experiments of Reddy et al. (2011), the corpus used in this experiment is the same fully-annotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008). As described in Grefenstette et al. (2013), this corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependencyparsed with the Malt Parser (Nivre, 2004). It contains about 1.9 billion tokens.
Before creating our APT lexicon and word embeddings from the dependency-parsed corpus, we further preprocessed the corpus by identifying occurrences of the 90 target noun phrases and recombining them into a single lexical item. Note that whilst all of the phrases are considered to be compound nouns, there are a number of possible dependency relationships which can occur between them. In the corpus we identified 1,236,264 occurrences of the candidate lemmas occurring contiguously (in the correct order). 76% of these had some kind of dependency relationship where the second lemma was the head. We note that the majority of occurrences where no dependency relationship was observed, this was due to the parser incorrectly parsing a three noun compound phrase, e.g., parsing the phrase interest rate rise so that both interest and rate modify rise. Of the occurrences where some kind of dependency relationship in the correct direction was identified, 95% of were an NMOD relationship. However, the modifier in an NMOD relationship can be an adjective or a noun. Many compounds (e.g. graduate student and silver screen) are seen with the modifier tagged both as a noun and as an adjective. So that we can carry out composition of the correct tokens (e.g. graduate as an adjective as opposed to grad-
uate as a noun) according to observed dependency relations, the token for the compound records both the individual lemmas and the dependency relation observed between them in the corpus. Where a compound is seen with multiple dependency relationships we selected the dependency relationship which occurred most frequently.
During this preprocessing stage, other dependency paths including the head constituent of the phrase are modified to include the new compound token. In this experiment, dependency paths including the modifying constituent are ignored. The rationale for this is that if the modifier is being modified e.g. as in (recently graduated) student, then this is not a modification which can be applied to the phrase — we do not apply adverbs to compound nouns — rather it is a modification of an internal part of the phrase.
Having preprocessed the corpus to contain compound nouns, we created elementary representations for every token in the corpus. Note that the elementary representation for the constituent of a compound phrase will not contain any of the contextual features associated with the compound phrase token unless they occurred with the constituent in some other context. If we allowed the same single observed contextual feature to feed into the representation of the compound and of the constituents, then intersective methods of composition would do very well at recreating the observed vector — ignoring any feature selection, recall would be 100%. Following Weeds et al. (2014) we expect a good method of composition to be able to infer the representation of a phrase without ever having observed the phrase in the corpus.
We will now discuss the parameters we have explored in terms of the construction and composition of elementary representations in each model.In relation to the construction of the elementary APTs, the most obvious parameter is the nature of the weight associated with each feature. We consider both the use of normalised counts and PPMI values. If w is a target word, w′ a context word and τ a dependency path from w to w′ then the normalised count is simply:
p(w′, τ ∣w) = #⟨w, τ, w ′⟩
#⟨w, ∗, ∗⟩ (2)
Levy et al. (2015) showed that the use of context distribution smoothing (α = 0.75) in the PMI cal-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
culation can lead to performance comparable with state-of-the-art word embeddings on word similarity tasks. We use this modified definition of PMI and experiment with α = 0.75 and α = 1
PMI (w,w′; τ,α) = #⟨w, τ, w ′⟩#⟨∗, τ, ∗⟩α
#⟨w, τ, ∗⟩#⟨∗, τ, w′⟩α (3)
We also carried out some experiments with shifted PMI which is analogous to the use of negative sampling in word embeddings (Levy et al., 2015). However, in this study we found that shifting PMI tended to have a strong negative effect on results. We suspect this is due to the low frequency of many of the compound nouns in the corpus. Removing features which tend to go with lots of things (low positive PMI) means that these phrases appear to have been observed in a very small number of (highly informative) contexts. If the composition process fails to recover one or more of these contexts, it has a very big impact on similarity. For compactness, we do not include the results with shifted PMI here.
Having constructed elementary APTs2, the APT composition process involves aligning those elementary APTs and composing the associated vectors of weights. However, the composition operation used after alignment is not fixed. Here, we have investigated using⊔INT, which takes the minimum of each of the constituent’s feature values and ⊔UNI, which performs pointwise addition on the aligned vector spaces. Following Reddy et al. (2011), who found that weighted addition worked best in their experiments, when using the ⊔UNI operation, we have experimented with weighting the contributions of each constituent to the composed APT representation using the parameter, h. For example, if A2 is the APT associated with the head of the phrase and Aδ1 is the properly aligned APT associated with the modifier where δ is the dependency path from the head to the modifier (e.g. NMOD or AMOD), the composition operations can be defined as:
⊔ INT {Aδ1,A2 } (4)
⊔ UNI { (1 − h)Aδ1, hA2 } (5)
2The size of the feature space for nouns is approximately 80,000 dimensions (when including only first-order paths) and approximately 230,000 dimensions (when including paths up to order 2).
We have also considered composition without alignment of the modifier’s APT, i.e, using A1:
⊔ INT {A1,A2 } (6)
⊔ UNI { (1 − h)A1, hA2 } (7)
In general, we would expect there to be little overlap between APTs which have not been properly aligned. However, in the case where δ is the NMOD relation, i.e., the internal relation in the vast majority of the compound phrases, there may well be considerable overlap between the conventional first-order dependency features of the modifier and the head. In order to examine the contribution of both the aligned and unaligned APTs in the composition process, we used a hybrid method where the composed representation is defined as:
⊔ INT { (qAδ1 + (1 − q)A1),A2 } (8)
⊔ UNI { (1 − h)(qAδ1 + (1 − q)A1), hA2 } (9)
In the case where representations consist of APT weights which are normalised counts, PPMI is estimated after composition. Similarity between composed and observed phrasal vectors (restricted to 1st order dependency features3) is then computed using the cosine measure.For each word and compound phrase, neural representations were constructed using the word2vec tool (Mikolov et al., 2013). Whilst it is not possible or appropriate to carry out an exhaustive parameter search, we experimented with a number of commonly used and recommended parameter settings. In particular, we investigate both the cbow and skip-gram models with 50, 100 and 300 dimensions. We also experiment with the subsampling threshold, trying 10−3, 10−4 and 10−5. As recommended in the documentation, in the cbow model we use a window size of 5 and and in the skip-gram model we use a window size of 10.
3The rationale for this is that, whilst both composed and observed representations contain higher-order dependency features, the second-order (and third-order) paths in the composed representations are not reliable since the elementary APTs only contained paths up to order 2.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Compositional Model
PPMI α = 1 PPMI α = 0.75
CF CS CF CS
Aligned ⊔INT (Eq. 4) 0.72 0.70 0.75 0.72 Aligned ⊔UNI (Eq. 5) 0.71 0.72 0.72 0.75 Unaligned ⊔INT (Eq. 6) 0.74 0.72 0.72 0.73 Unaligned ⊔UNI (Eq. 7) 0.77 0.75 0.78 0.77 Hybrid ⊔INT (Eq. 8) 0.74 0.73 0.73 0.73 Hybrid ⊔UNI (Eq. 9) 0.78 0.78 0.79 0.76
Table 3: Average ρ between human judgements and phrase compositionality scores using APT representations.
Early experiments with different composition operations, showed add to be the only promising option. Consequently, all of the results reported here are for weighted addition. Similarity between composed and observed representations is computed using the cosine measure.","In order to estimate the model parameters h and q, we use repeated 3-fold cross-validation. Optimum values of the parameters are selected using the training samples. Results are reported in terms of average Spearman rank correlation scores (ρ) of phrase compositionality scores with human judgements on the corresponding testing samples. For clarity, we do not include the errors in the tables, but we have used a sufficiently large number of repetitions that these are all small (≤ 0.0015) and thus any difference observed which is greater than 0.005 is statistically significant at the 95% level. Boldface is used to indicate the best performing configuration of parameters for a particular compositional model.
Table 3 summarises results for different composition operations and parameter settings using APT representations. We see that all of the results using standard PPMI (α = 1) and smoothed PPMI (α = 0.75) significantly outperform the result reported in Reddy et al. (2011), which used an untyped dependency space. Smoothing the PPMI calcu-
lation with a value of α = 0.75 generally has a small positive effect. On average, the results when normalised counts are composed and PPMI is calculated as part of the similarity calculation (CF) are slightly higher than the results when PPMI weights are composed (CS) . However, the differences are small and it is possible that different parameter settings would lead to better results for CS. In general, the unaligned model outperforms the aligned model. However, a small but significant performance gain is generally made using the hybrid model. This suggests that aligned APT composition and unaligned APT composition are predicting different contexts for compound nouns which all contribute to forming a better estimate of the compositionality of the phrase. Regarding different composition operations, ⊔UNI generally outperforms ⊔INT. However, it would appear that it may be better to use the intersective operation when composing aligned APTs where the weights are normalised counts.
Table 4 summarises results for different parameter settings for the neural word embeddings. Looking at the results in Table 4, we see that the cbow model significantly outperforms the skip-gram model. Using the cbow model with 100 dimensions and a subsampling threshold of t = 10−3 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in Reddy et al. (2011). Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of parameter optimisation.
We note that the cbow model seems to benefit from a higher subsampling threshold than recommend elsewhere in the literature (Levy et al., 2015). Since subsampling with lower thresholds is analogous to shifted PPMI, we hypothesise this
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Model Average ρ RAND 0.02 FREQ 0.63 REDDY 0.71 skip-gram 0.68 cbow 0.74 aligned APT 0.75 unaligned APT 0.78 hybrid APT 0.79
Table 5: Summary of optimal ρ values between phrase compositionality scores and human judgements for each compositional model
‘optimisation’ was not beneficial in the current task for the same reason. Many of the compound phrases are comparatively low frequency tokens in the corpus and therefore subsampling their cooccurrences with high frequency words damages their representations. We also tried using lower subsampling thresholds with the 50-dimensional and 100-dimensional cbow model, but results did not improve.
Finally, we note that consistently across all applicable models, optimal values of both h and q were in the range [0.3,0.5]. This suggests that for this dataset the modifier is slightly more informative than the head in determining the compositionality of compound nouns. It also suggests that the unaligned typed dependency features are slightly more informative than the aligned typed dependency features.
Table 5 summarises the results across all of the compositional models. We also report three baselines. The RAND baseline assigns a random compositionality score to a compound. The FREQ baseline uses the observed frequency of the compound in the corpus as a predictor of compositionality. The REDDY baseline is the reported stateof-the-art result (Reddy et al., 2011) which uses weighted addition of untyped co-occurrence vectors obtained from the ukWaC corpus.
Looking at the results in Table 5, we first note that the FREQ baseline performs much better than the RAND baseline. For this dataset, there is a significant amount of correlation between frequency and human judgments of compositionality i.e. the more frequently occurring compounds tend to be compositional. However, we see that all of the methods for predicting compositionality outperform this baseline. Further the methods based
on APT representations outperform the methods based on neural word embeddings. This suggests that the dependency paths encoded in the typed contextual features are highly informative in the task of determining the compositionality of a noun compound. When a linear combination of aligned and unaligned APTs is allowed, i.e. using the hybrid method, optimal performance is achieved.","In recent years, distributional representations of words have received a lot of interest. In many applications, the ability to cluster or find similar words in terms of their distribution in text or their hypothesised semantic similarity has a massive potential to reduce the sparse data problem. Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations. The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007). More recently the trend has been towards using neural models (Mikolov et al., 2013;
Pennington et al., 2014) to create dense, low dimensional representations, commonly referred to as word embeddings, built by training the models to predict corpus co-occurrences. As has been noted elsewhere (Pennington et al., 2014), whilst appearing at first sight very different, both count-based methods and prediction-based methods have in common the fact that they probe the underlying co-occurrence statistics in the corpus. In fact, Levy and Goldberg (2014) demonstrated that the skip-gram model with negative sampling (SGNS) proposed by Mikolov et al. (2013) is an implicit factorisation of the positive pointwise mutual information (PPMI) matrix commonly used in count-based methods.
One current focus within the field of distributional semantics is enabling systems to make inferences about phrase-level or sentence-level similarity. One approach (Turney, 2012) is to model the similarity of two phrases or sentences as a function of word-level similarities. An alternative approach (Mitchell and Lapata, 2010) is to build phrase or sentence-level representations by composing word-level representations and then measuring similarity directly. However, whilst this second approach has proved popular, success, usually measured in terms of correlation with human similarity judgments, has been limited. For example, Dinu et al. (2013) reported results for a number of state-of-the art compositional methods on a number of phrase-based benchmark tasks. In those experiments, correlation with human judgements for intransitive sentences (Mitchell and Lapata, 2008) does not exceed 0.30.
However, evaluating measures of phrase-level similarity directly against human judgments of similarity ignores the problem that it is not always possible to determine meaning in a compositional manner. If we compose the meaning representations for red and herring, we would ex-
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
pect to get a very different representation from the one which could be directly inferred from corpus observations of the phrase red herring. This would undoubtedly have an impact on our similarity measurements with other words and phrases. Whilst it should be possible to construct evaluation datasets which avoid clearly idiomatic phrases such as red herring, non-compositionality is an integral part of language which should not be ignored (Sag et al., 2002). Further, McCarthy et al. (2003) noted that the compositionality of a phrase should not be judged categorically, rather it should be viewed as existing on a continuum or a scale. Thus any judgements of the similarity of two composed phrases are confounded by the degree to which those phrases are compositional.
Reddy et al. (2011) introduced a new dataset of 90 compound nouns together with human judgments of compositionality. Using this benchmark, compositional methods can be evaluated by correlating the similarity of composed and observed (or holistic) phrase representations with human judgments of compositionality.
In this paper, we use this dataset to investigate the extent to which the underlying definition of context has an effect on a model’s ability to support composition. In particular, we distinguish between typed and untyped contextual features. For example, in traditional count-based models, contextual features based on proximity are usually untyped whereas contextual features based on dependency relations may be typed (i.e., include the name of the dependency relation) or untyped (Baroni and Lenci, 2010). Further, following Weeds et al. (2014) we investigate the use of higher order dependency paths in contextual features in order to support composition in typed vector space models. We compare these models, where composition is an integral part of the distributional model, with the commonly employed approach of applying naı̈ve compositional operations to state-of-theart distributional representations.","In distributional semantic models, composition has typically been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them using some function to produce a data structure that represents the phrase or sentence. The simplest functions which can be used in composition are pointwise addition
and multiplication. From a linguistic perspective, these are naı̈ve since they are commutative, completely ignoring the order and structure of the phrase or sentence. However, Mitchell and Lapata (2008, 2010) found that additive and multiplicative functions applied to proximity-based vector representations were no less effective than more complex functions when performance was assessed against human similarity judgements of simple paired phrases. More recently, Berant and Liang (2014) achieved state-of-the-art results in question-answering where the paraphrase model included a vector space model component. This vector space model component carried out composition by adding neural word embeddings obtained with the word2vec tool (Mikolov et al., 2013).
Over the past 6 years, other more linguistically motivated models of composition have been proposed e.g., the full additive model (Guevara, 2010), the lexical function method (Baroni and Zamparelli, 2010), the full lexical model (Socher et al., 2012) and various tensor methods (Coecke et al., 2011; Grefenstette et al., 2013). These methods all share the idea, taken from formal semantics, of function application derived from syntactic structure. In an evaluation across 3 different benchmark tasks (Dinu et al., 2013), the lexical function model was shown to be consistently the best-performing. However, in the composition of adjective-noun phrases, the simple additive and multiplicative models were still shown to be highly competitive.
Milajevs et al. (2014) compared neural word representations with count-based vectors in a variety of tasks using a variety of naı̈ve and tensorbased compositional models. Across 4 different tasks (word sense disambiguation, sentence similarity, paraphrase detection and dialogue act tagging), the neural word representations consistently outperformed the traditional count-based vectors. However, as concluded by the authors, this may well be due to differences in the size and nature of the corpora from which the different representations were obtained. Considering the results for the neural word representations, pointwise addition outperformed all of the other compositional models considered on 3 out of the 4 tasks. Tensorbased composition performed better than pointwise addition on just the verb disambiguation task, where the authors argue that verb senses depend strongly on the arguments of the verb.
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
Hashimoto et al. (2014) integrated a variety of syntactic and semantic dependencies into neural models in order to jointly learn composition functions and word representations. Whilst these models are well-motivated and achieved some state-ofthe art results on the Mitchell and Lapata (2010) phrase similarity task, the baseline of adding standard neural word embeddings produced by the word2vec tool proved particularly hard to beat.
Hermann et al. (2012) proposed using generative models for modeling the compositionality of noun-noun compounds. Using interpolation to mitigate the sparse data problem, their model beat the baseline of weighted addition on the Reddy et al. (2011) evaluation task when trained on the BNC. However, these results were still siginificantly lower than those reported by Reddy et al. (2011) using the larger ukWaC corpus.",,"We have demonstrated a number of ways in which compositionality detection for compound nouns can be improved. First, combining traditional compositional methods with state-of-theart low-dimensional word representations significantly improves results. However, further improvements can be achieved using an integrated compositional distributional approach. This approach maintains syntactic structure within the contextual features of words which is then central to the compositional process. We argue that some knowledge of syntactic structure is crucial in the fine-grained understanding of language. Since compositionality detection also provides a way of evaluating compositional methods without confounding judgements of phrase similarity with judgements of compositionality, it appears that the APT approach to compositionality is reasonably promising. Further work is of course needed with other datasets and other types of phrase.
The use of compositional methods to detect compositionality may also lead to improved compositional methods. For example, one reason why intersective approaches to composition may work less well than additive approaches is the sparse nature of the elementary representations. Improvements can be made (reference suppressed) by carrying out distributional smoothing (Dagan et al., 1993) on the elementary representations. However, further improvements might be made by first detecting non-compositional phrases. If a phrase is judged to be non-compositional, i.e., its composed representation is too dissimilar from its observed representation, the observed phrasal representation should be smoothed rather than its constituents’ representations."
25,"Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows.",Exploring Prediction Uncertainty in Machine Translation Quality Estimation,142,"The paper explores the use of probabilistic models (gaussian processes) to
regress on the target variable of post-editing time/rates for quality
estimation of MT output.
The paper is well structured with a clear introduction that highlights the
problem of QE point estimates in real-world applications. I especially liked
the description of the different asymmetric risk scenarios and how they entail
different estimators.
For readers familiar with GPs the paper spends quite some space to reflect
them, but I think it is worth the effort to introduce these concepts to the
reader.
The GP approach and the choices for kernels and using warping are explained
very clearly and are easy to follow. In general the research questions that are
to be answered by this paper are interesting and well phrased.

However, I do have some questions/suggestions about the Results and Discussion
sections for Intrinsic Uncertainty Evaluation:
- Why were post-editing rates chosen over prediction (H)TER? TER is a common
value to predict in QE research and it would have been nice to justify the
choice made in the paper.
- Section 3.2: I don't understand the first paragraph at all: What exactly is
the trend you see for fr-en & en-de that you do not see for en-es? NLL and NLPD
'drastically' decrease with warped GPs for all three datasets.
- The paper indeed states that it does not want to advance state-of-the-art
(given that they use only the standard 17 baseline features), but it would have
been nice to show another point estimate model from existing work in the result
tables, to get a sense of the overall quality of the models.
- Related to this, it is hard to interpret NLL and NLPD values, so one is
always tempted to look at MAE in the tables to get a sense of 'how different
the predictions are'. Since the whole point of the paper is to say that this is
not the right thing to do, it would be great provide some notion of what is a
drastic reduce in NLL/NLPD worth: A qualitative analysis with actual examples.

Section 4 is very nicely written and explains results very intuitively!

Overall, I like the paper since it points out the problematic use of point
estimates in QE. A difficult task in general where additional information such
as confidence arguably are very important. The submission does not advance
state-of-the-art and does not provide a lot of novelty in terms of modeling
(since GPs have been used before), but its research questions and goals are
clearly stated and nicely executed.

Minor problems:
- Section 4: ""over and underestimates"" -> ""over- and underestimates""
- Figure 1 caption: Lines are actually blue and green, not blue and red as
stated in the caption.
- If a certain toolkit was used for GP modeling, it would be great to refer to
this in the final paper.",,4,4,Poster,4,5,4,3,5,5,3,3,2016,"Traditionally, QE is treated as a regression task with hand-crafted features. Kernel methods are arguably the state-of-the-art in QE since they can easily model non-linearities in the data. Furthermore, the scalability issues that arise in kernel methods do not tend to affect QE in practice since the datasets are usually small, in the order of thousands of instances.
The most popular method for QE is Support Vector Regression (SVR), as shown in the multiple instances of the WMT QE shared tasks (Callisonburch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). While SVR models can generate competitive predictions for this task, they lack a probabilistic interpretation, which makes it hard to extract uncertainty estimates using them. Bootstrapping approaches like bagging (Abe and Mamitsuka, 1998) can be applied, but this requires setting and optimising hyperparameters like bag size and number of bootstraps. There is also no guarantee these estimates come from a well-calibrated probabilistic distribution.
Gaussian Processes (GPs) (Rasmussen and Williams, 2006) is an alternative kernel-based framework that gives competitive results for point estimates (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2014b). Unlike SVR, they explicitly model uncertainty in the data and in the predictions. This makes GPs very applicable when
well-calibrated uncertainty estimates are required. Furthermore, they are very flexible in terms of modelling decisions by allowing the use of a variety of kernels and likelihoods while providing efficient ways of doing model selection. Therefore, in this work we focus on GPs for probabilistic modelling of QE. In what follows we briefly describe the GPs framework for regression.Here we follow closely the definition of GPs given by Rasmussen and Williams (2006). Let X = {(x1, y1), (x2, y2), . . . , (xn, yn)} be our data, where each x ∈ RD is a D-dimensional input and y is its corresponding response variable. A GP is defined as a stochastic model over the latent function f that generates the data X :
f(x) ∼ GP(m(x), k(x,x′)),
where m(x) is the mean function, which is usually the 0 constant, and k(x,x′) is the kernel or covariance function, which describes the covariance between values of f at the different locations of x and x′.
The prior is combined with a likelihood via Bayes’ rule to obtain a posterior over the latent function:
p(f |X ) = p(y|X, f)p(f) p(y|X) ,
where X and y are the training inputs and response variables, respectively. For regression, we assume that each yi = f(xi) + η, where η ∼ N (0, σ2n) is added white noise. Having a Gaussian likelihood results in a closed form solution for the posterior.
Training a GP involves the optimisation of model hyperparameters, which is done by maximising the marginal likelihood p(y|X) via gradient ascent. Predictive posteriors for unseen x∗ are obtained by integrating over the latent function evaluations at x∗.
GPs can be extended in many different ways by applying different kernels, likelihoods and modifying the posterior, for instance. In the next Sections, we explain in detail some sensible modelling choices in applying GPs for QE.Choosing an appropriate kernel is a crucial step in defining a GP model (and any other kernel
3
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
method). A common choice is to employ the exponentiated quadratic (EQ) kernel1:
kEQ(x,x ′) = σv exp(−
r2
2 ) ,
where r2 = D∑ i=1 (xi − x′i)2 l2i
is the scaled distance between the two inputs, σv is a scale hyperparameter and l is a vector of lengthscales. Most kernel methods tie all lengthscale to a single value, resulting in an isotropic kernel. However, since in GPs hyperparameter optimisation can be done efficiently, it is common to employ one lengthscale per feature, a method called Automatic Relevance Determination (ARD).
The EQ kernel allows the modelling of nonlinearities between the inputs and the response variables but it makes a strong assumption: it generates smooth, infinitely differentiable functions. This assumption can be too strong for noisy data. An alternative is the Matèrn class of kernels, which relax the smoothness assumption by modelling functions which are ν-times differentiable only. Common values for ν are the half-integers 3/2 and 5/2, resulting in the following Matèrn kernels:
kM32 = σv(1 + √ 3r2) exp(− √ 3r2)
kM52 = σv
( 1 + √ 5r2 + 5r2
3
) exp(− √ 5r2) ,
where we have omitted the dependence of kM32 and kM52 on the inputs (x,x′) for brevity. Higher values for ν are usually not very useful since the resulting behaviour is hard to distinguish from limit case ν → ∞, which retrieves the EQ kernel (Rasmussen and Williams, 2006, Sec. 4.2).
The relaxed smoothness assumptions from the Matèrn kernels makes them promising candidates for QE datasets, which tend to be very noisy. We expect that employing them will result in a better models for this application.The Gaussian likelihood of standard GPs has support over the entire real number line. However, common quality scores are strictly positive values, which means that the Gaussian assumption
1Also known as Radial Basis Function (RBF) kernel.
is not ideal. A usual way to deal with this problem is model the logarithm of the response variables, since this transformation maps strictly positive values to the real line. However, there is no reason to believe this is the best possible mapping: a better idea would be to learn it from the data.
Warped GPs (Snelson et al., 2004) are an extension of GPs that allows the learning of arbitrary mappings. It does that by placing a monotonic warping function over the observations and modelling the warped values inside a standard GP. The posterior distribution is obtained by applying a change of variables:
p(y∗|x∗) = f ′(y∗)√ 2πσ2∗ exp
( f(y∗)− µ∗
2σ∗
) ,
where µ∗ and σ∗ are the mean and standard deviation of the latent (warped) response variable and f and f ′ are the warping function and its derivative.
Point predictions from this model depend on the loss function to be minimised. For absolute error, the median is the optimal value while for squared error it is the mean of the posterior. In standard GPs, since the posterior is Gaussian the median and mean coincide but this in general is not the case for a Warped GP posterior. The median can be easily obtained by applying the inverse warping function to the latent median:
ymed∗ = f −1(µ∗).
While the inverse of the warping function is usually not available in closed form, we can use its gradient to have a numerical estimate.
The mean is obtained by integrating y∗ over the latent density:
E[y∗] = ∫ f−1(z)Nz(µ∗, σ2∗)dz,
where z is the latent variable. This can be easily approximated using Gauss-Hermite quadrature since it is a one dimensional integral over a Gaussian density.
The warping function should be flexible enough to allow the learning of complex mappings, but it needs to be monotonic. Snelson et al. (2004) proposes a parametric form composed of a sum of tanh functions, similar to a neural network layer:
f(y) = y + I∑ i=1 ai tanh(bi(y + ci)),
4
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
where I is the number of tanh terms and a,b and c are treated as model hyperparameters and optimised jointly with the kernel and likelihood hyperparameters. Large values for I allow more complex mappings to be learned but raise the risk of overfitting.
Warped GPs provide an easy and elegant way to model response variables with non-Gaussian behaviour within the GP framework. In our experiments we explore models employing warping functions with up to 3 terms, which is the value recommended by Snelson et al. (2004). We also report results using the f(y) = log(y) warping function.Our experiments comprise datasets containing three different language pairs, where the label to predict is post-editing time:
English-Spanish (en-es) This dataset was used in the WMT14 QE shared task (Bojar et al., 2014). It contains 858 sentences translated by one MT system and post-edited by a professional translator.
French-English (fr-en) Described in (Specia, 2011), this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator.
English-German (en-de) This dataset is part of the WMT16 QE shared task2. It was translated by one MT system for consistency we use a subset of 2, 828 instances post-edited by a single professional translator.
As part of the process of creating these datasets, post-editing time was logged on an sentence basis for all datasets. Following common practice, we normalise the post-editing time by the length of the machine translated sentence to obtain postediting rates and use these as our response variables. For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015). These features are used in the strong baseline models provided by the WMT QE shared tasks. While the best performing systems in the shared tasks use larger feature sets, these are mostly resource-intensive and languagedependent, and therefore not equally applicable to all our language pairs. Moreover, our goal is to compare probabilistic QE models through the predictive uncertainty perspective, rather than improving the state-of-the-art in terms of point predictions. We perform 10-fold cross validation instead of using a single train/test splits and report averaged metric scores.
The model hyperparameters were optimised by maximising the likelihood on the training data. We perform a two-pass procedure similar to that
2www.statmt.org/wmt16
5
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
510
511
512
513
514
515
516
517
518
519
in (Cohn and Specia, 2013): first we employ an isotropic kernel and optimise all hyperparameters using 10 random restarts; then we move to an ARD equivalent kernel and perform a final optimisation step to fine tune feature lengthscales. Point predictions were fixed as the median of the distribution.Evaluation metrics for QE, including those used in the WMT QE shared tasks, are assumed to be symmetric, i.e., they penalise over and underestimates equally. This assumption is however too simplistic for many possible applications of QE. For example:
• In a post-editing scenario, a project manager may have translators with limited expertise in post-editing. In this case, automatic translations should not be provided to the translator unless they are highly likely to have very good quality. This can be enforced this by increasing the penalisation weight for underestimates. We call this the pessimistic scenario.
• In a gisting scenario, a company wants to automatically translate their product reviews so that they can be published in a foreign language without human intervention. The company would prefer to publish only the reviews translated well enough, but having more reviews published will increase the chances of selling products. In this case, having better recall is more important and thus only reviews with very poor translation quality should be discarded. We can accomplish this by heavier penalisation on overestimates, a scenario we call optimistic.
In this Section we show how these scenarios can be addressed by well-calibrated predictive distributions and by employing asymmetric loss functions. An example of such a function is the asymmetric linear (henceforth, AL) loss, which is a generalisation of the absolute error:
L(ŷ, y) = { w(ŷ − y) if ŷ > y y − ŷ if ŷ ≤ y,
where w > 0 is the weight given to overestimates. If w > 1 we have the pessimistic scenario, and the optimistic one can be obtained using 0 < w < 1. For w = 1 we retrieve the original absolute error loss.
Another asymmetric loss is the linear exponential or linex loss (Zellner, 1986):
L(ŷ, y) = exp[w(ŷ − y)]− (ŷ − y)− 1
where w ∈ R is the weight. This loss attempts to keep a linear penalty in lesser risk regions, while
imposing an exponential penalty in the higher risk ones. Negative values for w will result in a pessimistic setting, while positive values will result in the optimistic one. For w = 0, the loss approximates a squared error loss. Usual values for w tend to be close to 1 or−1 since for higher weights the loss can quickly reach very large scores. Both losses are shown on Figure 1.The losses introduced above can be incorporated directly into learning algorithms to obtain models for a given scenario. In the context of the AL loss this is called quantile regression (Koenker, 2005), since optimal estimators for this loss are posterior quantiles. However, in a production environment the loss can change over time. For instance, in the gisting scenario discussed above the parameter w could be changed based on feedback from indicators of sales revenue or user experience. If the loss is attached to the underlying learning algorithms, a change in w would require full model retraining, which can be costly.
Instead of retraining the model everytime there is a different loss, we can train a single probabilistic model and derive Bayes risk estimators for the loss we are interested in. This allows estimates to
7
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
be obtained without having to retrain models when the loss changes. Additionally, this allows different losses/scenarios to be employed at the same time using the same model.
Minimum Bayes risk estimators for asymmetric losses were proposed by Christoffersen and Diebold (1997) and we follow their derivations in our experiments. The best estimator for the AL loss is equivalent to the ww+1 quantile of the predictive distribution. Note that we retrieve the median when w = 1, as expected. The best estimator for the linex loss can be easily derived and results in:
ŷ = µy − wσ2y 2
where µy and σ2y are the mean and the variance of the predictive posterior.Here we assess the models and datasets used in Section 3.1 in terms of their performance in the asymmetric setting. Following the explanation in the previous Section, we do not perform any retraining: we collect the predictions obtained using the 10-fold cross-validation protocol and apply different Bayes estimators corresponding to the asymmetric losses. Evaluation is performed using the same loss employed in the estimator (for instance, when using the linex estimator with w = 0.75 we report the results using the linex loss with same w) and averaged over the 10 folds.
To simulate both pessimistic and optimistic scenarios, we use w ∈ {3, 1/3} for the AL loss and w ∈ {−0.75, 0.75} for the linex loss. The only exception is the en-de dataset, where we report results for w ∈ −0.25, 0.75 for linex3. We also report results only for models using the Matèrn52 kernel. While we did experiment with different kernels and weighting schemes4 our findings showed similar trends so we omit them for the sake of clarity.","Given a set of different probabilistic QE models, we are interested in evaluating the performance of these models, while also taking their uncertainty into account, particularly to distinguish among models with seemingly same or similar performance. A straightforward way to measure the performance of a probabilistic model is to inspect its negative (log) marginal likelihood. This measure, however, does not capture if a model overfit the training data.
We can have a better generalization measure by calculating the likelihood on test data instead. This was proposed in previous work and it is called Negative Log Predictive Density (NLPD) (Quiñonero-Candela et al., 2006):
NLPD(ŷ,y) = − 1 n n∑ i=1 log p(ŷi = yi|xi).
where ŷ is a set of test predictions, y is the set of true labels and n is the test set size. This metric has since been largely adopted by the ML community when evaluating GPs and other probabilistic models for regression (see Section 5 for some examples).
As with other error metrics, lower values are better. Intuitively, if two models produce equally incorrect predictions but they have different uncertainty estimates, NLPD will penalise the overconfident model more than the underconfident one. On the other hand, if predictions are close to the true value then NLPD will penalise the underconfident model instead.
In our first set of experiments we evaluate models proposed in Section 2 according to their negative log likelihood (NLL) and the NLPD on test
data. We also report two point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson’s r, which has recently proposed by Graham (2015) as a more robust alternative.Table 1 shows the results obtained for all datasets. The first two columns shows an interesting finding in terms of model learning: using a warping function drastically decreases both NLL and NLPD. The main reason behind this is that standard GPs distribute probability mass over negative values, while the warped models do not. For the fr-en and en-de datasets, NLL and NLPD follow similar trends. This means that we can trust NLL as a measure of uncertainty for these datasets. However, this is not observed in the en-es dataset. Since this dataset is considerably smaller than the others, we believe this is evidence of overfitting, thus showing that NLL is not a reliable metric for small datasets.
In terms of different warping functions, using the parametric tanh function with 3 terms performs better than the log for the fr-en and en-de datasets. This is not the case of the en-es dataset, where the log function tends to perform better. We believe that this is again due to the smaller dataset size. The gains from using a Matèrn kernel over EQ are less conclusive. While they do tend to perform better for fr-en, there does not seem to be any difference in the other datasets. Different kernels might be more appropriate depending on the language pair but more experiments are needed to check if this is the case, which we leave for future work.
The differences in uncertainty modelling are by and large not captured by the point estimate metrics. While MAE does show gains from standard to Warped GPs, it does not reflect the difference found between warping functions for fr-en. Pearson’s r is also quite inconclusive in this sense, except for some observed gains for en-es. This shows that NLPD indeed should be preferred as a evaluation metric when proper prediction uncertainty estimates are required by a QE model.
6
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623Results are shown on Table 2. In the optimistic scenario the tanh-based warped GP models give
3Using w = −0.75 in this case resulted in loss values on the order of 107. In fact, as it will be discussed in the next Section, the results for the linex loss in the pessimistic scenario were inconclusive. However, we report results using a higher w in this case for completeness and to clarify the inconclusive trends we found.
4We also tried w ∈ {1/9, 1/7, 1/5, 5, 7, 9} for the AL loss and w ∈ {−0.5,−0.25, 0.25, 0.5} for the linex loss.
consistently better results than standard GPs. The log-based models also gives good results for AL but for Linex the results are mixed except for enes. This is probably again related to the larger sizes of the fr-en and en-de datasets, which allows the tanh-based models to learn richer representations.
The pessimistic scenario shows interesting trends. While the results for AL follow a similar pattern when compared to the optimistic setting, the results for linex are consistently worse than the standard GP baseline. A key difference between AL and linex is that the latter depends on the
8
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
variance of the predictive distribution. Since the warped models tend to have less variance, we believe the estimator is not being “pushed” towards the positive tails as much as in the standard GPs. This turns the resulting predictions not conservative enough (i.e. the post-editing time predicitions are lower) and this is heavily (exponentially) penalized by the loss. This might be a case where a standard GP is preferred but can also indicate that this loss is biased towards models with high variance, even if it does that by assigning probability mass to nonsensical values (like negative time). We leave further investigation of this phenomenon for future work.","Quality Estimation (QE) (Blatz et al., 2004; Specia et al., 2009) models aim at predicting the quality of automatically translated text segments. Traditionally, these models provide point estimates and are evaluated using metrics like Mean Absolute Error (MAE), Root-Mean-Square Error (RMSE) and Pearson’s r correlation coefficient. However, in practice QE models are built for use in decision making in large workflows involving Machine Translation (MT). In these settings, relying on point estimates would mean that only very accurate prediction models can be useful in practice.
A way to improve decision making based on quality predictions is to explore uncertainty estimates. Consider for example a post-editing scenario where professional translators use MT in an effort to speed-up the translation process. A QE
model can be used to determine if an MT segment is good enough for post-editing or should be discarded and translated from scratch. But since QE models are not perfect they can end up allowing bad MT segments to go through for postediting because of a prediction error. In such a scenario, having an uncertainty estimate for the prediction can provide additional information for the filtering decision. For instance, in order to ensure good user experience for the human translator and maximise translation productivity, an MT segment could be forwarded for post-editing only if a QE model assigns a high quality score with low uncertainty (high confidence). Such a decision process is not possible with point estimates only.
Good uncertainty estimates can be acquired from well-calibrated probability distributions over the quality predictions. In QE, arguably the most successful probabilistic models are Gaussian Processes (GPs) since they considered the state-ofthe-art for regression (Cohn and Specia, 2013; Hensman et al., 2013), especially in the low-data regimes typical for this task. We focus our analysis in this paper on GPs since other common models used in QE can only provide point estimates as predictions. Another reason why we focus on probabilistic models is because this lets us employ the ideas proposed by Quiñonero-Candela et al. (2006), which defined new evaluation metrics that take into account probability distributions over predictions.
The remaining of this paper is organized as follows:
• In Section 2 we further motivate the use of GPs for uncertainty modelling in QE and revisit their underlying theory. We also propose some model extensions previously developed in the GP literature and argue they are more appropriate for the task.
2
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
• We intrinsically evaluate our proposed models in terms of their posterior distributions on training and test data in Section 3. Specifically, we show that differences in uncertainty modelling are not captured by the usual point estimate metrics commonly used for this task.
• As an example of an application for predicitive distributions, in Section 4 we show how they can be useful in scenarios with asymmetric risk and how the proposed models can provide better performance in this case.
We discuss related work in Section 5 and give conclusions and avenues for future work in Section 6.
While we focus on QE as application, the methods we explore in this paper can be applied to any text regression task where modelling predictive uncertainty is useful, either in human decision making or by propagating this information for further computational processing.","Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models.
The NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs (Snelson et al., 2004), but also others like Lázaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to evaluate recent work on uncertainty propagation methods for neural networks (Hernández-Lobato and Adams, 2015).
Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others. Besides the AL and the linex, another well studied loss is the asymmetric quadratic, which in turn relates to the concept of expectiles (Newey and Powell, 1987). This loss generalises the commonly used squared error loss. In terms of applications, Cain and Janssen (1995) gives an example in real estate assessment, where the consequences of under and overassessment are usually different depending on the specific scenario. An engineering example is given by Zellner (1986) in the context of dam construction, where an underestimate of peak water level is much more serious than an
overestimate. Such real-world applications guided many developments in this field: we believe that translation and other language processing scenarios which rely on NLP technologies can heavily benefit from these advancements.",,"This work explored new probabilistic models for machine translation QE that allow better uncertainty estimates. We proposed the use of NLPD, which can capture information on the whole predictive distribution, unlike usual point estimatebased metrics. By assessing models using NLPD we can make better informed decisions about which model to employ for different settings. Furthermore, we showed how information in the predictive distribution can be used in asymmetric loss scenarios and how the proposed models can be beneficial in these settings.
Uncertainty estimates can be useful in many other settings beyond the ones explored in this work. Active Learning can benefit from variance information in their query methods and it has shown to be useful for QE (Beck et al., 2013). Exploratory analysis is another avenue for future work, where error bars can provide further insights about the task, as shown in recent work (Nguyen and O’Connor, 2015). This kind of analysis can be useful for tracking post-editor behaviour and assessing cost estimates for translation projects, for instance.
Our main goal in this paper was to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future."
26,"We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVECCCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.",Massively Multilingual Word Embeddings,143,"This paper describes four methods of obtaining multilingual word embeddings and
a modified QVEC metric for evaluating the efficacy of these embeddings. The
embedding methods are: 

(1) multiCluster : Uses a dictionary to map words to multilingual clusters.
Cluster embeddings are then obtained which serve as embeddings for the words
that reside in each cluster. 

(2) multiCCA : Extends the approach presented by Faruqui and Dyer (2014) for
embedding bilingual words, to multilingual words by using English embeddings as
the anchor space. Bilingual dictionaries (other_language -> English) are then
used to obtain projections from other monolingual embeddings for words in other
languages to the anchor space. 

(3) multiSkip : Extends the approach presented by Luong et al. (2015b) for
embedding using source and target context (via alignment), to the multilingual
case by extending the objective function to include components for all
available parallel corpora. 

(4) Translation invariance : Uses a low rank decomposition of the word PMI
matrix with an objective with includes bilingual alignment frequency
components. May only work for  bilingual embeddings. 

The evaluation method uses CCA to maximize the correlation between the word
embeddings and possibly hand crafted linguistic data. Basis vectors are
obtained for the aligned dimensions which produce a score which is invariant to
rotation and linear transformations. The proposed method also extends this to
multilingual evaluations. 

In general, the paper is well written and describes the work clearly. A few
major issues:

(1) What is the new contribution with respect to the translation invariance
embedding approach of Gardner et al.? If it is the extension to multilingual
embeddings, a few lines explaining the novelty would help. 

(2) The use of super-sense annotations across multiple languages is a problem.
The number of features in the intersection of multiple languages may become
really small. How do the authors propose to address this problem (beyond
footnote 9)?

(3) How much does coverage affect the score in table 2? For example, for
dependency parsing, multi cluster and multiCCA have significantly different
coverage numbers with scores that are close. 

(4) In general, the results in table 3 do not tell a consistent story. Mainly,
for most of the intrinsic metrics, the multilingual embedding techniques do not
seem to perform the best.  Given that one of the primary goals of this paper
was to create embeddings that perform well under the word translation metric
(intra-language), it is disappointing that the method that performs best (by
far) is the invariance approach. It is also strange that the multi-cluster
approach, which discards inter-cluster (word and language) semantic information
performs the best with respect to the extrinsic metrics.

Other questions for the authors:

(1) What is the loss in performance by fixing the word embeddings in the
dependency parsing task? What was the gain by simply using these embeddings as
alternatives to the random embeddings in the LSTM stack parser? 

(2) Is table 1 an average over the 17 embeddings described in section 5.1? 

(3) Are there any advantages of using the multi-Skip approach instead of
learning bilingual embeddings and performing multi-CCA to learning projections
across the distinct spaces?

(4) The dictionary extraction approach (from parallel corpora via alignments or
from google translate) may not reflect the challenges of using real lexicons.
Did you explore the use of any real multi-lingual dictionaries?",,3,5,Poster,4,3,3,4,4,4,3,3,2016,"Let L be a set of languages, and let Vm be the set of surface forms (word types) in m ∈ L. Let
1A method for evaluating monolingual word embeddings. 2http://128.2.220.95/multilingual
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
V = ⋃
m∈L Vm. Our goal is to estimate a partial embedding function E : L × V 7→ Rd (allowing a surface form that appears in two languages to have different vectors in each). We would like to estimate this function such that: (i) semantically similar words in the same language are nearby, (ii) translationally equivalent words in different languages are nearby, and (iii) the domain of the function covers as many words in V as possible.
We use distributional similarity in a monolingual corpus Mm to model semantic similarity between words in the same language. For crosslingual similarity, either a parallel corpus Pm,n or a bilingual dictionary Dm,n ⊂ Vm ×Vn can be used. Our methods focus on the latter, in some cases extracting Dm,n from a parallel corpus.3
Most previous work on multilingual embeddings only considered the bilingual case, | L |= 2. We focus on estimating multilingual embeddings for | L |> 2 and describe two novel dictionarybased methods (multiCluster and multiCCA). We then describe our baselines: a variant of Coulmance et al. (2015) and Guo et al. (2016) (henceforth referred to as multiSkip),4 and the translation-invariance matrix factorization method (Gardner et al., 2015).In this approach, we decompose the problem into two simpler subproblems: E = Eembed ◦ Ecluster, where Ecluster : L×V 7→ C deterministically maps words to multilingual clusters C, and Eembed : C → Rd assigns a vector to each cluster. We use a bilingual dictionary to find clusters of translationally equivalent words, then use distributional similarities of the clusters in monolingual corpora from all languages in L to estimate an embedding for each cluster. By forcing words from different languages in a cluster to share the same embedding, we create anchor points in the vector space to bridge languages.
More specifically, we define the clusters as the
3To do this, we align the corpus using fast align (Dyer et al., 2013) in both directions. The estimated parameters of the word translation distributions are used to select pairs: Dm,n ={ (u, v) | u ∈ Vm, v ∈ Vn, pm|n(u | v)× pn|m(v | u) > τ } , where the threshold τ trades off dictionary recall and precision. We fixed τ = 0.1 early on based on manual inspection of the resulting dictionaries.
4We developed multiSkip independently of Coulmance et al. (2015) and Guo et al. (2016). One important distinction is that multiSkip is only trained on parallel corpora, while Coulmance et al. (2015) and Guo et al. (2016) also use monolingual corpora.
connected components in a graph where nodes are (language, surface form) pairs and edges correspond to translation entries in Dm,n. We assign arbitrary IDs to the clusters and replace each word token in each monolingual corpus with the corresponding cluster ID, and concatenate all modified corpora. The resulting corpus consists of multilingual cluster ID sequences. We can then apply any monolingual embedding estimator; here, we use the skipgram model from Mikolov et al. (2013a).Our proposed method (multiCCA) extends the bilingual embeddings of Faruqui and Dyer (2014). First, they use monolingual corpora to train monolingual embeddings for each language independently (Em and En), capturing semantic similarity within each language separately. Then, using a bilingual dictionary Dm,n, they use canonical correlation analysis (CCA) to estimate linear projections from the ranges of the monolingual embeddings Em and En, yielding a bilingual embedding Em,n. The linear projections are defined by Tm→m,n and Tn→m,n ∈ Rd×d; they are selected to maximize the correlation between Tm→m,nEm(u) and Tn→m,nEn(v) where (u, v) ∈ Dm,n. The bilingual embedding is then defined as ECCA(m, u) = Tm→m,nEm(u) (and likewise for ECCA(n, v)).
In this work, we use a simple extension (in hindsight) to construct multilingual embeddings for more languages. We let the vector space of the initial (monolingual) English embeddings serve as the multilingual vector space (since English typically offers the largest corpora and wide availability of bilingual dictionaries). We then estimate projections from the monolingual embeddings of the other languages into the English space.
We start by estimating, for each m ∈ L \ {en}, the two projection matrices: Tm→m,en and Ten→m,en; these are guaranteed to be non-singular. We then define the multilingual embedding as ECCA(en, u) = Een(u) for u ∈ Ven, and ECCA(m, v) = T−1en→m,enTm→m,enE
m(v) for v ∈ Vm,m ∈ L \ {en}.Luong et al. (2015b) proposed a method for estimating bilingual embeddings which only makes use of parallel data; it extends the skipgram model of Mikolov et al. (2013a). The skipgram model defines a distribution over words u that occur in a
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
context window (of size K) of a word v:
p(u | v) = expEskipgram(m, v)>Econtext(m, u)∑
u′∈Vm expEskipgram(m, v)>Econtext(m, u′)
In practice, this distribution can be estimated using a noise contrastive estimation approximation (Gutmann and Hyvärinen, 2012) while maximizing the log-likelihood:∑
i∈pos(Mm) ∑ k∈{−K,...,−1,1,...,K} log p(ui+k | ui)
where pos(Mm) are the indices of words in the monolingual corpus Mm.
To establish a bilingual embedding, with a parallel corpus Pm,n of source language m and target language n, Luong et al. (2015b) estimate conditional models of words in both source and target positions. The source positions are selected as sentential contexts (similar to monolingual skipgram), and the bilingual contexts come from aligned words. The bilingual objective is to maximize:∑ i∈m-pos(Pm,n) ∑ k∈{−K,...,−1,1,...,K} log p(ui+k | ui) + log p(va(i)+k | ui)
+ ∑
j∈n-pos(Pm,n) ∑ k∈{−K,...,−1,1,...,K} log p(vj+k | vj) + log p(ua(j)+k | vj)
where m-pos(Pm,n) and n-pos(Pm,n) are the indeces of the source and target tokens in the parallel corpus respectively, a(i) and a(j) are the positions of words that align to i and j in the other language. It is easy to see how this method can be extended for more than two languages by summing up the bilingual objectives for all available parallel corpora.Gardner et al. (2015) proposed that multilingual embeddings should be translation invariant. Consider a matrix X ∈ R|V|×|V| which summarizes the pointwise mutual information statistics between pairs of words in monolingual corpora, and let UV> be a low-rank decomposition of X where U,V ∈ R|V|×d. Now, consider another matrix A ∈ R|V|×|V| which summarizes bilingual alignment frequencies in a parallel corpus. Gardner et al. (2015) solves for a low-rank decomposition UV> which both approximates X as well as its
transformations A>X, XA and A>XA by defining the following objective:
minU,V ‖X − UV >‖2 + ‖XA− UV>‖2 + ‖A>X − UV>‖2 + ‖A>XA− UV>‖2
The multilingual embeddings are then taken to be the rows of the matrix U.One of our contributions is to streamline the evaluation of multilingual embeddings. In addition to assessing goals (i–iii) stated in §2, a good evaluation metric should also (iv) show good correlation with performance in downstream applications and (v) be computationally efficient.
It is easy to evaluate the coverage (iii) by counting the number of words covered by an embedding function in a closed vocabulary. Intrinsic evaluation metrics are generally designed to be computationally efficient (v) but may or may not meet the goals (i, ii, iv). Although intrinsic evaluations will never be perfect, a standard set of evaluation metrics will help drive research. By design, standard (monolingual) word similarity tasks meet (i) while cross-lingual word similarity tasks and the word translation tasks meet (ii). We propose another evaluation method (multiQVEC-CCA), designed to simultaneously assess goals (i, ii). MultiQVECCCA extends QVEC (Tsvetkov et al., 2015), a recently proposed monolingual evaluation method, addressing fundamental flaws and extending it to multiple languages. To assess the degree to which these evaluation metrics meet (iv), in §5 we perform a correlation analysis looking at which intrinsic metrics are best correlated with downstream task performance—i.e., we evaluate the evaluation metrics.Word similarity datasets such as WordSim-353 (Agirre et al., 2009) and MEN (Bruni et al., 2014) provide human judgments of semantic similarity. By ranking words by cosine similarity and by their empirical similarity judgments, a ranking correlation can be computed that assesses how well the estimated vectors capture human intuitions about semantic relatedness.
Some previous work on bilingual and multilingual embeddings focuses on monolingual word similarity to evaluate embeddings (e.g., Faruqui and Dyer, 2014). This approach is limited because
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
it cannot measure the degree to which embeddings from different languages are similar (ii). For this paper, we report results on an English word similarity task, the Stanford RW dataset (Luong et al., 2013), as well as a combination of several cross-lingual word similarity datasets (CamachoCollados et al., 2015).This task directly assesses the degree to which translationally equivalent words in different languages are nearby in the embedding space. The evaluation data consists of word pairs which are known to be translationally equivalent. The score for one word pair (l1,w1), (l2,w2) both of which are covered by an embedding E is 1 if cosine(E(l1,w1),E(l2,w2)) ≥ cosine(E(l1,w1),E(l2,w′2))∀w′2 ∈ Gl2 where Gl2 is the set of words of language l2 in the evaluation dataset, and cosine is the cosine similarity function. Otherwise, the score for this word pair is 0. The overall score is the average score for all word pairs covered by the embedding function. This is a variant of the method used by Mikolov et al. (2013b) to evaluate bilingual embeddings.In order to evaluate how useful the word embeddings are for a downstream task, we use the em-
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
bedding vector as a dense feature representation of each word in the input, and deliberately remove any other feature available for this word (e.g., prefixes, suffixes, part-of-speech). For each task, we train one model on the aggregate training data available for several languages, and evaluate on the aggregate evaluation data in the same set of languages. We apply this for multilingual document classification and multilingual dependency parsing.
For document classification, we follow Klementiev et al. (2012) in using the RCV corpus of newswire text, and train a classifier which differentiates between four topics. While most previous work used this data only in a bilingual setup, we simultaneously train the classifier on documents in seven languages,5 and evaluate on the development/test section of those languages. For this task, we report the average classification accuracy on the test set.
For dependency parsing, we train the stackLSTM parser of Dyer et al. (2015) on a subset of the languages in the universal dependencies v1.1,6 and test on the same languages, reporting unlabeled attachment scores. We remove all part-ofspeech and morphology features from the data, and prevent the model from optimizing the word embeddings used to represent each word in the corpus, thereby forcing the parser to rely completely on the provided (pretrained) embeddings as the token representation. Although omitting other features (e.g., parts of speech) hurts the performance of the parser, it emphasizes the contribution of the word embeddings being studied.Our experiments are designed to show two primary sets of results: (i) how well the proposed intrinsic evaluation metrics correlate with downstream tasks (§5.1) and (ii) which estimation methods work best according to each metric (§5.2). The data used for training and evaluation are available for download on the evaluation portal.We now turn to evaluating the four estimation methods described in §2. We use the proposed methods (i.e., multiCluster and multiCCA) to
8The 102 (17 × 6) values used to compute Pearson’s correlation coefficient are provided in the supplementary material.
9Although supersense annotations exist for other languages, the annotations are inconsistent across languages and may not be publicly available, which is a disadvantage of the multiQVEC and multiQVEC-CCA metrics. Therefore, we recommend that future multilingual supersense annotation efforts use the same set of supersense tags used in other languages. If the word embeddings are primarily needed for encoding syntactic information, one could use tag dictionaries based on the universal POS tag set (Petrov et al., 2012) instead of supersense tags.
train multilingual embeddings in 59 languages for which bilingual translation dictionaries are available.10 In order to compare our methods to baselines which use parallel data (i.e., multiSkip and translation-invariance), we also train multilingual embeddings in a smaller set of 12 languages for which high-quality parallel data are available.11
Training data: We use Europarl en-xx parallel data for the set of 12 languages. We obtain en-xx bilingual dictionaries from two different sources. For the set of 12 languages, we extract the bilingual dictionaries from the Europarl parallel corpora. For the remaining 47 languages, dictionaries were formed by translating the 20k most common words in the English monolingual corpus with Google Translate, ignoring translation pairs with identical surface forms and multi-word translations.
Evaluation data: Monolingual word similarity uses the MEN dataset in Bruni et al. (2014) as a development set and Stanford’s Rare Words dataset in Luong et al. (2013) as a test set. For the crosslingual word similarity task, we aggregate the RG65 datasets in six language pairs (fr-es, fr-de, enfr, en-es, en-de, de-es). For the word translation
10The 59-language set is { bg, cs, da, de, el, en, es, fi, fr, hu, it, sv, zh, af, ca, iw, cy, ar, ga, zu, et, gl, id, ru, nl, pt, la, tr, ne, lv, lt, tg, ro, is, pl, yi, be, hy, hr, jw, ka, ht, fa, mi, bs, ja, mg, tl, ms, uz, kk, sr, mn, ko, mk, so, uk, sl, sw }.
11The 12-language set is {bg, cs, da, de, el, en, es, fi, fr, hu, it, sv}.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
task, we use Wiktionary to extract translationallyequivalent word pairs to evaluate multilingual embeddings for the set of 12 languages. Since Wiktionary-based translations do not cover all 59 languages, we use Google Translate to obtain enxx bilingual dictionaries to evaluate the embeddings of 59 languages. For QVEC and QVEC-CCA, we split the English supersense annotations used in Tsvetkov et al. (2015) into a development set and a test set. For multiQVEC and multiQVECCCA, we use supersense annotations in English, Italian and Danish. For the document classification task, we use the multilingual RCV corpus in seven languages (da, de, en, es, fr, it, sv). For the dependency parsing task, we use the universal dependencies v1.1 in twelve languages (bg, cs, da, de, el, en, es, fi, fr, hu, it, sv).
Setup: All word embeddings in the following results are 512-dimensional vectors. Methods which indirectly use skipgram (i.e., multiCCA, multiSkip, and multiCluster) are trained using 10 epochs of stochastic gradient descent, and use a context window of size 5. The translation-invariance method use a context window of size 3.12 We only estimate embeddings for words/clusters which occur 5 times or more in the monolingual corpora. In a postprocessing step, all vectors are normalized to unit length. MultiCluster uses a maximum cluster size of 1,000 and 10,000 for the set of 12 and 59 languages, respectively. In the English tasks (monolingual word similarity, QVEC, QVEC-CCA), skipgram embeddings (Mikolov et al., 2013a) and multiCCA embeddings give identical results (since we project words in other languages to the English vector space, estimated using the skipgram model). The software used to train all embeddings as well as the trained embeddings are available for download on the evaluation portal.13
We note that intrinsic evaluation of word embeddings (e.g., word similarity) typically ignores test instances which are not covered by the embeddings being studied. When the vocabulary used in two sets of word embeddings is different, which is often the case, the intrinsic evaluation score for each set may be computed based on a different set
12Training translation-invariance embeddings with larger context window sizes using the matlab implementation provided by Gardner et al. (2015) is computationally challenging.
13URLs to software libraries on Github are redacted to comply with the double-blind reviewing of CoNLL.
of test instances, which may bias the results in unexpected ways. For instance, if one set of embeddings only covers frequent words while the other set also covers infrequent words, the scores of the first set may be inflated because frequent words appear in many different contexts and are therefore easier to estimate than infrequent words. To partially address this problem, we report the coverage of each set of embeddings in square brackets. When the difference in coverage is large, we repeat the evaluation using only the intersection of vocabularies covered by all embeddings being evaluated. Extrinsic evaluations are immune to this problem because the score is computed based on all test instances regardless of the coverage.
Results [59 languages]. We train the proposed dictionary-based estimation methods (multiCluster and multiCCA) for 59 languages, and evaluate the trained embeddings according to nine different metrics in Table 2. The results show that, when trained on a large number of languages, multiCCA consistently outperforms multiCluster according to all evaluation metrics. Note that most differences in coverage between multiCluster and multiCCA are relatively small.
It is worth noting that the mainstream approach of estimating one vector representation per word type (rather than word token) ignores the fact that the same word may have different semantics in different contexts. The multiCluster method exacerbates this problem by estimating one vector representation per cluster of translationally equivalent words. The added semantic ambiguity severely hurts the performance of multiCluster with 59 languages, but it is still competitive with 12 languages (see below).
Results on [12 languages]. We compare the proposed dictionary-based estimation methods to parallel text-based methods in Table 3. The ranking of the four estimation methods is not consistent across all evaluation metrics. This is unsurprising since each metric evaluates different traits of word embeddings, as detailed in §3. However, some patterns are worth noting in Table 3.
In five of the evaluations (including both extrinsic tasks), the best performing method is a dictionary-based one proposed in this paper. In the remaining four intrinsic methods, the best performing method is the translation-invariance method. MultiSkip ranks last in five evaluations,
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Task multiCluster multiCCA multiSkip invariance extrinsic metrics dependency parsing 61.0 [70.9] 58.7 [69.3] 57.7 [68.9] 59.8 [68.6] document classification 92.1 [48.1] 92.1 [62.8] 90.4 [45.7] 91.1 [31.3]
intrinsic metrics
monolingual word similarity 38.0 [57.5] 43.0 [71.0] 33.9 [55.4] 51.0 [23.0] multilingual word similarity 58.1 [74.1] 66.6 [78.2] 59.5 [67.5] 58.7 [63.0]
word translation 43.7 [45.2] 35.7 [53.2] 46.7 [39.5] 63.9 [30.3] monolingual QVEC 10.3 [98.6] 10.7 [99.0] 8.4 [98.0] 8.1 [91.7]
multiQVEC 9.3 [82.0] 8.7 [87.0] 8.7 [87.0] 5.3 [74.7] monolingual QVEC-CCA 62.4 [98.6] 63.4 [99.0] 58.9 [98.0] 65.8 [91.7]
multiQVEC-CCA 43.3 [82.0] 41.5 [87.0] 36.3 [75.6] 46.2 [74.7]
Table 3: Results for multilingual embeddings that cover Bulgarian, Czech, Danish, Greek, English, Spanish, German, Finnish, French, Hungarian, Italian and Swedish. Each row corresponds to one of the embedding evaluation metrics we use (higher is better). Each column corresponds to one of the embedding estimation methods we consider; i.e., numbers in the same row are comparable. Numbers in square brackets are coverage percentages.
and never ranks first. Since our implementation of multiSkip does not make use of monolingual data, it only learns from monolingual contexts observed in parallel corpora, it misses the opportunity to learn from contexts in the much larger monolingual corpora. Trained for 12 languages, multiCluster is competitive in four evaluations (and ranks first in three).
We note that multiCCA consistently achieves better coverage than the translation-invariance method. For intrinsic measures, this confounds the performance comparison. A partial solution is to test only on word types for which all four methods have a vector; this subset is in no sense a representative sample of the vocabulary. In this comparison (provided in the supplementary material), we find a similar pattern of results, though multiCCA outperforms the translation-invariance method on the monolingual word similarity task. Also, the gap (between multiCCA and the translationinvariance method) reduces to 0.7 in monolingual QVEC-CCA and 2.5 in multiQVEC-CCA.","We introduce QVEC-CCA—an intrinsic evaluation measure of the quality of word embeddings. Our method is an improvement of QVEC—a monolingual evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource (Tsvetkov et al., 2015). We review QVEC, and then describe QVEC-CCA.
QVEC. The main idea behind QVEC is to quantify the linguistic content of word embeddings by maximizing the correlation with a manuallyannotated linguistic resource. Let the number of common words in the vocabulary of the word embeddings and the linguistic resource be N. To quantify the semantic content of embeddings, a semantic linguistic matrix S ∈ RP×N is constructed from a semantic database, with a column vector for each word. Each word vector is a distribution of the word over P linguistic properties, based on annotations of the word in the database. Let X ∈ RD×N be embedding matrix with every row as a dimension vector x ∈ R1×N . D denotes the dimensionality of word embeddings. Then, S and X are aligned to maximize the cumulative correlation between the aligned dimensions of the two
matrices. Specifically, let A ∈ {0, 1}D×P be a matrix of alignments such that aij = 1 iff xi is aligned to sj, otherwise aij = 0. If r(xi, sj) is the Pearson’s correlation between vectors xi and sj, then QVEC is defined as:
QVEC = maxA:∑j aij≤1 X∑
i=1 S∑ j=1 r(xi, sj)× aij
The constraint ∑
j aij ≤ 1, warrants that one distributional dimension is aligned to at most one linguistic dimension.
QVEC has been shown to correlate strongly with downstream semantic tasks (Tsvetkov et al., 2015). However, it suffers from two major weaknesses. First, it is not invariant to linear transformations of the embeddings’ basis, whereas the bases in word embeddings are generally arbitrary (Szegedy et al., 2014). Second, a sum of correlations produces an unnormalized score: the more dimensions in the embedding matrix the higher the score. This precludes comparison of models of different dimensionality. QVEC-CCA simultaneously addresses both problems.
QVEC-CCA. To measure correlation between the embedding matrix X and the linguistic matrix S, instead of cumulative dimension-wise correlation we employ CCA. CCA finds two sets of basis vectors, one for X> and the other for S>, such that the correlations between the projections of the matrices onto these basis vectors are maximized. Formally, CCA finds a pair of basis vectors v and w such that
QVEC-CCA = CCA(X>,S>)
= maxv,w r(X >v,S>w)
Thus, QVEC-CCA ensures invariance to the matrices bases rotation, and since it is a single correlation, it produces a score in [−1, 1]. Both QVEC and QVEC-CCA rely on a matrix of linguistic properties constructed from a manually crafted linguistic resource. We extend both methods to multilingual evaluations—multiQVEC and multiQVECCCA—by constructing the linguistic matrix using supersense tag annotations for English (Miller et al., 1993), Danish (Martı́nez Alonso et al., 2015; Martı́nez Alonso et al., 2016) and Italian (Montemagni et al., 2003).In order to facilitate future research on multilingual word embeddings, we developed a web portal to enable researchers who develop new estimation methods to evaluate them using a suite of evaluation tasks. The portal serves the following purposes:
• Download the monolingual and bilingual data we used to estimate multilingual embeddings in this paper,
• Download standard development/test data sets for each of the evaluation metrics to help re-
5Danish, German, English, Spanish, French, Italian and Swedish.
6http://hdl.handle.net/11234/LRT-1478
searchers working in this area report trustworthy and replicable results,7
• Upload arbitrary multilingual embeddings, scan which languages are covered by the embeddings, allow the user to pick among the compatible evaluation tasks, and receive evaluation scores for the selected tasks, and
• Register a new evaluation data set or a new evaluation metric via the github repository which mirrors the backend of the web portal.In this experiment, we consider four intrinsic evaluation metrics (cross-lingual word similarity, word translation, multiQVEC and multiQVECCCA) and two extrinsic evaluation metrics (multilingual document classification and multilingual parsing).
Data: For the cross-lingual word similarity task, we use disjoint subsets of the en-it MWS353 dataset (Leviant and Reichart, 2015) for development (308 word pairs) and testing (307 word pairs). For the word translation task, we use Wiktionary to extract a development set (647 translations) and a test set (647 translations) of translationally-equivalent word pairs in en-it, enda and da-it. For both multiQVEC and multiQVECCCA, we used disjoint subsets of the multilingual (en, da, it) supersense tag annotations described in §3 for development (12,513 types) and testing (12,512 types).
For the document classification task, we use the multilingual RCV corpus (en, it, da). For the dependency parsing task, we use the universal dependencies v1.1 (Agić et al., 2015) in three languages (en, da, it).
7Except for the original RCV documents, which are restricted by the Reuters license and cannot be republished. All other data is available for download.
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
(→) extrinsic task document dependency (↓) intrinsic metric classification parsing
word similarity 0.386 0.007 word translation 0.066 -0.292 multiQVEC 0.635 0.444 multiQVEC-CCA 0.896 0.273
Table 1: Correlations between intrinsic evaluation metrics (rows) and downstream task performance (columns).
Setup: To estimate correlations between the proposed intrinsic evaluation metrics and downstream task performance, we train a total of 17 different multilingual embeddings for three languages (English, Italian and Danish). To compute the correlations, we evaluate each of the 17 embeddings (12 multiCluster embeddings, 1 multiCCA embeddings, 1 multiSkip embeddings, 2 translation-invariance embeddings) according to each of the six evaluation metrics (4 intrinsic, 2 extrinsic).8
Results: Table 1 shows Pearson’s correlation coefficients of eight (intrinsic metric, extrinsic metric) pairs. Although each of two proposed methods multiQVEC and multiQVEC-CCA correlate better with a different extrinsic task, we establish (i) that intrinsic methods previously used in the literature (cross-lingual word similarity and word translation) correlate poorly with downstream tasks, and (ii) that the intrinsic methods proposed in this paper (multiQVEC and multiQVEC-CCA) correlate better with both downstream tasks, compared to cross-lingual word similarity and word translation.9","Vector-space representations of words are widely used in statistical models of natural language. In addition to improving the performance on standard monolingual NLP tasks, shared representation of words across languages offers intriguing possibilities (Klementiev et al., 2012). For example, in machine translation, translating a word never seen in parallel data may be overcome by seeking its vector-space neighbors, provided the embeddings are learned from both plentiful monolingual corpora and more limited parallel data. A second opportunity comes from transfer learning, in which models trained in one language can be deployed in other languages. While previous work has used hand-engineered features that are crosslinguistically stable as the basis model transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Tsvetkov et al., 2014), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al., 2012; Hermann and Blunsom, 2014; Guo et al., 2016). We there-
fore conjecture that developing estimation methods for massively multilingual word embeddings (i.e., embeddings for words in a large number of languages) will play an important role in the future of multilingual NLP.
This paper builds on previous work in multilingual embeddings and makes the following contributions:
• We propose two dictionary-based methods— multiCluster and multiCCA—for estimating multilingual embeddings which only require monolingual data and pairwise parallel dictionaries, and use them to train embeddings in 59 languages for which these resources are available (§2). Parallel corpora are not required but can be used when available. We show that the proposed methods work well in some settings and evaluation metrics.
• We adapt QVEC (Tsvetkov et al., 2015)1 to evaluating multilingual embeddings (multiQVEC). We also develop a new evaluation method multiQVEC-CCA which addresses a theoretical shortcoming of multiQVEC (§3). Compared to other intrinsic metrics used in the literature, we show that both multiQVEC and multiQVEC-CCA achieve better correlations with extrinsic tasks.
• We develop an easy-to-use web portal2 for evaluating arbitrary multilingual embeddings using a suite of intrinsic and extrinsic metrics (§4). Together with the provided benchmarks, the evaluation portal will substantially facilitate future research in this area.","There is a rich body of literature on bilingual embeddings, including work on machine translation (Zou et al., 2013; Hermann and Blunsom, 2014; Cho et al., 2014; Luong et al., 2015b; Luong et al., 2015a, inter alia),14 cross-lingual dependency parsing (Guo et al., 2015; Guo et al., 2016), and cross-lingual document classification (Klementiev
14Hermann and Blunsom (2014) showed that the bicvm method can be extended to more than two languages, but the released software library only supports bilingual embeddings.
et al., 2012; Gouws et al., 2014; Kociskỳ et al., 2014). Al-Rfou’ et al. (2013) trained word embeddings for more than 100 languages, but the embeddings of each language are trained independently (i.e., embeddings of words in different languages do not share the same vector space). Word clusters are a related form of distributional representation; in clustering, cross-lingual distributional representations were proposed as well (Och, 1999; Täckström et al., 2012). Haghighi et al. (2008) used CCA to learn bilingual lexicons from monolingual corpora.",,"We proposed two dictionary-based estimation methods for multilingual word embeddings, multiCCA and multiCluster, and used them to train embeddings for 59 languages. We characterized important shortcomings of the QVEC previously used to evaluate monolingual embeddings, and proposed an improved metric multiQVEC-CCA. Both multiQVEC and multiQVEC-CCA obtain better correlations with downstream tasks compared to intrinsic methods previously used in the literature. Finally, in order to help future research in this area, we created a web portal for users to upload their multilingual embeddings and easily evaluate them on nine evaluation metrics, with two modes of operation (development and test) to encourage sound experimentation practices."
27,"We introduce new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. Our estimation methods, multiCluster and multiCCA, use dictionaries and monolingual data; they do not require parallel data. Our new evaluation method, multiQVECCCA, is shown to correlate better than previous ones with two downstream tasks (text categorization and parsing). We also describe a web portal for evaluation that will facilitate further research in this area, along with open-source releases of all our methods.",Massively Multilingual Word Embeddings,143,"This paper proposes two dictionary-based methods for estimating multilingual
word embeddings, one motivated in clustering (MultiCluster) and another in
canonical correlation analysis (MultiCCA).
In addition, a supersense similarity measure is proposed that improves on QVEC
by substituting its correlation component with CCA, and by taking into account
multilingual evaluation.
 The evaluation is performed on a wide range of tasks using the web portal
developed by the authors; it is shown that in some cases the proposed
representation methods outperform two other baselines.

I think the paper is very well written, and represents a substantial amount of
work done. The presented representation-learning and evaluation methods are
certainly timely. I also applaud the authors for the meticulous documentation.

My general feel about this paper, however, is that it goes (perhaps) in too
much breadth at the expense of some depth. I'd prefer to see a thorougher
discussion of results (e.g. regarding the conflicting outcome for MultiCluster
between 59- and 12-language set-up; regarding the effect of estimation
parameters and decisions in MultiCluster/CCA). So, while I think the paper is
of high practical value to me and the research community (improved QVEC
measure, web portal), I frankly haven't learned that much from reading it, i.e.
in terms of research questions addressed and answered.

Below are some more concrete remarks.

It would make sense to include the correlation results (Table 1) for
monolingual QVEC and QVEC-CCA as well. After all, it is stated in l.326--328
that the proposed QVEC-CCA is an improvement over QVEC.

Minor:
l. 304: ""a combination of several cross-lingual word similarity datasets"" ->
this sounds as though they are of different nature, whereas they are really of
the same kind, just different languages, right?

p. 3: two equations exceed the column margin

Lines 121 and 147 only mention Coulmance et al and Guo et al when referring to
the MultiSkip baseline, but section 2.3 then only mentions Luong et al. So,
what's the correspondence between these works?

While I think the paper does reasonable justice in citing the related works,
there are more that are relevant and could be included:

Multilingual embeddings and clustering:
Chandar A P, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B.,
Raykar, V. C., and Saha, A. (2014). An autoencoder approach to learning
bilingual word representations. In NIPS.
Hill, F., Cho, K., Jean, S., Devin, C., and Bengio, Y. (2014). Embedding word
similarity with neural machine translation. arXiv preprint arXiv:1412.6448.
Lu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep
multilingual correlation for improved word embeddings. In NAACL.
Faruqui, M., & Dyer, C. (2013). An Information Theoretic Approach to Bilingual
Word Clustering. In ACL.

Multilingual training of embeddings for the sake of better source-language
embeddings:
Suster, S., Titov, I., and van Noord, G. (2016). Bilingual learning of
multi-sense embeddings with discrete autoencoders. In NAACL-HLT.
Guo, J., Che, W., Wang, H., and Liu, T. (2014). Learning sense-specific word
embeddings by exploiting bilingual resources. In COLING.

More broadly, translational context has been explored e.g. in
Diab, M., & Resnik, P. (2002). An unsupervised method for word sense tagging
using parallel corpora. In ACL.",,3,5,Poster,5,5,5,4,5,4,4,3,2016,"Let L be a set of languages, and let Vm be the set of surface forms (word types) in m ∈ L. Let
1A method for evaluating monolingual word embeddings. 2http://128.2.220.95/multilingual
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
V = ⋃
m∈L Vm. Our goal is to estimate a partial embedding function E : L × V 7→ Rd (allowing a surface form that appears in two languages to have different vectors in each). We would like to estimate this function such that: (i) semantically similar words in the same language are nearby, (ii) translationally equivalent words in different languages are nearby, and (iii) the domain of the function covers as many words in V as possible.
We use distributional similarity in a monolingual corpus Mm to model semantic similarity between words in the same language. For crosslingual similarity, either a parallel corpus Pm,n or a bilingual dictionary Dm,n ⊂ Vm ×Vn can be used. Our methods focus on the latter, in some cases extracting Dm,n from a parallel corpus.3
Most previous work on multilingual embeddings only considered the bilingual case, | L |= 2. We focus on estimating multilingual embeddings for | L |> 2 and describe two novel dictionarybased methods (multiCluster and multiCCA). We then describe our baselines: a variant of Coulmance et al. (2015) and Guo et al. (2016) (henceforth referred to as multiSkip),4 and the translation-invariance matrix factorization method (Gardner et al., 2015).In this approach, we decompose the problem into two simpler subproblems: E = Eembed ◦ Ecluster, where Ecluster : L×V 7→ C deterministically maps words to multilingual clusters C, and Eembed : C → Rd assigns a vector to each cluster. We use a bilingual dictionary to find clusters of translationally equivalent words, then use distributional similarities of the clusters in monolingual corpora from all languages in L to estimate an embedding for each cluster. By forcing words from different languages in a cluster to share the same embedding, we create anchor points in the vector space to bridge languages.
More specifically, we define the clusters as the
3To do this, we align the corpus using fast align (Dyer et al., 2013) in both directions. The estimated parameters of the word translation distributions are used to select pairs: Dm,n ={ (u, v) | u ∈ Vm, v ∈ Vn, pm|n(u | v)× pn|m(v | u) > τ } , where the threshold τ trades off dictionary recall and precision. We fixed τ = 0.1 early on based on manual inspection of the resulting dictionaries.
4We developed multiSkip independently of Coulmance et al. (2015) and Guo et al. (2016). One important distinction is that multiSkip is only trained on parallel corpora, while Coulmance et al. (2015) and Guo et al. (2016) also use monolingual corpora.
connected components in a graph where nodes are (language, surface form) pairs and edges correspond to translation entries in Dm,n. We assign arbitrary IDs to the clusters and replace each word token in each monolingual corpus with the corresponding cluster ID, and concatenate all modified corpora. The resulting corpus consists of multilingual cluster ID sequences. We can then apply any monolingual embedding estimator; here, we use the skipgram model from Mikolov et al. (2013a).Our proposed method (multiCCA) extends the bilingual embeddings of Faruqui and Dyer (2014). First, they use monolingual corpora to train monolingual embeddings for each language independently (Em and En), capturing semantic similarity within each language separately. Then, using a bilingual dictionary Dm,n, they use canonical correlation analysis (CCA) to estimate linear projections from the ranges of the monolingual embeddings Em and En, yielding a bilingual embedding Em,n. The linear projections are defined by Tm→m,n and Tn→m,n ∈ Rd×d; they are selected to maximize the correlation between Tm→m,nEm(u) and Tn→m,nEn(v) where (u, v) ∈ Dm,n. The bilingual embedding is then defined as ECCA(m, u) = Tm→m,nEm(u) (and likewise for ECCA(n, v)).
In this work, we use a simple extension (in hindsight) to construct multilingual embeddings for more languages. We let the vector space of the initial (monolingual) English embeddings serve as the multilingual vector space (since English typically offers the largest corpora and wide availability of bilingual dictionaries). We then estimate projections from the monolingual embeddings of the other languages into the English space.
We start by estimating, for each m ∈ L \ {en}, the two projection matrices: Tm→m,en and Ten→m,en; these are guaranteed to be non-singular. We then define the multilingual embedding as ECCA(en, u) = Een(u) for u ∈ Ven, and ECCA(m, v) = T−1en→m,enTm→m,enE
m(v) for v ∈ Vm,m ∈ L \ {en}.Luong et al. (2015b) proposed a method for estimating bilingual embeddings which only makes use of parallel data; it extends the skipgram model of Mikolov et al. (2013a). The skipgram model defines a distribution over words u that occur in a
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
context window (of size K) of a word v:
p(u | v) = expEskipgram(m, v)>Econtext(m, u)∑
u′∈Vm expEskipgram(m, v)>Econtext(m, u′)
In practice, this distribution can be estimated using a noise contrastive estimation approximation (Gutmann and Hyvärinen, 2012) while maximizing the log-likelihood:∑
i∈pos(Mm) ∑ k∈{−K,...,−1,1,...,K} log p(ui+k | ui)
where pos(Mm) are the indices of words in the monolingual corpus Mm.
To establish a bilingual embedding, with a parallel corpus Pm,n of source language m and target language n, Luong et al. (2015b) estimate conditional models of words in both source and target positions. The source positions are selected as sentential contexts (similar to monolingual skipgram), and the bilingual contexts come from aligned words. The bilingual objective is to maximize:∑ i∈m-pos(Pm,n) ∑ k∈{−K,...,−1,1,...,K} log p(ui+k | ui) + log p(va(i)+k | ui)
+ ∑
j∈n-pos(Pm,n) ∑ k∈{−K,...,−1,1,...,K} log p(vj+k | vj) + log p(ua(j)+k | vj)
where m-pos(Pm,n) and n-pos(Pm,n) are the indeces of the source and target tokens in the parallel corpus respectively, a(i) and a(j) are the positions of words that align to i and j in the other language. It is easy to see how this method can be extended for more than two languages by summing up the bilingual objectives for all available parallel corpora.Gardner et al. (2015) proposed that multilingual embeddings should be translation invariant. Consider a matrix X ∈ R|V|×|V| which summarizes the pointwise mutual information statistics between pairs of words in monolingual corpora, and let UV> be a low-rank decomposition of X where U,V ∈ R|V|×d. Now, consider another matrix A ∈ R|V|×|V| which summarizes bilingual alignment frequencies in a parallel corpus. Gardner et al. (2015) solves for a low-rank decomposition UV> which both approximates X as well as its
transformations A>X, XA and A>XA by defining the following objective:
minU,V ‖X − UV >‖2 + ‖XA− UV>‖2 + ‖A>X − UV>‖2 + ‖A>XA− UV>‖2
The multilingual embeddings are then taken to be the rows of the matrix U.One of our contributions is to streamline the evaluation of multilingual embeddings. In addition to assessing goals (i–iii) stated in §2, a good evaluation metric should also (iv) show good correlation with performance in downstream applications and (v) be computationally efficient.
It is easy to evaluate the coverage (iii) by counting the number of words covered by an embedding function in a closed vocabulary. Intrinsic evaluation metrics are generally designed to be computationally efficient (v) but may or may not meet the goals (i, ii, iv). Although intrinsic evaluations will never be perfect, a standard set of evaluation metrics will help drive research. By design, standard (monolingual) word similarity tasks meet (i) while cross-lingual word similarity tasks and the word translation tasks meet (ii). We propose another evaluation method (multiQVEC-CCA), designed to simultaneously assess goals (i, ii). MultiQVECCCA extends QVEC (Tsvetkov et al., 2015), a recently proposed monolingual evaluation method, addressing fundamental flaws and extending it to multiple languages. To assess the degree to which these evaluation metrics meet (iv), in §5 we perform a correlation analysis looking at which intrinsic metrics are best correlated with downstream task performance—i.e., we evaluate the evaluation metrics.Word similarity datasets such as WordSim-353 (Agirre et al., 2009) and MEN (Bruni et al., 2014) provide human judgments of semantic similarity. By ranking words by cosine similarity and by their empirical similarity judgments, a ranking correlation can be computed that assesses how well the estimated vectors capture human intuitions about semantic relatedness.
Some previous work on bilingual and multilingual embeddings focuses on monolingual word similarity to evaluate embeddings (e.g., Faruqui and Dyer, 2014). This approach is limited because
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
it cannot measure the degree to which embeddings from different languages are similar (ii). For this paper, we report results on an English word similarity task, the Stanford RW dataset (Luong et al., 2013), as well as a combination of several cross-lingual word similarity datasets (CamachoCollados et al., 2015).This task directly assesses the degree to which translationally equivalent words in different languages are nearby in the embedding space. The evaluation data consists of word pairs which are known to be translationally equivalent. The score for one word pair (l1,w1), (l2,w2) both of which are covered by an embedding E is 1 if cosine(E(l1,w1),E(l2,w2)) ≥ cosine(E(l1,w1),E(l2,w′2))∀w′2 ∈ Gl2 where Gl2 is the set of words of language l2 in the evaluation dataset, and cosine is the cosine similarity function. Otherwise, the score for this word pair is 0. The overall score is the average score for all word pairs covered by the embedding function. This is a variant of the method used by Mikolov et al. (2013b) to evaluate bilingual embeddings.In order to evaluate how useful the word embeddings are for a downstream task, we use the em-
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
bedding vector as a dense feature representation of each word in the input, and deliberately remove any other feature available for this word (e.g., prefixes, suffixes, part-of-speech). For each task, we train one model on the aggregate training data available for several languages, and evaluate on the aggregate evaluation data in the same set of languages. We apply this for multilingual document classification and multilingual dependency parsing.
For document classification, we follow Klementiev et al. (2012) in using the RCV corpus of newswire text, and train a classifier which differentiates between four topics. While most previous work used this data only in a bilingual setup, we simultaneously train the classifier on documents in seven languages,5 and evaluate on the development/test section of those languages. For this task, we report the average classification accuracy on the test set.
For dependency parsing, we train the stackLSTM parser of Dyer et al. (2015) on a subset of the languages in the universal dependencies v1.1,6 and test on the same languages, reporting unlabeled attachment scores. We remove all part-ofspeech and morphology features from the data, and prevent the model from optimizing the word embeddings used to represent each word in the corpus, thereby forcing the parser to rely completely on the provided (pretrained) embeddings as the token representation. Although omitting other features (e.g., parts of speech) hurts the performance of the parser, it emphasizes the contribution of the word embeddings being studied.Our experiments are designed to show two primary sets of results: (i) how well the proposed intrinsic evaluation metrics correlate with downstream tasks (§5.1) and (ii) which estimation methods work best according to each metric (§5.2). The data used for training and evaluation are available for download on the evaluation portal.We now turn to evaluating the four estimation methods described in §2. We use the proposed methods (i.e., multiCluster and multiCCA) to
8The 102 (17 × 6) values used to compute Pearson’s correlation coefficient are provided in the supplementary material.
9Although supersense annotations exist for other languages, the annotations are inconsistent across languages and may not be publicly available, which is a disadvantage of the multiQVEC and multiQVEC-CCA metrics. Therefore, we recommend that future multilingual supersense annotation efforts use the same set of supersense tags used in other languages. If the word embeddings are primarily needed for encoding syntactic information, one could use tag dictionaries based on the universal POS tag set (Petrov et al., 2012) instead of supersense tags.
train multilingual embeddings in 59 languages for which bilingual translation dictionaries are available.10 In order to compare our methods to baselines which use parallel data (i.e., multiSkip and translation-invariance), we also train multilingual embeddings in a smaller set of 12 languages for which high-quality parallel data are available.11
Training data: We use Europarl en-xx parallel data for the set of 12 languages. We obtain en-xx bilingual dictionaries from two different sources. For the set of 12 languages, we extract the bilingual dictionaries from the Europarl parallel corpora. For the remaining 47 languages, dictionaries were formed by translating the 20k most common words in the English monolingual corpus with Google Translate, ignoring translation pairs with identical surface forms and multi-word translations.
Evaluation data: Monolingual word similarity uses the MEN dataset in Bruni et al. (2014) as a development set and Stanford’s Rare Words dataset in Luong et al. (2013) as a test set. For the crosslingual word similarity task, we aggregate the RG65 datasets in six language pairs (fr-es, fr-de, enfr, en-es, en-de, de-es). For the word translation
10The 59-language set is { bg, cs, da, de, el, en, es, fi, fr, hu, it, sv, zh, af, ca, iw, cy, ar, ga, zu, et, gl, id, ru, nl, pt, la, tr, ne, lv, lt, tg, ro, is, pl, yi, be, hy, hr, jw, ka, ht, fa, mi, bs, ja, mg, tl, ms, uz, kk, sr, mn, ko, mk, so, uk, sl, sw }.
11The 12-language set is {bg, cs, da, de, el, en, es, fi, fr, hu, it, sv}.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
task, we use Wiktionary to extract translationallyequivalent word pairs to evaluate multilingual embeddings for the set of 12 languages. Since Wiktionary-based translations do not cover all 59 languages, we use Google Translate to obtain enxx bilingual dictionaries to evaluate the embeddings of 59 languages. For QVEC and QVEC-CCA, we split the English supersense annotations used in Tsvetkov et al. (2015) into a development set and a test set. For multiQVEC and multiQVECCCA, we use supersense annotations in English, Italian and Danish. For the document classification task, we use the multilingual RCV corpus in seven languages (da, de, en, es, fr, it, sv). For the dependency parsing task, we use the universal dependencies v1.1 in twelve languages (bg, cs, da, de, el, en, es, fi, fr, hu, it, sv).
Setup: All word embeddings in the following results are 512-dimensional vectors. Methods which indirectly use skipgram (i.e., multiCCA, multiSkip, and multiCluster) are trained using 10 epochs of stochastic gradient descent, and use a context window of size 5. The translation-invariance method use a context window of size 3.12 We only estimate embeddings for words/clusters which occur 5 times or more in the monolingual corpora. In a postprocessing step, all vectors are normalized to unit length. MultiCluster uses a maximum cluster size of 1,000 and 10,000 for the set of 12 and 59 languages, respectively. In the English tasks (monolingual word similarity, QVEC, QVEC-CCA), skipgram embeddings (Mikolov et al., 2013a) and multiCCA embeddings give identical results (since we project words in other languages to the English vector space, estimated using the skipgram model). The software used to train all embeddings as well as the trained embeddings are available for download on the evaluation portal.13
We note that intrinsic evaluation of word embeddings (e.g., word similarity) typically ignores test instances which are not covered by the embeddings being studied. When the vocabulary used in two sets of word embeddings is different, which is often the case, the intrinsic evaluation score for each set may be computed based on a different set
12Training translation-invariance embeddings with larger context window sizes using the matlab implementation provided by Gardner et al. (2015) is computationally challenging.
13URLs to software libraries on Github are redacted to comply with the double-blind reviewing of CoNLL.
of test instances, which may bias the results in unexpected ways. For instance, if one set of embeddings only covers frequent words while the other set also covers infrequent words, the scores of the first set may be inflated because frequent words appear in many different contexts and are therefore easier to estimate than infrequent words. To partially address this problem, we report the coverage of each set of embeddings in square brackets. When the difference in coverage is large, we repeat the evaluation using only the intersection of vocabularies covered by all embeddings being evaluated. Extrinsic evaluations are immune to this problem because the score is computed based on all test instances regardless of the coverage.
Results [59 languages]. We train the proposed dictionary-based estimation methods (multiCluster and multiCCA) for 59 languages, and evaluate the trained embeddings according to nine different metrics in Table 2. The results show that, when trained on a large number of languages, multiCCA consistently outperforms multiCluster according to all evaluation metrics. Note that most differences in coverage between multiCluster and multiCCA are relatively small.
It is worth noting that the mainstream approach of estimating one vector representation per word type (rather than word token) ignores the fact that the same word may have different semantics in different contexts. The multiCluster method exacerbates this problem by estimating one vector representation per cluster of translationally equivalent words. The added semantic ambiguity severely hurts the performance of multiCluster with 59 languages, but it is still competitive with 12 languages (see below).
Results on [12 languages]. We compare the proposed dictionary-based estimation methods to parallel text-based methods in Table 3. The ranking of the four estimation methods is not consistent across all evaluation metrics. This is unsurprising since each metric evaluates different traits of word embeddings, as detailed in §3. However, some patterns are worth noting in Table 3.
In five of the evaluations (including both extrinsic tasks), the best performing method is a dictionary-based one proposed in this paper. In the remaining four intrinsic methods, the best performing method is the translation-invariance method. MultiSkip ranks last in five evaluations,
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Task multiCluster multiCCA multiSkip invariance extrinsic metrics dependency parsing 61.0 [70.9] 58.7 [69.3] 57.7 [68.9] 59.8 [68.6] document classification 92.1 [48.1] 92.1 [62.8] 90.4 [45.7] 91.1 [31.3]
intrinsic metrics
monolingual word similarity 38.0 [57.5] 43.0 [71.0] 33.9 [55.4] 51.0 [23.0] multilingual word similarity 58.1 [74.1] 66.6 [78.2] 59.5 [67.5] 58.7 [63.0]
word translation 43.7 [45.2] 35.7 [53.2] 46.7 [39.5] 63.9 [30.3] monolingual QVEC 10.3 [98.6] 10.7 [99.0] 8.4 [98.0] 8.1 [91.7]
multiQVEC 9.3 [82.0] 8.7 [87.0] 8.7 [87.0] 5.3 [74.7] monolingual QVEC-CCA 62.4 [98.6] 63.4 [99.0] 58.9 [98.0] 65.8 [91.7]
multiQVEC-CCA 43.3 [82.0] 41.5 [87.0] 36.3 [75.6] 46.2 [74.7]
Table 3: Results for multilingual embeddings that cover Bulgarian, Czech, Danish, Greek, English, Spanish, German, Finnish, French, Hungarian, Italian and Swedish. Each row corresponds to one of the embedding evaluation metrics we use (higher is better). Each column corresponds to one of the embedding estimation methods we consider; i.e., numbers in the same row are comparable. Numbers in square brackets are coverage percentages.
and never ranks first. Since our implementation of multiSkip does not make use of monolingual data, it only learns from monolingual contexts observed in parallel corpora, it misses the opportunity to learn from contexts in the much larger monolingual corpora. Trained for 12 languages, multiCluster is competitive in four evaluations (and ranks first in three).
We note that multiCCA consistently achieves better coverage than the translation-invariance method. For intrinsic measures, this confounds the performance comparison. A partial solution is to test only on word types for which all four methods have a vector; this subset is in no sense a representative sample of the vocabulary. In this comparison (provided in the supplementary material), we find a similar pattern of results, though multiCCA outperforms the translation-invariance method on the monolingual word similarity task. Also, the gap (between multiCCA and the translationinvariance method) reduces to 0.7 in monolingual QVEC-CCA and 2.5 in multiQVEC-CCA.","We introduce QVEC-CCA—an intrinsic evaluation measure of the quality of word embeddings. Our method is an improvement of QVEC—a monolingual evaluation based on alignment of embeddings to a matrix of features extracted from a linguistic resource (Tsvetkov et al., 2015). We review QVEC, and then describe QVEC-CCA.
QVEC. The main idea behind QVEC is to quantify the linguistic content of word embeddings by maximizing the correlation with a manuallyannotated linguistic resource. Let the number of common words in the vocabulary of the word embeddings and the linguistic resource be N. To quantify the semantic content of embeddings, a semantic linguistic matrix S ∈ RP×N is constructed from a semantic database, with a column vector for each word. Each word vector is a distribution of the word over P linguistic properties, based on annotations of the word in the database. Let X ∈ RD×N be embedding matrix with every row as a dimension vector x ∈ R1×N . D denotes the dimensionality of word embeddings. Then, S and X are aligned to maximize the cumulative correlation between the aligned dimensions of the two
matrices. Specifically, let A ∈ {0, 1}D×P be a matrix of alignments such that aij = 1 iff xi is aligned to sj, otherwise aij = 0. If r(xi, sj) is the Pearson’s correlation between vectors xi and sj, then QVEC is defined as:
QVEC = maxA:∑j aij≤1 X∑
i=1 S∑ j=1 r(xi, sj)× aij
The constraint ∑
j aij ≤ 1, warrants that one distributional dimension is aligned to at most one linguistic dimension.
QVEC has been shown to correlate strongly with downstream semantic tasks (Tsvetkov et al., 2015). However, it suffers from two major weaknesses. First, it is not invariant to linear transformations of the embeddings’ basis, whereas the bases in word embeddings are generally arbitrary (Szegedy et al., 2014). Second, a sum of correlations produces an unnormalized score: the more dimensions in the embedding matrix the higher the score. This precludes comparison of models of different dimensionality. QVEC-CCA simultaneously addresses both problems.
QVEC-CCA. To measure correlation between the embedding matrix X and the linguistic matrix S, instead of cumulative dimension-wise correlation we employ CCA. CCA finds two sets of basis vectors, one for X> and the other for S>, such that the correlations between the projections of the matrices onto these basis vectors are maximized. Formally, CCA finds a pair of basis vectors v and w such that
QVEC-CCA = CCA(X>,S>)
= maxv,w r(X >v,S>w)
Thus, QVEC-CCA ensures invariance to the matrices bases rotation, and since it is a single correlation, it produces a score in [−1, 1]. Both QVEC and QVEC-CCA rely on a matrix of linguistic properties constructed from a manually crafted linguistic resource. We extend both methods to multilingual evaluations—multiQVEC and multiQVECCCA—by constructing the linguistic matrix using supersense tag annotations for English (Miller et al., 1993), Danish (Martı́nez Alonso et al., 2015; Martı́nez Alonso et al., 2016) and Italian (Montemagni et al., 2003).In order to facilitate future research on multilingual word embeddings, we developed a web portal to enable researchers who develop new estimation methods to evaluate them using a suite of evaluation tasks. The portal serves the following purposes:
• Download the monolingual and bilingual data we used to estimate multilingual embeddings in this paper,
• Download standard development/test data sets for each of the evaluation metrics to help re-
5Danish, German, English, Spanish, French, Italian and Swedish.
6http://hdl.handle.net/11234/LRT-1478
searchers working in this area report trustworthy and replicable results,7
• Upload arbitrary multilingual embeddings, scan which languages are covered by the embeddings, allow the user to pick among the compatible evaluation tasks, and receive evaluation scores for the selected tasks, and
• Register a new evaluation data set or a new evaluation metric via the github repository which mirrors the backend of the web portal.In this experiment, we consider four intrinsic evaluation metrics (cross-lingual word similarity, word translation, multiQVEC and multiQVECCCA) and two extrinsic evaluation metrics (multilingual document classification and multilingual parsing).
Data: For the cross-lingual word similarity task, we use disjoint subsets of the en-it MWS353 dataset (Leviant and Reichart, 2015) for development (308 word pairs) and testing (307 word pairs). For the word translation task, we use Wiktionary to extract a development set (647 translations) and a test set (647 translations) of translationally-equivalent word pairs in en-it, enda and da-it. For both multiQVEC and multiQVECCCA, we used disjoint subsets of the multilingual (en, da, it) supersense tag annotations described in §3 for development (12,513 types) and testing (12,512 types).
For the document classification task, we use the multilingual RCV corpus (en, it, da). For the dependency parsing task, we use the universal dependencies v1.1 (Agić et al., 2015) in three languages (en, da, it).
7Except for the original RCV documents, which are restricted by the Reuters license and cannot be republished. All other data is available for download.
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
(→) extrinsic task document dependency (↓) intrinsic metric classification parsing
word similarity 0.386 0.007 word translation 0.066 -0.292 multiQVEC 0.635 0.444 multiQVEC-CCA 0.896 0.273
Table 1: Correlations between intrinsic evaluation metrics (rows) and downstream task performance (columns).
Setup: To estimate correlations between the proposed intrinsic evaluation metrics and downstream task performance, we train a total of 17 different multilingual embeddings for three languages (English, Italian and Danish). To compute the correlations, we evaluate each of the 17 embeddings (12 multiCluster embeddings, 1 multiCCA embeddings, 1 multiSkip embeddings, 2 translation-invariance embeddings) according to each of the six evaluation metrics (4 intrinsic, 2 extrinsic).8
Results: Table 1 shows Pearson’s correlation coefficients of eight (intrinsic metric, extrinsic metric) pairs. Although each of two proposed methods multiQVEC and multiQVEC-CCA correlate better with a different extrinsic task, we establish (i) that intrinsic methods previously used in the literature (cross-lingual word similarity and word translation) correlate poorly with downstream tasks, and (ii) that the intrinsic methods proposed in this paper (multiQVEC and multiQVEC-CCA) correlate better with both downstream tasks, compared to cross-lingual word similarity and word translation.9","Vector-space representations of words are widely used in statistical models of natural language. In addition to improving the performance on standard monolingual NLP tasks, shared representation of words across languages offers intriguing possibilities (Klementiev et al., 2012). For example, in machine translation, translating a word never seen in parallel data may be overcome by seeking its vector-space neighbors, provided the embeddings are learned from both plentiful monolingual corpora and more limited parallel data. A second opportunity comes from transfer learning, in which models trained in one language can be deployed in other languages. While previous work has used hand-engineered features that are crosslinguistically stable as the basis model transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Tsvetkov et al., 2014), automatically learned embeddings offer the promise of better generalization at lower cost (Klementiev et al., 2012; Hermann and Blunsom, 2014; Guo et al., 2016). We there-
fore conjecture that developing estimation methods for massively multilingual word embeddings (i.e., embeddings for words in a large number of languages) will play an important role in the future of multilingual NLP.
This paper builds on previous work in multilingual embeddings and makes the following contributions:
• We propose two dictionary-based methods— multiCluster and multiCCA—for estimating multilingual embeddings which only require monolingual data and pairwise parallel dictionaries, and use them to train embeddings in 59 languages for which these resources are available (§2). Parallel corpora are not required but can be used when available. We show that the proposed methods work well in some settings and evaluation metrics.
• We adapt QVEC (Tsvetkov et al., 2015)1 to evaluating multilingual embeddings (multiQVEC). We also develop a new evaluation method multiQVEC-CCA which addresses a theoretical shortcoming of multiQVEC (§3). Compared to other intrinsic metrics used in the literature, we show that both multiQVEC and multiQVEC-CCA achieve better correlations with extrinsic tasks.
• We develop an easy-to-use web portal2 for evaluating arbitrary multilingual embeddings using a suite of intrinsic and extrinsic metrics (§4). Together with the provided benchmarks, the evaluation portal will substantially facilitate future research in this area.","There is a rich body of literature on bilingual embeddings, including work on machine translation (Zou et al., 2013; Hermann and Blunsom, 2014; Cho et al., 2014; Luong et al., 2015b; Luong et al., 2015a, inter alia),14 cross-lingual dependency parsing (Guo et al., 2015; Guo et al., 2016), and cross-lingual document classification (Klementiev
14Hermann and Blunsom (2014) showed that the bicvm method can be extended to more than two languages, but the released software library only supports bilingual embeddings.
et al., 2012; Gouws et al., 2014; Kociskỳ et al., 2014). Al-Rfou’ et al. (2013) trained word embeddings for more than 100 languages, but the embeddings of each language are trained independently (i.e., embeddings of words in different languages do not share the same vector space). Word clusters are a related form of distributional representation; in clustering, cross-lingual distributional representations were proposed as well (Och, 1999; Täckström et al., 2012). Haghighi et al. (2008) used CCA to learn bilingual lexicons from monolingual corpora.",,"We proposed two dictionary-based estimation methods for multilingual word embeddings, multiCCA and multiCluster, and used them to train embeddings for 59 languages. We characterized important shortcomings of the QVEC previously used to evaluate monolingual embeddings, and proposed an improved metric multiQVEC-CCA. Both multiQVEC and multiQVEC-CCA obtain better correlations with downstream tasks compared to intrinsic methods previously used in the literature. Finally, in order to help future research in this area, we created a web portal for users to upload their multilingual embeddings and easily evaluate them on nine evaluation metrics, with two modes of operation (development and test) to encourage sound experimentation practices."
28,"This paper studies how word embeddings trained on the British National Corpus interact with part of speech boundaries. Our work targets the Universal PoS tag set, which is currently actively being used for annotation of a range of languages. We experiment with training classifiers for predicting PoS tags for words based on their embeddings. The results show that the information about PoS affiliation contained in the distributional vectors allows us to discover groups of words with distributional patterns that differ from other words of the same part of speech.   This data often reveals hidden inconsistencies of the annotation process or guidelines. At the same time, it supports the notion of `soft' or `graded' part of speech affiliations. Finally, we show that information about PoS is distributed among dozens of vector components, not limited to only one or two features.",Redefining part-of-speech classes with distributional semantic models,163,"The aim of this paper is to show that distributional information stored in word
vector models contain information about POS labels. They use a version of the
BNC annotated with UD POS and in which words have been replaced by lemmas. They
train word embeddings on this corpus, then use the resulting vectors to train a
logistic classifier to predict the word POS. Evaluations are performed on the
same corpus (using cross-validation) as well as on other corpora. Results are
clearly presented and discussed and analyzed at length.

The paper is clear and well-written. The main issue with this paper is that it
does not contain anything new in terms of NLP or ML. It describe a set of
straightforward experiments without any new NLP or ML ideas or methods. Results
are interesting indeed, in so far that they provide an empirical grounding to
the notion of POS. In that regard, it is certainly worth being published in a
(quantitative/emprirical) linguistic venue.

On another note, the literature on POS tagging and POS induction using word
embeddings should be cited more extensively (cf. for instance Lin, Ammar, Duer
and Levin 2015; Ling et al. 2015 [EMNLP]; Plank, SÃ¸gaard and Goldberg
2016...).",,2,5,Poster,4,3,1,4,4,3,2,2,2016,"Our hypothesis is that for the majority of words their part of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word embeddings do contain PoS-related data, the properly trained classifier will correctly predict PoS tags for the majority of words: it means that these lexical entities conform to a dominant distributional pattern of their part of speech class. At the same time, the words for which the classifier outputs incorrect predictions, are expected to be ‘outliers’, with different distributional patterns, different from other words in the same class. These cases are the points of linguistic interest, and in the rest of the paper we mostly concentrate on them.
To test the initial hypothesis, we used the XML Edition of British National Corpus (BNC), a balanced and representative corpus of English language of about 98 million word tokens in size. As stated in the corpus documentation, ‘it was [PoS]tagged automatically, using the CLAWS4 automatic tagger developed by Roger Garside at Lancaster, and a second program, known as Template Tagger, developed by Mike Pacey and Steve Fligelstone’ (Burnard, 2007). The corpus authors report a precision of 0.96 and recall of 0.99 for their tools, based on a manually checked sample. For this research, it is important that BNC is an established and well-studied corpus of English with PoS-tags and lemmas assigned to all words.
We produced a version of BNC where all the words were replaced with their lemmas and PoStags converted into the Universal Part-of-Speech Tagset (Petrov et al., 2012)1. Thus, each token was represented as a concatination of its lemma and PoS tag (for example, ‘love_VERB’ and ‘love_NOUN’ yield different word types). The mappings between BNC tags and Universal tags were created manually by us and released online2. We worked with the following 16 Universal tags: ADJ, ADP, ADV, AUX, CONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, SCONJ, SYM, VERB, X (tokens marked with PUNCT tag were excluded).
1We used the latest version of the tagset available at http://universaldependencies.org
2Anonymized
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
Then, a Continuous Skipgram embedding model (Mikolov et al., 2013a) was trained on this corpus, using a vector size of 300, 10 negative samples, a symmetric window of 2 words, no down-sampling, and 5 iterations over the training data. Words with corpus frequency less than 5 were ignored. This model was then taken to represent the semantics of the words it contained. But at the same time, for each word, a gold standard PoS tag is known (from the BNC annotation). It means that is is possible to test how good the word embeddings are in grouping words according to their parts of speech.
To this end, we extracted vectors for the 10 000 most frequent words from the resulting model (roughly, these are the words with corpus frequency more than 500). Then, these vectors were used to train a simple logistic regression multinomial classifier aimed to predict the word’s part of speech3.
Note that during training (and subsequent testing), each word’s vector was used several times, proportional to frequency of the word in the corpus, so the classifier was trained on 177 343 (sometimes repeating) instances, instead of the original 10 000. This was done to alleviate the classifier’s part of speech bias. There are much fewer word types in the closed PoS classes (pronouns, conjunctions, etc.) than in the open ones (nouns, verbs, etc.), so without considering word frequency, the model does not have a chance to learn good predictors for ‘rare’ classes and ends up never predicting them. At the same time, words from closed classes occur very frequently in the running text, so after ‘weighting’ training instances by corpus frequency, the balance is restored and the classifier model has enough training instances to learn to predict closed PoS classes as well. As an additional benefit, by this modification we make frequent words from all classes to be more ‘influential’ in training the classifier.
The resulting classifier showed a weighted average F-score equal to 0.979 with 10-fold crossvalidation on the training set. This is a significant improvement over the majority class baseline classifier (classify everything as nouns), which showed an F-score of 0.07 and over the onefeature baseline classifier (classify using only one
3It is important that we applied classification, not clustering here. Attempts to naively cluster word vectors into the number of clusters equal to the number of PoS tags inescapably failed.
vector dimension with maximum F-value in relation to class tags), with F-score equal to only 0.22. Thus, the results support the hypothesis that word embeddings contain information allowing to group words together based on their parts of speech. At the same time, we see that this information is not restricted to some particular vector component: rather, it is distributed among several axis of the vector space.
After training the classifier, we were able to use it to detect ‘outlying’ words in the BNC (judging by the distributional model). So as not to experiment on the same data we had trained our classifier on, we compiled another test set of 17 000 vectors for words with BNC frequencies between 100 and 500. They were weighted by word frequencies in the same way as the training set, and the resulting test set contained 30 710 instances. Then, we predicted parts of speech for these words using our classifier and evaluated its performance on them. The results on totally unseen data were not much worse, with an F-score equal to 0.91.
Furthermore, to make sure that the results can potentially be extended to other texts, we applied the trained classifier to all lemmas from the human-annotated Universal Dependencies English Treebank (Silveira et al., 2014). The words not present in the distributional model were omitted (they sum to 27% of word types and 10% of word tokens). The classifier showed an F-Score equal to 0.99, further demonstrating the robustness of the classifier.
In sum, the vast majority of words are classified correctly, which means that their embeddings enables detecting their parts of speech. In fact, one can even visualize ‘prototypical’ vectors for each PoS by simply averaging vectors of words belonging to this part of speech. We did this for 10 000 words from our training set.
Plots for prototypical vectors of coordinating and subordinating conjunctions are shown in the Figures 1 and 2 respectively. Even visually one can notice a very strongly expressed feature near the ‘100’ mark in the horizontal axis. In fact, this is vector component number 94, and it is indeed an idiosyncratic feature of conjunctions: none of other parts of speech shows such a property. More details about what vector components are relevant to part of speech affiliation are given in Section 5.
With prototypical PoS vectors we can even find out how similar different parts of speech are to
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
Figure 1. Prototypical embedding for coordinating conjunctions
Figure 2. Prototypical embedding for subordinating conjunctions
each other, by simply measuring cosine similarity between them. If we rank PoS pairs according to their similarity, what we see is that nominative parts of speech are close to each other, determiners and pronouns are also similar, as well as prepositions and subordinating conjunctions, everything quite in accordance with language intuition. It is interesting that proper nouns are not much similar to common nouns, with cosine similarity between them only 0.67 (even adverbs are closer). As we show below, this helps the model to successfully separate the former from the latter.
Despite generally good performance of the classifier, if we look at our BNC test set, 1741 word types (about 10% of the whole test set vocabulary) were still classified incorrectly. Thus, they are somehow dissimilar to ‘prototypical’ words of their parts of speech. These are the ‘outliers’ we were after. We investigate the patterns found among them in the next section.We filtered out mis-classified word types with ‘X’ gold annotation (they are mostly foreign or nonsense words). This left us with 1558 words; the classifier assigned them part of speech tags
different from the ones in the BNC. It probably means that these words’ distributional patterns differ somehow from the ‘mainstream’, and they tend to exhibit behavior similar to another part of speech. Table 1 shows the most frequent misclassification cases, together accounting for more than 85% of errors.
Additionally, we ranked mis-classification cases by ‘part of speech coverage’, that is by the ratio of the words belonging to a particular PoS for which our classifier outputs this particular type of misclassification. For example, proper nouns misclassified as common nouns constitute the most numerous error type in Table 1, but in fact only 9% of all proper nouns in the test set were misclassified in this way. There are parts of speech with a much larger portion of word-types predicted erroneously: e.g., 22% of subordinate conjunctions were classified as adverbs. Table 2 lists error types with the highest coverage (we excluded error types with absolute frequency equal to 1, as it is impossible to speculate on solitary cases).
We now describe some of the interesting cases. Almost 30% of error types (judging by absolute amount of mis-classified words) consist of proper nouns predicted to be common ones and vice versa. These cases do not tell us anything new, as it is obvious that distributionally these two classes of words are very similar, take the same syntactic contexts and hardly can be considered differ-
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
ent parts of speech at all. At the same time, it is interesting that the majority of proper nouns in the test set (88%) was correctly predicted as such. It means that in spite of contextual similarity, the distributional model has managed to extract features typical for proper names. Errors mostly cover comparatively rare names, such as ‘luftwaffe’, ‘stasi’, ‘stonehenge’, or ‘himalayas’. Our guess is that the model was just not presented with enough contexts for these words to learn meaningful representations. Also, they are mostly not personal names but toponyms or organization names. Most probably, the model has trained to distinguish proper names by contexts like ‘My name is’, etc, obviously not appropriate here.
Another 30% of errors are due to vague boundaries between nominal and adjectival distribution patterns in English: nouns can be modified by both (it seems that cases where a proper noun is mistaken for an adjective are often caused by the same factor). Words like ‘materialist_NOUN’, ‘starboard_NOUN’ or ‘hypertext_NOUN’ are tagged as nouns in the BNC, but they often modify other nouns, and their contexts are so ‘adjectival’ that the distributional model actually assigned them semantic features highly similar to those of adjectives. Vice versa, ‘white-collar_ADJ’ (an adjective in BNC) is surely a noun from the point of view of our model. Indeed, there can be contradicting views on the correct part of speech for this word in phrases like ‘and all the other whitecollar workers’. Thus, in this case the distributional model highlights the already known similarity between two word classes.
The cases with verbs mistaken for adjectives seem to be caused mostly by passive participles (‘was overgrown’, ‘is indented’, ‘’), which intuitively are indeed very adjective-like. So, this gives us a set of verbs dominantly (or almost exclusively, like ‘to intertwine’ or ‘to disillusion’) used in passive. Of course, we will hardly announce such verbs to be adjectives based on that evidence, but at least we can be sure that this subclass of verbs is clearly semantically and distributionally different from other verbs.
The next numerous type of errors consists of common nouns predicted to be numerals. A quick glance at the data reveals that 90% of these ‘nouns’ are in fact currency amounts and percentages (‘£70’, ‘33%’, ‘$1’, etc). It seems pretty log-
ical to classify these as numerals, despite of them containing some kind of nominative entities inside. Judging by the classifier’s decisions, their contexts do not differ much from those of simple numbers, and their semantics is similar. The Universal Dependencies Treebank is more consistent in this respect: it separates entities like ‘1$’ into two tokens: a numeral (NUM) and a symbol (SYM). Consequently, when our classifier was tested on words from the UD Treebank, there was only one occurrence of this type of error.
Related to this is the inverse case of numerals predicted to be common or proper nouns. It is interesting that this error type is also quite massive in its coverage: if we combine numerals predicted to be common and proper nouns, we will see that 17% of all numerals in the test set were subject to this error. The majority of these ‘numerals’ are years (‘1804’, ‘1776’, ‘1822’) and decades (‘1820s’, ‘60s’ and even ‘twelfths’). Intuitively, such entities do indeed functions as nouns (‘I’d like to return to the sixties’). Anyway, it is difficult to invent a persuasive reason for why ‘fifty pounds’ should be tagged as a noun, but ‘the year 1776’ as a numeral. So, this points to possible (minor) inconsistencies in the annotation strategy of the BNC. Note that a similar problem exists in the Penn Treebank as well (Manning, 2011).
Adverbs classified as nouns (53 words in total for both common and proper nouns) are possibly the ones often followed by verbs or appearing in company of adjectives (examples are ‘ultra’ and ‘kinda’). This made the model treat them as close to the nominative classes. Interestingly, most ‘adverbs’ predicted to be proper nouns are time indicators (‘7pm’, ‘11am’); this also raises questions about what adverbial features are present in these entities. Once again, the UD Treebank does not tag them as adverbs.
The cases we described above revealed some inconsistencies in the BNC annotation. However, it seems that with adverbs mistaken for adjectives, we actually found a systematic error in the BNC tagging: these cases are mostly connected to adjectives like ‘plain’, ‘clear’ or ‘sharp’ (including comparative and superlative forms) erroneously tagged in the corpus as adverbs. These cases are not rare: just the three adjectives we mentioned alone appear in the BNC about 600 times with an adverb tag, mostly in phrases of the kind ‘the author makes it plain that. . . ’. Sometimes these to-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Table 2. Coverage of mis-classifications (from all word types of this PoS) with distributional predictor
Coverage Actual PoS
Predicted PoS Absolute amount
0.22 SCONJ ADV 2 0.17 INTJ PROPN 8 0.11 ADP ADJ 3 0.09 ADJ NOUN 313 0.09 PROPN NOUN 347 0.09 NUM NOUN 52 0.08 NUM PROPN 45
kens are tagged as ambiguous, and the adjective tag is there as a second variant; however, the corpus documentation states that in such cases the first variant is always more likely. Thus, distributional models can actually detect outright errors in PoS-tagged corpora, when incorrectly tagged words strongly tend to cluster with another part of speech. In the UD treebank such examples can also be observed, but they are much fewer and more ‘adverbial’, like ‘it goes clear through’.
Some of the entries from Table 2 were already covered above, except the first three cases. They are related to closed word classes (functional words), that’s why the absolute number of influenced word types is low, but the coverage (ratio of all words of this PoS) is quite high.
First, of 9 distinct subordinate conjunctions in the test set, two were predicted to be adverbs. This is not surprising, as these words are ‘seeing’ and ‘immediately’. For ‘seeing’ the prediction seems to be just a random guess (the prediction confidence was as low as 0.3), but with ‘immediately’ the classifier was actually more correct than the BNC tagger (the prediction confidence was about 0.5). In BNC, these words are mostly tagged as subordinate conjunctions in cases when they are in the beginning of sentences (‘Immediately, she lowered the gun’). The other words marked as SCONJ in the test set are really such, and the classifier made correct predictions matching the BNC tags.
Interjections mistaken for proper names do not seem very interpretable (examples are ‘gee’, ‘oy’ and ‘farewell’). At the same time, 3 prepositions predicted to be adjectives clearly form a separate group: they are ‘cross’, ‘pre’ and ‘pro’. They are
not often used as separate words, but when they are (‘Did anyone encounter any trouble from Hibs fans in Edinburgh pre season?’), they are very close to adjectives or adverbs, so the predictions of the distributional classifier once again suggest shifting parts of speech boundaries a bit.
Error analysis on the vocabulary from the Universal Dependencies Treebank showed pretty much the same results, except for some differences mentioned above.
There exists another way to retrieve this kind of data: to process gold standard data with a mainstream PoS tagger and analyze the resulting confusion matrix. We tested this approach by processing the whole BNC with the Stanford PoS Tagger (Toutanova et al., 2003). Note that as an input to the tagger we used not the whole sentences from the corpora, but separate tokens, to mimic our workflow with the distributional predictor. Prior to this, BNC tags were converted to the Penn Treebank tagset4 to match the output of the tagger. As we are interested in coarse, ‘overarching’ word classes, inflectional forms were merged into one tag (for example plural and singular nouns NNS and NN were considered to belong to one noun class NN, etc). That was easy to accomplish by dropping all characters of the tags after the first two (excluding proper noun tags, which were all converted to NNP).
Analysis of the confusion matrix (cases where the tag predicted by the Stanford tagger was different from the BNC tag) revealed the most frequent error types shown in Table 3. Despite similar top positions of errors types ‘proper noun predicted as common noun’ and ‘nouns and adjectives mistaken for each other’, there are also very frequent errors of types ‘verb to noun’ and ‘adjective to verb’, not observed in the distributional confusion matrix (Table 1). We would not be able to draw the same insights that we drew from the distributional confusion matrix: the case with verbs mistaken for adjective is ranked only 12th, adverbs mistaken for nouns - 13th, etc.
Table 4 shows top mis-classification types by their word type coverage. Once again, interesting cases we discovered with the distributional confusion matrix (like subordinating conjunctions mistaken for adverbs and prepositions mistaken for adjectives) did not show up. Obviously, a lot of other insights can be extracted from the Stanford
4https://www.cis.upenn.edu/~treebank/
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Table 3. Most frequent PoS mis-classifications using the Stanford tagger
Amount (word types)
Actual PoS Predicted PoS
172675 NNP NN 47202 VB NN 40218 JJ NN 24075 NN JJ 9723 JJ VB
Table 4. Coverage of mis-classifications (from all word types of this PoS) with the Stanford tagger
Coverage Actual PoS
Predicted PoS Absolute amount
0.91 NNP NN 172675 0.8 UH NN 576 0.79 DT NN 217 0.78 EX JJ 11 0.78 PR NN 517
Tagger errors (and it was studied before), but it seems that employing a distributional predictor reveals different error cases and thus might be a useful tool.
To sum it up, analysis of ‘boundary cases’ detected by a classifier trained on distributional vectors, indeed reveals sub-classes of words lying on the verge between different parts of speech. It also allows for quickly discovering systematic errors or inconsistencies in PoS tags, whether they be automatic or manual. Thus, discussions about PoS boundaries definitely should take into consideration this data (expanded and revised).In the experiment described in the previous section, we used the model trained on words concatenated with their PoS tags. Thus, our ‘classifier’ was a bit artificial in that it demanded a word plus a tag as an input, and then its output is a judgment about what part of speech is most applicable to this combination from the point of view of the BNC distributional patterns. This was not a problem for us, as our aim was exactly to discover lexical outliers.
But is it possible to construct a proper predictor in the same way, which is able to predict a PoS tag for a word without any pre-existing tags as hints? Preliminary experiments seem to indicate that it is.
We trained a Continuous Skipgram distributional model on the BNC lemmas without PoS tags. After that, we constructed a vocabulary of all unambiguous lemmas from the UD Treebank training set. ‘Unambiguous’ here means that the lemma either was always tagged with one and the same PoS tag in the Treebank, or has one ‘dominant’ tag, with frequencies of other PoS assignments not exceeding 1/2 of the dominant assignment frequency. Our hypothesis was that these words are prototypical examples of their PoS classes, with corresponding prototypical features most pronounced. We also removed words with frequency less than 10 in the Treebank. This left us with 1564 words from all Universal Tag classes (excluding PUNCT, X and SYM, as we hardly want to predict punctuation or symbol tag).
Then the same simple logistic regression classifier was trained on the distributional vectors from the model for these 1564 words only, using UD Treebank tags as class labels (the training instances were again weighted proportionally to the words’ frequencies in the Treebank). The resulting classifier showed an accuracy of 0.938 after 10-fold cross-validation on the training set.
We then evaluated the classifier on tokens from the UD Treebank test set. Now the input to the classifier consisted of lemma only. Lemmas which were missing from the model’s vocabulary were omitted (860 of a total of 21759 tokens in the test set). It reached an accuracy of 0.84 (weighted precision 0.85, weighted recall 0.84).
These numbers may not seem very impressive in comparison with the performance of modern state-of-the-art PoS taggers. However, one should remember that this classifier knows absolutely nothing about a word’s context in the current sentence. It assigns PoS tags based solely on the proximity of the word’s distributional vector in an unsupervised model to those of prototypical PoS examples. The classifier was in fact based only on knowledge of what words occurred in the BNC near other words within a symmetric window of 2 word to the left and to the right. It did not even have access to the information about exact word order within this sliding window, which makes its performance even more impressive.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Figure 3. Classifier accuracy depending on the number of used vector components (k)
It is also interesting that one needs as few as a thousand example words to train a decent classifier. Thus, it seems that PoS affiliation is expressed quite strongly and robustly in word embeddings. It can be employed, for example, in preliminary tagging of large corpora of resource-poor languages. Only a handful of non-ambiguous words need be manually PoS-tagged, and the rest is done by a distributional model trained on the corpus.
To find out how many features are important for the classifier, we used the same training and test set, and ranked all embedding components (features, vector dimensions) by their ANOVA F-value related to PoS class. Then we successively trained the classifier on increasing amounts of top-ranked features (top k best) and measured the training set accuracy.
The results are shown in Figure 3. One can see that the accuracy smoothly grows with the number of used features, eventually reaching almost ideal performance on the training set. It is difficult to define the point where the influence of adding features reaches a plateau; it may lie somewhere near k = 100. It means that the knowledge about PoS affiliation is distributed among at least one hundred components of the word embeddings, quite consistent with the underlying idea of embedding models.
One might argue that the largest gap in performance is between k = 2 and k = 3 (from 0.38 to 0.51) and thus most PoS-related information is contained in the 3 components with the largest Fvalue (in our case, these 3 features were components 31, 51 and 11). But an accuracy of 0.51 is certainly not an adequate result, so even if im-
portant, these components are not sufficient to robustly predict part of speech affiliation for a word. Further research is needed to study the effects of adding features to the classifier training.
Regardless, an interesting finding is that part of speech affiliation is distributed among many components of the word embeddings, not concentrated in one or two specific features. Thus, the strongly expressed component 94 in the average vector of conjunctions (Figures 1 and 2) seems to be a solitary case.",,"Parts of speech (PoS) are useful abstractions, but still abstractions. Boundaries between them in natural languages are flexible. Sometimes, large open classes of words are situated on the verge between several parts of speech: for example, participles in English are in many respects both verbs and adjectives. In other cases, closed word classes ‘intersect’ between themselves: it is often difficult to tell a determiner from a possessive pronoun, etc. As (Houston, 1985) puts it, ‘Grammatical categories exist along a continuum which does not exhibit sharp boundaries between the categories’.
When annotating natural texts for parts of speech, the choice of a PoS tag in many ways depends on the human annotators themselves, but also on the quality of linguistic conventions behind the division into different word classes. That is
why there were always attempts to refine the definitions of parts of speech and to make them more ‘real’ and data-driven, produced from corpora of real texts: see, among others, the seminal work of (Biber et al., 1999). The aim of such attempts is to identify clusters of words occurring naturally and corresponding to what we usually call ‘parts of speech’. One of the main distance metrics that can be used in detecting such clusters is a distance between distributional features of words (their contexts in a reference training corpus).
In this paper, we test this approach using predictive models developed in the field of distributional semantics. Recent achievements in training distributional models of language using machine learning allows for robust representations of natural language semantics created in a completely unsupervised way, using only large corpora of raw text. Relations between dense word vectors (embeddings) in the resulting vector space are of course mostly semantic. But can they be used to discover something new about grammar and syntax, particularly parts of speech? Do learned semantic vectors help here? Below we show that such models do contain a lot of interesting data related to PoS classes.
The rest of the paper is organized as follows. In Section 2 we briefly cover the previous work on the subject of parts of speech and distributional models. Section 3 describes data processing and the training of a PoS predictor based on word embeddings. In Section 4 errors of this predictor are analyzed and insights gained from them described. Section 5 introduces an attempt to build a fullfledged PoS tagger within the same approach. It also analyzes the correspondence between particular vector components and PoS affiliation, before we conclude in Section 6.
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199","Traditionally three types of criteria are used to distinguish different parts of speech: formal (or morphological), syntactic (or distributional) and semantic (Aarts and McMahon, 2008). Arguably, syntactic and semantic criteria are not much different from each other, if one follows the famous distributional hypothesis stating that meaning is determined by context (Firth, 1957). Below we show that unsupervised distributional semantic models obviously contain data related to parts of speech.
For several years already it has been known that some information about morphological word classes is indeed stored in distributional models; (Tsuboi, 2014) even employed it to improve PoStagging. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing to separate them from words belonging to other parts of speech.
(Mikolov et al., 2013b) showed that there are indeed also regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, (Liu et al., 2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and (Tsvetkov et al., 2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing the evaluation dataset based on this.
It seems that one can infer data about PoS classes of words from embedding models. But then, it can be useful for deeper analysis of part of speech boundaries, leading to discovery of separate words or whole classes that tend to behave ‘strangely’. Discovering such cases is one possible way to improve performance of existing automatic PoS taggers (Manning, 2011). These ‘outliers’ may signal the necessity to revise the annotation strategy or classification system in general. Section 3 describes the process of constructing typical PoS clusters and detecting words which seem to belong to a cluster different from their traditional annotation.",,"We showed that semantic features derived in the process of training distributional vector models, can be employed both in supporting linguistic hypotheses about part of speech class changes and in detecting and fixing possible annotation errors in corpora. Word embeddings contain rather robust information about the PoS class of the corresponding words. Moreover, this knowledge seems to be distributed among several components (at least a hundred in our case of 300-dimensional model).
Distributional models trained in a nondeterministic and stochastic way on large amounts of word contexts learn knowledge about part of speech clusters. Arguably, they are good at this precisely because part of speech boundaries are not strict, and even sometimes considered to be a non-categorical linguistic phenomenon (Manning, 2015).
The reported experiment form part of ongoing research, and we plan to extend it, particularly conducting similar experiments with other languages typologically different from English. We also plan to continue studying the issue of correspondence between particular embedding components and part of speech affiliation. Another direction of future work is finding out how different hyperparameters for training distributional models (including training corpus pre-processing) influence their performance in PoS discrimination."
29,"This paper studies how word embeddings trained on the British National Corpus interact with part of speech boundaries. Our work targets the Universal PoS tag set, which is currently actively being used for annotation of a range of languages. We experiment with training classifiers for predicting PoS tags for words based on their embeddings. The results show that the information about PoS affiliation contained in the distributional vectors allows us to discover groups of words with distributional patterns that differ from other words of the same part of speech.   This data often reveals hidden inconsistencies of the annotation process or guidelines. At the same time, it supports the notion of `soft' or `graded' part of speech affiliations. Finally, we show that information about PoS is distributed among dozens of vector components, not limited to only one or two features.",Redefining part-of-speech classes with distributional semantic models,163,"## General comments:
This paper presents an exploration of the connection between part-of-speech
tags and word embeddings. Specifically the authors use word embeddings to draw
some interesting (if not somewhat straightforward) conclusions about the
consistency of PoS tags and the clear connection of word vector representations
to PoS. The detailed error analysis (outliers of classification) is definitely
a strong point of this paper.

However, the paper seems to have missing one critical main point: the reason
that corpora such as the BNC were PoS tagged in the first place. Unlike a
purely linguistic exploration of morphosyntactic categories (which are
underlined by a semantic prototype theory - e.g. see Croft, 1991), these
corpora were created and tagged to facilitate further NLP tasks, mostly
parsing. The whole discussion could then be reframed as whether the
distinctions made by the distributional vectors are more beneficial to parsing
as compared to the original tags (or UPOS for that matter). 

Also, this paper is missing a lot of related work in the context of
distributional PoS induction. I recommend starting with the review
Christodoulopoulos et al. 2010 and adding some more recent non-DNN work
including Blunsom and Cohn (2011), Yatbaz et al. (2012), etc. In light of this
body of work, the results of section 5 are barely novel (there are systems with
more restrictions in terms of their external knowledge that achieve comparable
results).

## Specific issues
In the abstract one of the contributed results is that ""distributional vectors
do contain information about PoS affiliation"". Unless I'm misunderstanding the
sentence, this is hardly a new result, especially for English: every
distributionally-based PoS induction system in the past 15 years that presents
""many-to-one"" or ""cluster purity"" numbers shows the same result.

The assertion in lines 79-80 (""relations between... vectors... are mostly
semantic"") is not correct: the <MIKOLOV or COLOBERT> paper (and subsequent
work) shows that there is a lot of syntactic information in these vectors. Also
see previous comment about cluster purity scores. In fact you revert that
statement in the beginning of section 2 (lines 107-108).

Why move to UPOS? Surely the fine-grained distinctions of the original tagset
are more interesting.

I do not understand footnote 3. Were these failed attempts performed by you or
other works? Under what criteria did they fail? What about Brown cluster
vectors? They almost perfectly align with UPOS tags.

Is the observation that ""proper nouns are not much similar to common nouns""
(lines 331-332) that interesting? Doesn't the existence of ""the"" (the most
frequent function word) almost singlehandedly explain this difference?

While I understand the practical reasons for analysing the most frequent
word/tag pairs, it would be interesting to see what happens in the tail, both
in terms of the vectors and also for the types of errors the classifier makes.
You could then try to imagine alternatives to pure distributional (and
morphological - since you're lemmatizing) features that would allow better
generalizations of the PoS tags to these low-frequency words.

## Minor issues
Change the sentential references to \newcite{}: e.g. ""Mikolov et al. (2013b)
showed""",,2,4,Poster,4,1,4,5,4,5,2,2,2016,"Our hypothesis is that for the majority of words their part of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word embeddings do contain PoS-related data, the properly trained classifier will correctly predict PoS tags for the majority of words: it means that these lexical entities conform to a dominant distributional pattern of their part of speech class. At the same time, the words for which the classifier outputs incorrect predictions, are expected to be ‘outliers’, with different distributional patterns, different from other words in the same class. These cases are the points of linguistic interest, and in the rest of the paper we mostly concentrate on them.
To test the initial hypothesis, we used the XML Edition of British National Corpus (BNC), a balanced and representative corpus of English language of about 98 million word tokens in size. As stated in the corpus documentation, ‘it was [PoS]tagged automatically, using the CLAWS4 automatic tagger developed by Roger Garside at Lancaster, and a second program, known as Template Tagger, developed by Mike Pacey and Steve Fligelstone’ (Burnard, 2007). The corpus authors report a precision of 0.96 and recall of 0.99 for their tools, based on a manually checked sample. For this research, it is important that BNC is an established and well-studied corpus of English with PoS-tags and lemmas assigned to all words.
We produced a version of BNC where all the words were replaced with their lemmas and PoStags converted into the Universal Part-of-Speech Tagset (Petrov et al., 2012)1. Thus, each token was represented as a concatination of its lemma and PoS tag (for example, ‘love_VERB’ and ‘love_NOUN’ yield different word types). The mappings between BNC tags and Universal tags were created manually by us and released online2. We worked with the following 16 Universal tags: ADJ, ADP, ADV, AUX, CONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, SCONJ, SYM, VERB, X (tokens marked with PUNCT tag were excluded).
1We used the latest version of the tagset available at http://universaldependencies.org
2Anonymized
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
Then, a Continuous Skipgram embedding model (Mikolov et al., 2013a) was trained on this corpus, using a vector size of 300, 10 negative samples, a symmetric window of 2 words, no down-sampling, and 5 iterations over the training data. Words with corpus frequency less than 5 were ignored. This model was then taken to represent the semantics of the words it contained. But at the same time, for each word, a gold standard PoS tag is known (from the BNC annotation). It means that is is possible to test how good the word embeddings are in grouping words according to their parts of speech.
To this end, we extracted vectors for the 10 000 most frequent words from the resulting model (roughly, these are the words with corpus frequency more than 500). Then, these vectors were used to train a simple logistic regression multinomial classifier aimed to predict the word’s part of speech3.
Note that during training (and subsequent testing), each word’s vector was used several times, proportional to frequency of the word in the corpus, so the classifier was trained on 177 343 (sometimes repeating) instances, instead of the original 10 000. This was done to alleviate the classifier’s part of speech bias. There are much fewer word types in the closed PoS classes (pronouns, conjunctions, etc.) than in the open ones (nouns, verbs, etc.), so without considering word frequency, the model does not have a chance to learn good predictors for ‘rare’ classes and ends up never predicting them. At the same time, words from closed classes occur very frequently in the running text, so after ‘weighting’ training instances by corpus frequency, the balance is restored and the classifier model has enough training instances to learn to predict closed PoS classes as well. As an additional benefit, by this modification we make frequent words from all classes to be more ‘influential’ in training the classifier.
The resulting classifier showed a weighted average F-score equal to 0.979 with 10-fold crossvalidation on the training set. This is a significant improvement over the majority class baseline classifier (classify everything as nouns), which showed an F-score of 0.07 and over the onefeature baseline classifier (classify using only one
3It is important that we applied classification, not clustering here. Attempts to naively cluster word vectors into the number of clusters equal to the number of PoS tags inescapably failed.
vector dimension with maximum F-value in relation to class tags), with F-score equal to only 0.22. Thus, the results support the hypothesis that word embeddings contain information allowing to group words together based on their parts of speech. At the same time, we see that this information is not restricted to some particular vector component: rather, it is distributed among several axis of the vector space.
After training the classifier, we were able to use it to detect ‘outlying’ words in the BNC (judging by the distributional model). So as not to experiment on the same data we had trained our classifier on, we compiled another test set of 17 000 vectors for words with BNC frequencies between 100 and 500. They were weighted by word frequencies in the same way as the training set, and the resulting test set contained 30 710 instances. Then, we predicted parts of speech for these words using our classifier and evaluated its performance on them. The results on totally unseen data were not much worse, with an F-score equal to 0.91.
Furthermore, to make sure that the results can potentially be extended to other texts, we applied the trained classifier to all lemmas from the human-annotated Universal Dependencies English Treebank (Silveira et al., 2014). The words not present in the distributional model were omitted (they sum to 27% of word types and 10% of word tokens). The classifier showed an F-Score equal to 0.99, further demonstrating the robustness of the classifier.
In sum, the vast majority of words are classified correctly, which means that their embeddings enables detecting their parts of speech. In fact, one can even visualize ‘prototypical’ vectors for each PoS by simply averaging vectors of words belonging to this part of speech. We did this for 10 000 words from our training set.
Plots for prototypical vectors of coordinating and subordinating conjunctions are shown in the Figures 1 and 2 respectively. Even visually one can notice a very strongly expressed feature near the ‘100’ mark in the horizontal axis. In fact, this is vector component number 94, and it is indeed an idiosyncratic feature of conjunctions: none of other parts of speech shows such a property. More details about what vector components are relevant to part of speech affiliation are given in Section 5.
With prototypical PoS vectors we can even find out how similar different parts of speech are to
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
Figure 1. Prototypical embedding for coordinating conjunctions
Figure 2. Prototypical embedding for subordinating conjunctions
each other, by simply measuring cosine similarity between them. If we rank PoS pairs according to their similarity, what we see is that nominative parts of speech are close to each other, determiners and pronouns are also similar, as well as prepositions and subordinating conjunctions, everything quite in accordance with language intuition. It is interesting that proper nouns are not much similar to common nouns, with cosine similarity between them only 0.67 (even adverbs are closer). As we show below, this helps the model to successfully separate the former from the latter.
Despite generally good performance of the classifier, if we look at our BNC test set, 1741 word types (about 10% of the whole test set vocabulary) were still classified incorrectly. Thus, they are somehow dissimilar to ‘prototypical’ words of their parts of speech. These are the ‘outliers’ we were after. We investigate the patterns found among them in the next section.We filtered out mis-classified word types with ‘X’ gold annotation (they are mostly foreign or nonsense words). This left us with 1558 words; the classifier assigned them part of speech tags
different from the ones in the BNC. It probably means that these words’ distributional patterns differ somehow from the ‘mainstream’, and they tend to exhibit behavior similar to another part of speech. Table 1 shows the most frequent misclassification cases, together accounting for more than 85% of errors.
Additionally, we ranked mis-classification cases by ‘part of speech coverage’, that is by the ratio of the words belonging to a particular PoS for which our classifier outputs this particular type of misclassification. For example, proper nouns misclassified as common nouns constitute the most numerous error type in Table 1, but in fact only 9% of all proper nouns in the test set were misclassified in this way. There are parts of speech with a much larger portion of word-types predicted erroneously: e.g., 22% of subordinate conjunctions were classified as adverbs. Table 2 lists error types with the highest coverage (we excluded error types with absolute frequency equal to 1, as it is impossible to speculate on solitary cases).
We now describe some of the interesting cases. Almost 30% of error types (judging by absolute amount of mis-classified words) consist of proper nouns predicted to be common ones and vice versa. These cases do not tell us anything new, as it is obvious that distributionally these two classes of words are very similar, take the same syntactic contexts and hardly can be considered differ-
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
ent parts of speech at all. At the same time, it is interesting that the majority of proper nouns in the test set (88%) was correctly predicted as such. It means that in spite of contextual similarity, the distributional model has managed to extract features typical for proper names. Errors mostly cover comparatively rare names, such as ‘luftwaffe’, ‘stasi’, ‘stonehenge’, or ‘himalayas’. Our guess is that the model was just not presented with enough contexts for these words to learn meaningful representations. Also, they are mostly not personal names but toponyms or organization names. Most probably, the model has trained to distinguish proper names by contexts like ‘My name is’, etc, obviously not appropriate here.
Another 30% of errors are due to vague boundaries between nominal and adjectival distribution patterns in English: nouns can be modified by both (it seems that cases where a proper noun is mistaken for an adjective are often caused by the same factor). Words like ‘materialist_NOUN’, ‘starboard_NOUN’ or ‘hypertext_NOUN’ are tagged as nouns in the BNC, but they often modify other nouns, and their contexts are so ‘adjectival’ that the distributional model actually assigned them semantic features highly similar to those of adjectives. Vice versa, ‘white-collar_ADJ’ (an adjective in BNC) is surely a noun from the point of view of our model. Indeed, there can be contradicting views on the correct part of speech for this word in phrases like ‘and all the other whitecollar workers’. Thus, in this case the distributional model highlights the already known similarity between two word classes.
The cases with verbs mistaken for adjectives seem to be caused mostly by passive participles (‘was overgrown’, ‘is indented’, ‘’), which intuitively are indeed very adjective-like. So, this gives us a set of verbs dominantly (or almost exclusively, like ‘to intertwine’ or ‘to disillusion’) used in passive. Of course, we will hardly announce such verbs to be adjectives based on that evidence, but at least we can be sure that this subclass of verbs is clearly semantically and distributionally different from other verbs.
The next numerous type of errors consists of common nouns predicted to be numerals. A quick glance at the data reveals that 90% of these ‘nouns’ are in fact currency amounts and percentages (‘£70’, ‘33%’, ‘$1’, etc). It seems pretty log-
ical to classify these as numerals, despite of them containing some kind of nominative entities inside. Judging by the classifier’s decisions, their contexts do not differ much from those of simple numbers, and their semantics is similar. The Universal Dependencies Treebank is more consistent in this respect: it separates entities like ‘1$’ into two tokens: a numeral (NUM) and a symbol (SYM). Consequently, when our classifier was tested on words from the UD Treebank, there was only one occurrence of this type of error.
Related to this is the inverse case of numerals predicted to be common or proper nouns. It is interesting that this error type is also quite massive in its coverage: if we combine numerals predicted to be common and proper nouns, we will see that 17% of all numerals in the test set were subject to this error. The majority of these ‘numerals’ are years (‘1804’, ‘1776’, ‘1822’) and decades (‘1820s’, ‘60s’ and even ‘twelfths’). Intuitively, such entities do indeed functions as nouns (‘I’d like to return to the sixties’). Anyway, it is difficult to invent a persuasive reason for why ‘fifty pounds’ should be tagged as a noun, but ‘the year 1776’ as a numeral. So, this points to possible (minor) inconsistencies in the annotation strategy of the BNC. Note that a similar problem exists in the Penn Treebank as well (Manning, 2011).
Adverbs classified as nouns (53 words in total for both common and proper nouns) are possibly the ones often followed by verbs or appearing in company of adjectives (examples are ‘ultra’ and ‘kinda’). This made the model treat them as close to the nominative classes. Interestingly, most ‘adverbs’ predicted to be proper nouns are time indicators (‘7pm’, ‘11am’); this also raises questions about what adverbial features are present in these entities. Once again, the UD Treebank does not tag them as adverbs.
The cases we described above revealed some inconsistencies in the BNC annotation. However, it seems that with adverbs mistaken for adjectives, we actually found a systematic error in the BNC tagging: these cases are mostly connected to adjectives like ‘plain’, ‘clear’ or ‘sharp’ (including comparative and superlative forms) erroneously tagged in the corpus as adverbs. These cases are not rare: just the three adjectives we mentioned alone appear in the BNC about 600 times with an adverb tag, mostly in phrases of the kind ‘the author makes it plain that. . . ’. Sometimes these to-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
Table 2. Coverage of mis-classifications (from all word types of this PoS) with distributional predictor
Coverage Actual PoS
Predicted PoS Absolute amount
0.22 SCONJ ADV 2 0.17 INTJ PROPN 8 0.11 ADP ADJ 3 0.09 ADJ NOUN 313 0.09 PROPN NOUN 347 0.09 NUM NOUN 52 0.08 NUM PROPN 45
kens are tagged as ambiguous, and the adjective tag is there as a second variant; however, the corpus documentation states that in such cases the first variant is always more likely. Thus, distributional models can actually detect outright errors in PoS-tagged corpora, when incorrectly tagged words strongly tend to cluster with another part of speech. In the UD treebank such examples can also be observed, but they are much fewer and more ‘adverbial’, like ‘it goes clear through’.
Some of the entries from Table 2 were already covered above, except the first three cases. They are related to closed word classes (functional words), that’s why the absolute number of influenced word types is low, but the coverage (ratio of all words of this PoS) is quite high.
First, of 9 distinct subordinate conjunctions in the test set, two were predicted to be adverbs. This is not surprising, as these words are ‘seeing’ and ‘immediately’. For ‘seeing’ the prediction seems to be just a random guess (the prediction confidence was as low as 0.3), but with ‘immediately’ the classifier was actually more correct than the BNC tagger (the prediction confidence was about 0.5). In BNC, these words are mostly tagged as subordinate conjunctions in cases when they are in the beginning of sentences (‘Immediately, she lowered the gun’). The other words marked as SCONJ in the test set are really such, and the classifier made correct predictions matching the BNC tags.
Interjections mistaken for proper names do not seem very interpretable (examples are ‘gee’, ‘oy’ and ‘farewell’). At the same time, 3 prepositions predicted to be adjectives clearly form a separate group: they are ‘cross’, ‘pre’ and ‘pro’. They are
not often used as separate words, but when they are (‘Did anyone encounter any trouble from Hibs fans in Edinburgh pre season?’), they are very close to adjectives or adverbs, so the predictions of the distributional classifier once again suggest shifting parts of speech boundaries a bit.
Error analysis on the vocabulary from the Universal Dependencies Treebank showed pretty much the same results, except for some differences mentioned above.
There exists another way to retrieve this kind of data: to process gold standard data with a mainstream PoS tagger and analyze the resulting confusion matrix. We tested this approach by processing the whole BNC with the Stanford PoS Tagger (Toutanova et al., 2003). Note that as an input to the tagger we used not the whole sentences from the corpora, but separate tokens, to mimic our workflow with the distributional predictor. Prior to this, BNC tags were converted to the Penn Treebank tagset4 to match the output of the tagger. As we are interested in coarse, ‘overarching’ word classes, inflectional forms were merged into one tag (for example plural and singular nouns NNS and NN were considered to belong to one noun class NN, etc). That was easy to accomplish by dropping all characters of the tags after the first two (excluding proper noun tags, which were all converted to NNP).
Analysis of the confusion matrix (cases where the tag predicted by the Stanford tagger was different from the BNC tag) revealed the most frequent error types shown in Table 3. Despite similar top positions of errors types ‘proper noun predicted as common noun’ and ‘nouns and adjectives mistaken for each other’, there are also very frequent errors of types ‘verb to noun’ and ‘adjective to verb’, not observed in the distributional confusion matrix (Table 1). We would not be able to draw the same insights that we drew from the distributional confusion matrix: the case with verbs mistaken for adjective is ranked only 12th, adverbs mistaken for nouns - 13th, etc.
Table 4 shows top mis-classification types by their word type coverage. Once again, interesting cases we discovered with the distributional confusion matrix (like subordinating conjunctions mistaken for adverbs and prepositions mistaken for adjectives) did not show up. Obviously, a lot of other insights can be extracted from the Stanford
4https://www.cis.upenn.edu/~treebank/
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Table 3. Most frequent PoS mis-classifications using the Stanford tagger
Amount (word types)
Actual PoS Predicted PoS
172675 NNP NN 47202 VB NN 40218 JJ NN 24075 NN JJ 9723 JJ VB
Table 4. Coverage of mis-classifications (from all word types of this PoS) with the Stanford tagger
Coverage Actual PoS
Predicted PoS Absolute amount
0.91 NNP NN 172675 0.8 UH NN 576 0.79 DT NN 217 0.78 EX JJ 11 0.78 PR NN 517
Tagger errors (and it was studied before), but it seems that employing a distributional predictor reveals different error cases and thus might be a useful tool.
To sum it up, analysis of ‘boundary cases’ detected by a classifier trained on distributional vectors, indeed reveals sub-classes of words lying on the verge between different parts of speech. It also allows for quickly discovering systematic errors or inconsistencies in PoS tags, whether they be automatic or manual. Thus, discussions about PoS boundaries definitely should take into consideration this data (expanded and revised).In the experiment described in the previous section, we used the model trained on words concatenated with their PoS tags. Thus, our ‘classifier’ was a bit artificial in that it demanded a word plus a tag as an input, and then its output is a judgment about what part of speech is most applicable to this combination from the point of view of the BNC distributional patterns. This was not a problem for us, as our aim was exactly to discover lexical outliers.
But is it possible to construct a proper predictor in the same way, which is able to predict a PoS tag for a word without any pre-existing tags as hints? Preliminary experiments seem to indicate that it is.
We trained a Continuous Skipgram distributional model on the BNC lemmas without PoS tags. After that, we constructed a vocabulary of all unambiguous lemmas from the UD Treebank training set. ‘Unambiguous’ here means that the lemma either was always tagged with one and the same PoS tag in the Treebank, or has one ‘dominant’ tag, with frequencies of other PoS assignments not exceeding 1/2 of the dominant assignment frequency. Our hypothesis was that these words are prototypical examples of their PoS classes, with corresponding prototypical features most pronounced. We also removed words with frequency less than 10 in the Treebank. This left us with 1564 words from all Universal Tag classes (excluding PUNCT, X and SYM, as we hardly want to predict punctuation or symbol tag).
Then the same simple logistic regression classifier was trained on the distributional vectors from the model for these 1564 words only, using UD Treebank tags as class labels (the training instances were again weighted proportionally to the words’ frequencies in the Treebank). The resulting classifier showed an accuracy of 0.938 after 10-fold cross-validation on the training set.
We then evaluated the classifier on tokens from the UD Treebank test set. Now the input to the classifier consisted of lemma only. Lemmas which were missing from the model’s vocabulary were omitted (860 of a total of 21759 tokens in the test set). It reached an accuracy of 0.84 (weighted precision 0.85, weighted recall 0.84).
These numbers may not seem very impressive in comparison with the performance of modern state-of-the-art PoS taggers. However, one should remember that this classifier knows absolutely nothing about a word’s context in the current sentence. It assigns PoS tags based solely on the proximity of the word’s distributional vector in an unsupervised model to those of prototypical PoS examples. The classifier was in fact based only on knowledge of what words occurred in the BNC near other words within a symmetric window of 2 word to the left and to the right. It did not even have access to the information about exact word order within this sliding window, which makes its performance even more impressive.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Figure 3. Classifier accuracy depending on the number of used vector components (k)
It is also interesting that one needs as few as a thousand example words to train a decent classifier. Thus, it seems that PoS affiliation is expressed quite strongly and robustly in word embeddings. It can be employed, for example, in preliminary tagging of large corpora of resource-poor languages. Only a handful of non-ambiguous words need be manually PoS-tagged, and the rest is done by a distributional model trained on the corpus.
To find out how many features are important for the classifier, we used the same training and test set, and ranked all embedding components (features, vector dimensions) by their ANOVA F-value related to PoS class. Then we successively trained the classifier on increasing amounts of top-ranked features (top k best) and measured the training set accuracy.
The results are shown in Figure 3. One can see that the accuracy smoothly grows with the number of used features, eventually reaching almost ideal performance on the training set. It is difficult to define the point where the influence of adding features reaches a plateau; it may lie somewhere near k = 100. It means that the knowledge about PoS affiliation is distributed among at least one hundred components of the word embeddings, quite consistent with the underlying idea of embedding models.
One might argue that the largest gap in performance is between k = 2 and k = 3 (from 0.38 to 0.51) and thus most PoS-related information is contained in the 3 components with the largest Fvalue (in our case, these 3 features were components 31, 51 and 11). But an accuracy of 0.51 is certainly not an adequate result, so even if im-
portant, these components are not sufficient to robustly predict part of speech affiliation for a word. Further research is needed to study the effects of adding features to the classifier training.
Regardless, an interesting finding is that part of speech affiliation is distributed among many components of the word embeddings, not concentrated in one or two specific features. Thus, the strongly expressed component 94 in the average vector of conjunctions (Figures 1 and 2) seems to be a solitary case.",,"Parts of speech (PoS) are useful abstractions, but still abstractions. Boundaries between them in natural languages are flexible. Sometimes, large open classes of words are situated on the verge between several parts of speech: for example, participles in English are in many respects both verbs and adjectives. In other cases, closed word classes ‘intersect’ between themselves: it is often difficult to tell a determiner from a possessive pronoun, etc. As (Houston, 1985) puts it, ‘Grammatical categories exist along a continuum which does not exhibit sharp boundaries between the categories’.
When annotating natural texts for parts of speech, the choice of a PoS tag in many ways depends on the human annotators themselves, but also on the quality of linguistic conventions behind the division into different word classes. That is
why there were always attempts to refine the definitions of parts of speech and to make them more ‘real’ and data-driven, produced from corpora of real texts: see, among others, the seminal work of (Biber et al., 1999). The aim of such attempts is to identify clusters of words occurring naturally and corresponding to what we usually call ‘parts of speech’. One of the main distance metrics that can be used in detecting such clusters is a distance between distributional features of words (their contexts in a reference training corpus).
In this paper, we test this approach using predictive models developed in the field of distributional semantics. Recent achievements in training distributional models of language using machine learning allows for robust representations of natural language semantics created in a completely unsupervised way, using only large corpora of raw text. Relations between dense word vectors (embeddings) in the resulting vector space are of course mostly semantic. But can they be used to discover something new about grammar and syntax, particularly parts of speech? Do learned semantic vectors help here? Below we show that such models do contain a lot of interesting data related to PoS classes.
The rest of the paper is organized as follows. In Section 2 we briefly cover the previous work on the subject of parts of speech and distributional models. Section 3 describes data processing and the training of a PoS predictor based on word embeddings. In Section 4 errors of this predictor are analyzed and insights gained from them described. Section 5 introduces an attempt to build a fullfledged PoS tagger within the same approach. It also analyzes the correspondence between particular vector components and PoS affiliation, before we conclude in Section 6.
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199","Traditionally three types of criteria are used to distinguish different parts of speech: formal (or morphological), syntactic (or distributional) and semantic (Aarts and McMahon, 2008). Arguably, syntactic and semantic criteria are not much different from each other, if one follows the famous distributional hypothesis stating that meaning is determined by context (Firth, 1957). Below we show that unsupervised distributional semantic models obviously contain data related to parts of speech.
For several years already it has been known that some information about morphological word classes is indeed stored in distributional models; (Tsuboi, 2014) even employed it to improve PoStagging. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing to separate them from words belonging to other parts of speech.
(Mikolov et al., 2013b) showed that there are indeed also regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, (Liu et al., 2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and (Tsvetkov et al., 2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing the evaluation dataset based on this.
It seems that one can infer data about PoS classes of words from embedding models. But then, it can be useful for deeper analysis of part of speech boundaries, leading to discovery of separate words or whole classes that tend to behave ‘strangely’. Discovering such cases is one possible way to improve performance of existing automatic PoS taggers (Manning, 2011). These ‘outliers’ may signal the necessity to revise the annotation strategy or classification system in general. Section 3 describes the process of constructing typical PoS clusters and detecting words which seem to belong to a cluster different from their traditional annotation.",,"We showed that semantic features derived in the process of training distributional vector models, can be employed both in supporting linguistic hypotheses about part of speech class changes and in detecting and fixing possible annotation errors in corpora. Word embeddings contain rather robust information about the PoS class of the corresponding words. Moreover, this knowledge seems to be distributed among several components (at least a hundred in our case of 300-dimensional model).
Distributional models trained in a nondeterministic and stochastic way on large amounts of word contexts learn knowledge about part of speech clusters. Arguably, they are good at this precisely because part of speech boundaries are not strict, and even sometimes considered to be a non-categorical linguistic phenomenon (Manning, 2015).
The reported experiment form part of ongoing research, and we plan to extend it, particularly conducting similar experiments with other languages typologically different from English. We also plan to continue studying the issue of correspondence between particular embedding components and part of speech affiliation. Another direction of future work is finding out how different hyperparameters for training distributional models (including training corpus pre-processing) influence their performance in PoS discrimination."
30,"How can we automatically discover the most important correspondences between words from two or more languages? How can we do so allowing for correspondences between any subset of languages, without drowning in redundant results, and at the same time maintaining control over the level of detail? These are exactly the questions we answer in this paper. We approach the problem with the Minimum Description Length principle, and give an efficient algorithm for discovering statistically important correspondences. We test the efficacy of our method against a set of Slavic languages. The experiments show our method automatically discovers non-trivial associations, allowing for both quantitative and qualitative analysis of multiple languages.",Discovering Correspondences between Multiple Languages by MDL,165,"This paper proposes a method for discovering correspondences between languages
based on MDL. The author model correspondences between words sharing the same
meaning in a number of Slavic languages. They develop codes for rules that
match substrings in two or more languages and formulate an MDL objective that
balances the description of the model and the data given the model. 
The model is trained with EM and tested on a set of 13 Slavic languages. The
results are shown by several distance measures, a phylogenetic tree, and
example of found correspondences. 

The motivation and formulation of the approach makes sense. MDL seems like a
reasonable tool to attack the problem and the motivation for employing EM is
presented nicely. I must admit, though, that some of the derivations were not
entirely clear to me.
The authors point out the resemblance of the MDL objective to Bayesian
inference, and one thinks of the application of Bayesian inference in
(biological) phylogenetic inference, e.g. using the MrBayes tool. An empirical
comparison here could be insightful.  

Related work: 
- Lacking comparison to methods for borrowing and cognate detection or other
computational methods for historical linguistics. For example, the studies by
Alexandre Bouchard-Cote, Tandy Warnow, Luay Nakhleh and Andrew Kitchen. Some
may not have available tools to apply in the given dataset, but one can mention
List and Moran (2013). There are also relevant tools for biological phylogeny
inference that can be applied (paup, MrBayes, etc.). 

Approach and methodology
- Alignment procedure: the memory/runtime bottleneck appears to be a major
drawback, allowing the comparison of only 5 languages at most. As long as
multiple languages are involved, and phylogenetic trees, it would be
interesting to see more languages. I'm curious what ideas the authors have for
dealing with this issue. 
- Phylogenetic tree: using neighbor joining for creating phylogenetic trees is
known to have disadvantages (like having to specify the root manually). How
about more sophisticated methods?  
- Do you run EM until convergence or have some other stopping criterion? 

Data
- Two datasets are mixed, one of cognates and one not necessarily (the Swadesh
lists). Have you considered how this might impact the results? 
- The data is in orthographic form, which might hide many correspondences. This
is especially apparent in languages with different scripts. Therefore the
learned rules might indicate change of script more than real linguistic
correspondences. This seems like a shortcoming that could be avoided by working
on the level of phonetic transcriptions.

Unclear points
- What is the ""optimal unigram for symbol usages in all rules""? (line 286)
- The merging done in the maximization step was not entirely clear to me. 

Minor issue
- ""focus in on"" -> ""focus on"" (line 440)

Refs
Johann-Mattis List, Steven Moran. 2013. An Open Source Toolkit for Quantitative
Historical Linguistics. Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics: System Demonstrations, pages
13â18, Sofia, Bulgaria. Association for Computational Linguistics.
http://www.aclweb.org/anthology/P13-4003.  
Andrew Kitchen, Christopher Ehret, Shiferaw Assefa and Connie J. Mulligan.
2009. Bayesian phylogenetic analysis of Semitic languages identifies an Early
Bronze Age origin of Semitic in the Near East",,3,3,Oral Presentation,3,2,4,3,4,5,4,4,2016,"We seekmodels which consist of sets of correspondence rules. We propose that correspondences should be treated simply as associated strings of characters with no assumed underlying distribution. Doing this results in objective, unbiased string-level measures of linguistic similarity and allows to observe the actual distributions of correspondence rules. Furthermore, we want to learn our rules deterministically, to exclude chance's influence on our assessments.
We build our approach on these key principles and use a number of observations to realize them.
1) In order to evaluate how good a given set of correspondence rules is, we should evaluate how well these rules describe the data. However, it is not immediately obvious how to do this. For example, if we are given the Polish-Czech correspondences (s,š), (sz,š), (c,t), (cz,t), and (szcz,št), then we can segment, or align, the initial sub-strings szcz and št from our happiness example in multiple ways. Three possible alignments are:
a) s z c z š t b) s z c z š t c) szcz št
Lacking an evaluation function, we cannot tell which of the three example alignments above is the best. However, if we are given the best alignment of our data, then we can straightforwardly compute probabilities for each of the correspondence rules, from which we can then compute the optimal rule costs. Similarly, knowing the costs of the rules allows us to compute the optimal alignment. Thus, our problem lends itself well to an Expectation-Maximization (EM) (Dempster et al., 1977) approach. To use EM, we must formulate both the expectation and the maximization steps. The expectation step is straightforward; we simply align the data with the current model. Themaximization step can be made intuitive with another observation. 2) If the optimal alignment is c), using rule (szcz,št), then any of the rules making up alignments a) and b) occur at least as often in the data as (szcz,št) does. Therefore, improving the model can be done by checking if an alignment is improved by merging any two compatible rules. 3) With this in mind, initialization of this model is straightforward: if we start by assuming no structure at all, then we will find the dominant structures in the data. Therefore, we start training from what we call a null alignment: one which uses only rules which contain exactly one symbol from exactly one language. An example for such a rule is (z,) in alignment a). We call such correspondence rules singleton rules. We next explain how we formalize our model in terms of MDL.The Minimum Description Length principle proposes that the optimal model is the one resulting in the most concise description of the modeled data. Importantly, in MDL-based modeling, we use only the data as evidence for our models and forego making assumptions about the nature of models in the form of prior probabilities. This makes it ideally suited for our purposes. For our model, we employ two-part MDL (Grünwald, 2007), which consists of a stack of two-part codes. The main formula for this is
M = arg min M∈M L(M) + L(D|M),
with D being the data at hand, M the explaining model, andM themodel classwe draw ourmodels
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
from. L(M) is the length, in bits1, of the description of modelM . Similarly,L(D|M) is the length, in bits, of the data given model M . Description lengths are simply code lengths: Shannon's source coding theorem (Shannon, 2001) tells us that the best prefix-free code for some data is derived from the (negative logarithm of the) probabilities of the data, i.e.
L(M) + L(D|M) = − log p(M)− log p(D|M).
From this, we see that two-part MDL can be considered to be a regularized maximum likelihood approach very similar to Bayesian inference. A crucial difference is that in MDL, L(M) serves as statistical formalization of the desired model class, whereas in Bayesian inference the analogous term expresses prior beliefs about the distribution of model parameters.
Due to MDL's roots in coding theory, it is common to call an MDL objective function a code, and to speak of encoding, transmitting, or sending the individual elements that make up a code.
We next detail how we design our code.Given a listD of cognate tuples fromN languages, we use the Minimum Description Length principle to infer the statistically significant correspondence rules. Our encoding builds upon and extends the encoding introduced by (Tatti and Vreeken, 2012) for discovering small sets of serial episodes in event sequences.
We proceed with a two-part code, requiring us to solve the optimization problem
M = arg min M∈M L(M) + L(D|M).
First, we must determine the model class, M. As explained in Section 2, we here seek to find associated character strings between languages. Thus, mathematically our model class is the set of sets of tuples associating strings from the individual languages' alphabets. Wemake no assumptions about either their shape or their distribution.
We begin by discussing our model code L(M).
4.1 Model Code L(M) Our models consist of N alphabets Σi and a correspondence rule table which we call Π. Our total
1We use log(.) = log2(.) throughout the paper.
model description length is given by
L(M) = N∑ i=1 (L(Σi)) + L(Π).
In order to describe the rules fromΠ, we require the code lengths for all letters σ from all alphabets Σi. Since we are interested only in complexities, we disregard the actual code words and focus only on their lengths. For ease of exposition, we first discuss L(Π). In essence, our Π is a list of independent correspondence rules. We next describe how we encode an individual rule.
Encoding a Correspondence Rule Rules π ∈ Π are of the form π = (π1, ..., πN ) with πi ∈ Σ∗i . To encode one such rule, we must specify a) how long the string from each of the languages is and b) which letters it contains. However, if we specify lengths even when they are zero, we pay with higher description lengths than are necessary. This imposes a bias particularly against rules which are sparsely populated. Thus, we encode each rule π ∈ Π, π = (π1, ..., πN ) as follows: we transmit all N entries independently of each other, sending their lengths and character sequences only where defined:
L(π) = N∑
n=1 πn ̸=ϵ
( LN(|πn|) +
∑ σ∈πn L(code(σ))
) .
We encode each string's length with LN, the universal code for the integers (Rissanen, 1983), which is the MDL-optimal code for natural numbers of unknown, arbitrary size. For transmitting the strings itself, we use code(σ), i.e. the optimal unigram for symbol usages in all rules.
Encoding the Rule Table  If we want to use the code for individual rules given above, we must specify for every rule which subset of languages it is defined on. We can straightforwardly classify each rule according towhich subset it is defined on. Then, we must specify how many rules defined on each of the different subsets there are. There are 2N − 1 different language subsets on which a rule may be defined. We encode the number of rules of each kind via LN. These numbers must be offset since LN(n) is defined for n ≥ 1, and there may be zero rules of a certain kind.
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
Thus:
L(Π) = 2N−1∑ i=1 LN(|ΠCi |+ 1) + ∑ π∈ΠCi L(π)  + LN(TΠ) + log( ( TΠ − 1 |Π| − 1 ) ).
where count(π) is the number of occurrences of π, TΠ = ∑ π∈Π count(π) and where ΠCi is the set of rules defined on the i-th subset of languages enumerated in some canonical way.
Additionally, to describe our data items using rules, we must specify the optimal code lengths for using each of the rules. In L(Π), this corresponds to the last two summands. We do this by a datato-model code (Grünwald, 2007). Data-to-model codes are used to code uniformly from an enumeration of models, i.e. without preference towards any particular model. Since we know that none of the symbols of any alphabets will have a non-zero count, the data-to-model code is given by the weak number composition of the alphabets symbols' total counts over the number of symbols.
Encoding the Alphabets For describing the strings of each rule, we use the Shannon-optimal code for the individual alphabets' symbols. Thus, we must first transmit the unigrams, i.e. code(σ) ∀σ ∈ Σi for every alphabet Σi. We again do this by a data-to-model code, i.e. by coding uniformly from all possible distributions.
Setting TΣi = ∑
σ∈Σi count(σ), the total transmission cost relating to some Σi becomes
L(Σi) = LN(TΣi) + log( ( TΣi − 1 |Σi| − 1 ) ).
The alphabet sizes are constant for any given data set and therefore it is not necessary to include them in the code.
Having described our model, we next turn our attention to encoding the data with a given model.
4.2 Data Code L(D|M) To encode data with our model, we simply transmit the correspondences best used to describe each data entry. Thus, we get
L(D|M) = ∑ d∈D L(d|M)
where L(d|M) = LN(|d|) + ∑ π∈d L(code(π)).
Again, it is not necessary to specify the number of data entries as they do not change for the same data set. For the individual data entries, we transmit their lengths via LN and specify which correspondence rules they are best aligned with via the best usage code for the rules, code(π). This leaves us to discuss finding the data's description given some rules, and how to infer rules.Computationally, finding the best description for a data item boils down to finding the best alignment for it. We formulate this as a shortest-path problem in a weighted, directed graph and use Dijkstra's algorithm (Dijkstra, 1959) to find optimal alignments. Nodes in the graph represent index tuples, while edges describe the applicable rules. By partial order reduction, we make our graphs as small as possible. Nonetheless, due to the combinatorial nature of the problem, there are bottlenecks in memory consumption as well as in runtime. With our current implementation, we can process up to five languages from our most complicated data set within a few hours on a 4GB RAM, 2.5GHz single core desktop machine. We plan to extend this to higher N in future work.Inferring correspondences of arbitrary length is a combinatorial, non-convex optimization problem defined over a large, unstructured search space. However, as we argued in Section 2, if we are given a rule table with costs, we can compute the optimal alignment of all data with these rules. Likewise, if we are given an alignment of all data, we can improve our model from it. Therefore, we can find good solutions by Expectation-Maximization (Dempster et al., 1977).
Initialization At the beginning of training, we either initialize our model with a null alignment or with a greedy alignment from a given set of rules. A null alignment is one in which only singleton rules are used, i.e. only rules which consist of exactly one character from exactly one language. Starting from a given rule set allows to input linguistic knowledge in an intuitive way.
Expectation Step In the Expectation step, we align all data items with the rules from the current rule table Π and the current usage costs for the rules. This results in new counts for all rules from which we compute costs in the next step. We
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
employ Laplace correction in order to ensure that the algorithm is always able to explain all data and may choose to not use locally suboptimal patterns.
The time complexity of our E step is in O(|D| · |R|2), where R is the maximum number of possible rule applications in a single data entry.
Maximization Step In the Maximization step, we optimize our code table. We do this by merging together the two patterns which lead to the highest decrease in overall description length. The intuition behind this is the observation that if a longer pattern is useful, then any sub-pattern of it will be at least as or more useful. It is important to note that in this way, the learned correspondences grow according to their statistical significance.
EachM step has time complexityO(|D|·A2/2), where A is the maximum number of rules used to align a single data entry.
It is possible that a rule R is deemed good on the basis of the entire data set when in fact it is suboptimal for some subset S of the data. Let us assume that the entries in S have become aligned with ruleR even though an overlapping, but different rule Q would have been a better choice. Then, S will be ""lost"" in regards to discovering Q, as S will not be counted as evidence for Q. However, if the remaining data contain enough evidence to learn the rule Q independently of S, Q will be in fact learned and may then be used for aligning S.
In this fashion, we deterministically learn the important structures in the data, although occasionally we may miss some of the more subtle correspondence rules.MDL guards against overfitting by balancing the complexity of themodel with that of the data – only those correspondences with sufficient evidence in the data are included. By confining ourselves fully to the data and not relying on any further assumptions, we obtain objective results. However, it may be that our focus in on the obtained rules rather than on the objective statistical analysis of the data.
For example, in cognate reconstruction, we require rules which relate strings across languages, but not those which exclusively describe substrings from the separate languages – the latter contribute nothing to successfully adapting a word from its known form(s) to an unknown one.
We can also encounter problems working on under-resourced or very rich languages, or even
when simply exploring correspondences between higher numbers of languages, where the need for data may exceed our ability to provide it. Sometimes, we simply may not have sufficient statistical evidence to discover all desired rules. In such cases we can turn to correspondences which objectively are not statistically significant for the given data, but are almost so. We can straightforwardly include those near-significant correspondences by over-representing the data. At the corpus level, this can be done by simply assigning the data complexity term a higher weight than the model term, i.e. via optimizing
L(M) + αL(D|M)
with α ̸= 1 instead of the original formula. In this formula, a data weight of e.g. 2 corresponds to using twice the amount of identical data. More subtly, we can also assign individual cognate tuples a higher weight than others. Again, assigning some data entry a weight of 2 intuitively duplicates that entry in the data set. With the data overrepresented in such a fashion, the algorithm will have enough evidence to include larger correspondence rules than before, as they effectively become more useful for describing the (over-represented) data. Nonetheless, new rules will be discovered in the ""correct"" order; that is, ordered by statistical significance. Clearly, this moves results away from the objective territory of MDL and introduces a user's subjective judgment. However, it allows a linguistic expert to choose a desired level of detail for inferred correspondences. We illustrate the effects and helpfulness of this idea experimentally, in Section 5.We compiled two data sets for our experiments. Firstly, we use Swadesh lists for 13 modern Slavic languages taken from the wiktionary.2 The languages are Czech, Polish, Slovak, Lower Sorbian, Upper Sorbian (west Slavic), Russian, Belarussian, Ukrainian, Rusyn (east Slavic), Bulgarian, Macedonian, Slovenian, and Serbo-Croatian (south Slavic). For Serbo-Croatian, we have both a version in Latin script and one in Cyrillic script.
Secondly, we add a set of Slavic cognates containing internationalisms and pan-Slavic words for Czech, Polish, Russian, and Bulgarian.3
All our data is in raw orthographic form, without transcriptions of any kind. It consists mostly of verbs, adjectives, and nouns.
data all lang. RU-BG CS-PL CS-PL-RU-BG size 207 778 778 778
T 1: Data set sizes for experiments.
For all of our experiments, we use only those entries that contain words for all languages in question. While our algorithm is agnostic to gaps in data, this makes for easier comparison.For pairwise analysis, we require some measure of distance between pairs of languages. In MDLbased modeling, it is common to use Normalized Compression Distance (NCD) (Cilibrasi and Vitanyi, 2005) for this. Intuitively, NCD measures how hard it is to describe X and Y together compared to how hard it is to describe them separately. It is defined as
NCD(X,Y ) = L(X,Y )−min(L(X,X), L(Y, Y ))
max(L(X,X), L(Y, Y ))
where L(X,Y ) is the description length when encoding languagesX and Y jointly. NCD is a mathematical distance; lower values mean that two data sets are more similar.
We train models for all pairs from our languages and obtain correspondence rules and NCDs.
2Taken from https://en.wiktionary.org/wiki/ Appendix:Slavic_Swadesh_lists.
3Compiled from (Likomanova, 2004) and (Angelov, 2004).
NCDs In Table 2 we show the NCD values for all pairwise comparisons. We use ISO 639-1 and ISO 639-3 codes to identify the languages, except for Serbo-Croatian, which we denote by SCl in its Latin version and and SCc in its Cyrillic version. We indicate lowest and highest NCDs per row in bold and italic text, respectively. Our table reveals that languages from the same linguistic group tend to have lower NCD than languages from differing groups. The south Slavic group is linguistically further divided into a southwestern group (Slovene and Serbo-Croatian) and a southeastern sub-group (Macedonian and Bulgarian). Indeed we identify Slovenian and SerboCroatian as more similar to languages from the west Slavic group than to the east Slavic group. We can see that the Serbo-Croatian data in Latin script was assessed to be slightly closer to the other languages that use Latin script, while the Cyrillic version was deemed more similar to other languages using Cyrillic.
usb lsb CS SK PL SL SCl SCc MK BG RU UK rue BE usb .00 .52 .53 .52 .60 .57 .61 .62 .76 .75 .68 .70 .67 .64 lsb .52 .00 .65 .66 .72 .67 .68 .71 .87 .85 .80 .82 .78 .74 CS .53 .65 .00 .41 .56 .50 .53 .55 .71 .69 .61 .64 .58 .59 SK .52 .66 .41 .00 .58 .48 .51 .56 .68 .66 .60 .65 .59 .60 PL .60 .72 .56 .58 .00 .64 .64 .67 .82 .79 .71 .74 .69 .63 SL .57 .67 .50 .48 .64 .00 .36 .39 .59 .58 .61 .65 .60 .61 SCl .61 .68 .53 .51 .64 .36 .00 .04 .54 .57 .63 .66 .62 .63 SCc .62 .71 .55 .56 .67 .39 .04 .00 .51 .53 .60 .63 .59 .59 MK .76 .87 .71 .68 .82 .59 .54 .51 .00 .54 .74 .78 .75 .75 BG .75 .85 .69 .66 .79 .58 .57 .53 .54 .00 .70 .77 .70 .71 RU .68 .80 .61 .60 .71 .61 .63 .60 .74 .70 .00 .52 .53 .51 UK .70 .82 .64 .65 .74 .65 .66 .63 .78 .77 .52 .00 .45 .45 rue .67 .78 .58 .59 .69 .60 .62 .59 .75 .70 .53 .45 .00 .54 BE .64 .74 .59 .60 .63 .61 .63 .59 .75 .71 .51 .45 .54 .00
T 2: NCDs for 13 Slavic languages.
Inferred Phylogenetic Tree For easier viewing, we construct a phylogenetic tree from theNCDvalues, which we show in Figure 1. For this we use the neighbor joining method (Saitou and Nei, 1987) and place the root manually.4 The greater the horizontal distance between two languages, the less similar they are. As we see, the algorithm groups the languages according to their linguistic classification. It identifies Bulgarian and Macedonian as slight outliers
4Picture generated with http://etetoolkit. org/treeview/, tree generated with scikit-bio: http://scikit-bio.org/.
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
F 1: NCD-based Slavic phylogenetic tree.
in the south Slavic group, and Polish, Upper and Lower Sorbian as such in the west Slavic group.
This is an expected result. Bulgarian and Macedonian are outliers in that they have largely lost case declension. Adjectives and verbs from these languages oftentimes employ zero endings or comparatively shorter endings than the other languages. This leads to overall lower BG-BG and MK-MK description lengths, but still high NCDs to the other languages. In the Swadesh list, Upper and Lower Sorbian words are often of a different etymological heritage than words from the other Slavic languages. This explains their outlier status. Polish is a slight outlier due to its frequent use of digraphs and prolific palatalization, which increases complexity of Polish patterns.
Inferred Correspondences In Table 3, we present some example alignments from the models for the CS-PL and RU-BG language pairs.
To give insight into what kinds of rules emerge naturally, and what kind emerge from overrepresenting data as proposed in Section 4.5, we present two different alignments per selected example: firstly one obtained from an objective model, and secondly one obtained from overrepresented data. For the latter, we over-weight our data until there are no more singleton rules in any of the alignments. We mark over-weighted versions with *.
(PL) (CS) z e m ě z ie m i a *ziem ia *zem ě (PL) (CS) m i l c z e ć m l č e t *mi l cz eć *m l č et (PL) (CS) r ó g r o h *ró g *ro h (PL) (CS) r o z d z i e l i ć r o z d ě l i t *roz dzie li ć *roz dě li t
(RU) (BG) м о л о д о с т ь м л а д о с т *м оло до сть *м ла до ст
(RU) (BG) п ъ л е н п о л н ый *п ъл ен *п ол ный
T 3: Example CS-PL, RU-BG correspondences.
As can be seen, the discovered correspondences are of different granularities and linguistic char-
acter. We find purely phonological rules such as (g,h) along with purely orthographic ones such as (cz,č), verb endings such as (ć,t), aberrations thereof such as (eć,et), liquid metatheses such as (ла,оло), palatalizations such as (dzie,dě), and even stem correspondences such as (ziem,zem). Over-representing the data allows to select from a desired level of detail. We plan to discuss potential applications of the different resulting rules in future work.Next, we turn our attention to more fine-grained analysis of linguistic similarity. Restricting ourselves to pairwise analyses and grouping the most similar languages together in a phylogeny causes us to miss many subtle similarities. Existing approaches which are limited to pairwise correspondences incur infeasible amounts of computationwhen trying to use them for the simultaneous analysis of multiple languages. Our algorithm is agnostic to the number of input languages and can be used to efficiently analyze more than two languages at a time. This allows for highly detailed information-theoretic quantification of the similarities among groups of languages. To show how this can be done, we first present some four-way CS-PL-RU-BG example alignments in Figure 4. Theywere computedwithout over-weighting.
(PL) (CS) (RU) (BG) p i ć p í t п и ть п и я (PL) (CS) (RU) (BG) s p e c j a l n y s p e c i á l n í с п е ц и а ль н ы й с п е ц и а л ен
(PL) (CS) (RU) (BG) m i ł y m i l ý м и л ый м и л (PL) (CS) (RU) (BG) ś m i a ł y s m ě l ý с м е л ый с м е л
T 4: Example CS-PL-RU-BG correspondences.
Observe that some of the discovered rules link only two or three languages, with the other language(s) described by separate patterns. We have selected some examples to highlight the differences in i vowels. In the data, there is enough evidence to discover various rules, such as (,,и,и), (,i,и,и), but not enough evidence to include a fourway rule (j,i,и,и). In consequence, the internationalism specjalny is analyzed with the three-way i correspondence plus a Polish singleton rule (j,,,). There are also cases where using two rules, such as (i,í,,) plus (,,и,и) in pić (to drink), is a good choice.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
Because we designed our algorithm to discover the correspondence rules according to their statistical significance, this can be exploited for finegrained analyses of similarity. Statistically, the observed correspondences between two or three languages are significant for the data, while potential larger ones are not. In other words: there is more regularity in some of the languages than others.To quantify the amount of structure that individual languages share, we can compare the description lengths of their rules. For this, we define the shared Description Length of languages Li1 , ..., Lik as
sDL(Li1 , ..., Lik) := ∑
π∈Q(Li1 ,...,Lik )
L(π).
where Q(Li1 , ..., Lik) contains all rules which are non-empty exactly for languages Li1 , ..., Lik .
Figures 2 and 3 show sDLs for the CS-PL, RUBG, and CS-PL-RU-BG models.
.
.cs-pl ru-bg
PL
BG
CS
RU
CS-PL
RU-BG
shared Description Length
F 2: sDLs for the CS-PL and RU-BGmodels. Total rule desc. lengths: 1852.43 bits (CS-PL), 1496.62 bits (RU-BG)
Figure 2 reveals that RU diverges more than BG does from the RU-BG joint description, and that CS does so for CS-PL. Because we chose only cognate tuples defined for all four languages for this experiment, we can also compare the two language pairs. There, we see that CS-PL requires a larger description, and that Czech alone takes a somewhat larger fraction of total description length.
. all CS PL RU BG
RU -BGCS -PL PL -RU
PL -RU
-BG
CS -PL
-RU
CS -PL
-BG PL -BG CS -RU CS -BG
CS -RU
-BG 0
500
1,000
F 3: sDLs for the CS-PL-RU-BG model.
Figure 3 finally gives us a quantification of the similarities between all language sub-sets from our Czech-Polish-Russian-Bulgarian set. We see that
the four-way sDL is the biggest contributor overall. It quantifies the complexity of the structure shared by all four languages. We also see that each of the individual languages have significant overheads to the four-way shared description length. The language with the highest individual description length by far is Czech. This is not surprising, as Czech has the largest number of diacritically-modified symbols. For example, every Czech vowel can be marked as long with the čárka, giving us e.g. é as long version of e. The algorithm furthermore identifies the linguistic grouping between Czech and Polish. Both CS-PL and RU-BG share significant portions of description length. In the NCD table, Table 2, we can observe that the south Slavic languages were somewhat in between the west and the east Slavic languages. In this analysis, we can see that Bulgarian is in fact very similar to Russian, so much so that in the four-way analysis, grouping RU-BG seems as good a choice as grouping CS-PL.5 Beyond this, we identify further, more subtle similarities. Taking a closer look we see they are between Russian and other, not-yet-covered language subsets. To highlight this, we have plotted the similarities to Russian in red. This is a highly satisfying result, as in fact we expect Russian, the Slavic language with the largest amount of native speakers, to heavily influence the other languages.","We show our approach's efficacy in several ways. First, we present a standard pairwise analysis for a group of languages for which we have collected data we deem representative. We compute pairwise distances, construct a phylogenetic tree, and compare this tree to linguistic classifications. Second, we present a detailed analysis of four languages simultaneously. Because our approach is not limited to pairs of languages, we can give an information-theoretic quantification of linguistic similarity in a much more detailed fashion than was previously possible.
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
In both cases, we report some of the learned correspondences. We also show the effect of data over-weighting, as introduced in Section 4.5.
We first discuss our data, then present results.","Systematic correspondences between languages form the basis for much linguistic work. Researchers employ them to e.g. improve teaching, analyze and quantify the similarity or relatedness of languages, or to formulate hypotheses aboutmutual intelligibility. Beyond that, they are of importance in multi-language natural language processing, notably in machine translation.
Correspondence rules can be established on the basis of various linguistic features, such as the language's alphabets, their orthographies, their phonologies, or their inflectional and derivational morphologies. As an example, the Czech, Polish, Russian, andBulgarian forms of the pan-Slavic word for happiness could be analyzed to have the following ortho-phonetic correspondences:
In order to find correspondences, a linguist typically collects cognates from two ormore languages and compares them manually. If the linguist observes an often-occurring pattern, or one that fits well with other known changes that occurred between the languages, then she might conclude that this pattern is systematic and use it as basis for further investigations. This technique, called the comparative method, dates back to at least the 1800s (Szemerenyi, 1970). Recently, researchers have devised various statistical approaches to identifying the regular correspondences between languages. Much of these focus on cognate identification or reconstruction (Schulz et al., 2004; Snyder et al., 2010), on discovering and quantifying etymological relationships between languages (Wettig et al., 2011), or on discovery of pseudo-morphological sub-word alignments (Snyder and Barzilay, 2008). However, most existing statistical approaches are afflicted by a number of problems: they may impose arbitrary assumptions on the distribution or shape of correspondences, may not allow for integration of linguistic knowledge, or may be limited to pairs of languages. While imposing assumptions is sometimes necessary in order to obtain any results at all, it leads to finding not the ""true"" correspondences hidden in the data, but their closest similes from the assumed distribution. How, then, can we discover correspondences between more than two languages without prior assumptions about their shape or distribution? This is the question we answer in this paper. For this, we employ the Minimum Description Length (MDL) principle (Grünwald, 2007). MDL provides a statistically well-founded approach to identifying the
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
best model for given data, and has amongst others been used to model changes in etymologicallyrelated words (Wettig et al., 2011).
Using MDL, we deem the set of correspondences that describes the data most succinctly to be the best. We propose an efficient, deterministic algorithm to infer good sets of correspondence rules directly from data. In our experiments, we present a phylogenetic analysis of a number of Slavic languages and show how our approach can be used for efficient, highly detailed quantification of stringlevel similarities among more than two languages.
In our pairwise analysis, we confirm that stringlevel similarity between languages is a strong reflection of linguistic classification. In our fourlanguage experiment, we find that our algorithm successfully identifies linguistic sub-groups while quantifying the similarity between all subsets of our four analyzed languages.
The paper is structured as follows: we give an overview of our approach and terminology in Section 2, then present our model in Section 4. After this, we report learned correspondences and provide information-theoretic analyses of language similarity in Section 5, and conclude in Section 6.",,,"We studied the problem of automatically inferring objective string-level correspondences from data. We introduced an MDL-based approach and gave an efficient algorithm for finding correspondences. Our experiments show that the approach works well in practice. We constructed a sensible phylogeny for our languages, demonstrated the discovered correspondences, and showed that our algorithm quantifies similarity not only between pairs, but between all subsets of analyzed languages. While our algorithm is deterministic in its pure form, it is easy to integrate non-determinism e.g. by simulated annealing, since the alignments can be randomized in any step. However, the possibility to input linguistic knowledge and obtain deterministic results makes our approach particularly promising for linguists. We plan to discuss its linguistic potential in future work.
5Please note that this is purely on the basis of superficial word form similarity.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
31,"Named Entity Recognition (NER) models for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on another, source, language (or multiple source languages).  We introduce a language independent method for NER, building on cross-lingual wikification, a technique that grounds words and phrases in non-English text into English Wikipedia entries. Thus, mentions in any language can be described using a set of categories and FreeBase types, yielding, as we show, strong language-independent features.  With this insight, we propose an NER model that can be applied to all languages in Wikipedia.                        When trained on English, our model outperforms comparable approaches on the standard CoNLL datasets (Spanish, German, and Dutch) and also performs very well on low-resource languages (e.g., Turkish, Tagalog, Yoruba, Bengali, and Tamil) that have significantly smaller Wikipedia. Moreover, our method allows us to train on multiple source languages, typically  improving NER results on the target languages. Finally, we show that our language-independent features can be used also to enhance monolingual NER systems, yielding improved results for all 9 languages.",Cross-Lingual Named Entity Recognition via Wikification,166,"This paper proposes an approach for multi-lingual named entity recognition
using features from Wikipedia. By relying on a cross-lingual Wikifier, it
identifies English Wikipedia articles for phrases in a target language and uses
features based on the wikipedia entry. Experiments show that this new feature
helps not only in the monolingual case, but also in the more interesting direct
transfer setting, where the English model is tested on a target language.

I liked this paper. It proposes a new feature for named entity recognition and
conducts a fairly thorough set of experiments to show the utility of the
feature. The analysis on low resource and the non-latin languages are
particularly interesting.

But what about named entities that are not on Wikipedia? In addition to the
results in the paper, it would be interesting to see results on how these
entities are affected by the proposed method. 

The proposed method is strongly dependent on the success of the cross-lingual
wikifier. With this additional step in the pipeline, how often do we get errors
in the prediction because of errors in the wikifier?

Given the poor performance of direct transfer on Tamil and Bengali when lexical
features are added, I wonder if it is possible to regularize the various
feature classes differently, so that the model does not become over-reliant on
the lexical features.",,4,4,Oral Presentation,5,4,4,4,4,5,3,4,2016,"Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language.
There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011).
Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data.Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Song and Roth, 2014), to generating parallel data (Smith et
3
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
al., 2010), to use in open information extraction (Wu and Weld, 2010). It has also been used to extract training data for NER, under the intuition that Wikipedia is already (partially) annotated with NER labels, in the form of links to pages. Nothman et al. (2012) generate silver-standard NER data from Wikipedia using link targets, and other heuristics. This can be gathered for any language in Wikipedia, but several of the heuristics depend on language-specific rules. Al-Rfou et al. (2015) generate training data from Wikipedia articles using a similar manner. The polyglot word embeddings (Al-Rfou et al., 2013) are used as features in their NER model. Although the features are delexicalized, the embeddings are unique to each language, and so the model cannot transfer.
Kim et al. (2012) use Wikipedia to generate parallel sentences with NE annotations. They propose a semi-CRF model for aligning entities in parallel sentences. Results are very strong on Wikipedia data. This is a hybrid approach in that it is supervised projection using Wikipedia.
Our work is most closely related to Kazama and Torisawa (2007). They do NER using Wikipedia category features for each mention. However, their method for wikifying text is not robust to ambiguity, and they only do monolingual NER.
Sil and Yates (2013) create a joint model for NER and entity linking (EL) in English. They avoid the traditional pipeline of NER then EL by overgenerating mentions in the first stage and using NER features to rank candidates. While the results are promising, the model is not scalable to other languages because it requires a trained NER system and NP chunker.The idea of direct transfer is to train a model in a high-resource setting using delexicalized features, that is, features that do not depend on word forms, and to directly apply it to text in a new language.
Täckström et al. (2012) experiments with direct transfer of dependency parsing and NER, and shows that using word cluster features can help, especially if the clusters are forced to conform across languages. The cross-lingual word clusters are induced using large parallel corpora.
Building on this work, Täckström (2012) focuses solely on NER, and includes experiments on self-training, and multi-source transfer for NER. Their experiments are orthogonal to ours, and
could be combined nicely. This work is closest to ours in terms of method, and so we compare against it in our experiments.
Our work falls under the umbrella of direct transfer methods combined with use of Wikipedia. We introduce wikifier features, which are truly delexicalized, and use Wikipedia as a source of information for each language.We use the state of the art English NER model from Ratinov and Roth (2009) as a the base model. This model approaches NER as a multiclass classification problem with greedy decoding, using the BIO labeling scheme. The underlying classifier is averaged perceptron.
Table 1 summarizes the features used in our model. These can be divided into a base set of standard features which are included in Ratinov and Roth (2009), a set of gazetteer features which are based on titles in multilingual Wikipedia, and our novel cross-lingual wikifier features. The base set of features can be further divided into nonlexical and lexical categories.Non-Lexical Features Ratinov and Roth (2009) uses a small number of non-lexical features. For example, the previous tag feature is useful in pre-
4
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
dicting I- tags, because the previous tag should never be an O. The tag context feature looks in a 1000 word history and gathers statistics over tags assigned to words [wi, wi+1, wi+2]. These features are included in all experiments.
In contrast with (Täckström et al., 2012), we do not use POS tags as features. We could not get the universal POS tags for all languages in our experiments, and an earlier experiment indicated that adding POS tags do not improve the performance due to the accuracy of tagger. Lexical Features Lexical features are very important for monolingual NER. In the direct transfer setting, lexical features are useful if the target language is close to the training language. We use a small number of simple features, including word forms, affixes, capitalization, and tag patterns. The latter feature looks at a small window of text (at most 2 tokens) before the word in question. If there is a named entity in the window, it makes a feature out of NETag+wi−2 + wi−1. Word type features simply indicate whether the word in question is all capitalized, is all digits, or is all letters.One of the larger performance improvements in Ratinov and Roth (2009) came from the use of gazetteers. We include gazetteers also in our model, except we gather them in each language from Wikipedia. As in Ratinov and Roth (2009), we use the gazetteers as features in the model. Specifically, we group gazetteers by topic, and use the name of the gazetteer file as the feature.
The method is to iteratively extend a short window to the right of the word in question. As the window increases in size, we search all gazetteers for occurrences of the phrase in the window. If we find a match, we add a feature to each word in the phrase according to its position in the phrase, either B for beginning, I for inside, or L for last. If the phrase is a single word, it is given a U feature.
This method generalizes gazetteers to unseen entities. For example, given the phrase “Bill and Melinda Gates Foundation”, “Bill” is marked as both B-PersonNames and B-Organizations, while “Foundation” is marked as L-Organizations. Imagine encountering at test time a fictional organization called “Dave and Sue Harris Foundation.” Although there is no gazetteer that contains this name, we have learned that “B-PersonName and B-PersonName B-PersonName Foundation” is a
strong signal for an organization.As shown in Figure 1, disambiguating words to Wikipedia entries allows us to obtain useful information for NER from the corresponding FreeBase types and Wikipedia categories. A crosslingual wikifier grounds words and phrases of non-English languages to the English Wikipedia, which provides language-independent features for transferring an NER model directly.
We use the system proposed in Tsai and Roth (2016), which grounds input strings to the intersection of the English Wikipedia and the target language Wikipedia. The only requirement of this system is the multilingual Wikipedia dump and it can be applied on all languages in Wikipedia.
Since we want to ground every n-gram (n ≤ 4) in the document, deviating from the normal usage that only considers few mentions of interest, we modify the system in the following two ways:
• The original candidate generation process queries the index by both whole input string and the individual tokens of the string. For the n-grams where n > 1, we generate title candidates only according to the whole string, not individual tokens. For instance, if it is allowed to generate title candidates based on individual tokens, the bigram “in Germany” will be linked to the title Germany thus wrongly considered as a named entity.
• The original ranking model includes the embeddings of other mentions in the document as features. It is clear that if we know what other important entities exist in the document, they provide useful clues to disambiguate a mention. However, if we want to wikify all n-grams, it makes no sense to include all of them as features, since the ranking model has already included features from TF-IDF weighted context words.
After wikifying every n-gram 1, we set the types of each n-gram as the coarse- and fine-grained FreeBase types and Wikipedia categories from the top 2 title candidates returned by wikifier. For each word wi, we use the types of wi, wi+1, and wi−1, and the types of the n-grams which contain wi as features. Moreover, we also include the
1We set n to 4 in all our experiments.
5
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
ranker features in wikifier from the top candidate as features. This could serve as a linker (Ratinov et al., 2011), which rejects the top prediction if it has a low confidence.In this section, we conduct experiments to validate and analyze the proposed NER model. First, we show that adding wikifier features improves results on monolingual NER. Second, we show that wikifier features are strong signals in direct transfer of a trained NER model across languages. Finally, we explore the importance of Wikipedia size to the quality of wikifier features and study using multiple source languages.We use data from CoNLL2002/2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The 4 languages represented are English, German, Spanish, and Dutch, each annotated using the IOB1 labeling scheme, which we convert to the BIO labeling scheme. All training is on the train set, and testing is on the test set. The evaluation metric for all experiments is phrase level F1, as explained in (Tjong Kim Sang, 2002).
In order to experiment on a broader range of languages, we also use data from the REFLEX (Simpson et al., 2008) and LORELEI projects. From LORELEI, we use Turkish,2 From REFLEX, we use Bengali, Tagalog, Tamil, and Yoruba.3 While Turkish, Tagalog, and Yoruba each have a few non-Latin characters, Bengali and Tamil are with an entirely non-Latin script. This is a major reason for inclusion in our experiments. We use the same set of test documents as used in Zhang et al. (2016). All other documents in the REFLEX and LORELEI packages are used as the training documents in our monolingual experiments. We refer to these five languages collectively as low-resource languages.
Besides PER, LOC, and ORG, some lowresource languages contain TIME tags and TTL tags, which represented titles in text, such as Secretary, President, or Minister. Since such words are not tagged in CoNLL training data, we opted to simply remove these tags. On the other hand, there is no MISC tag in the low-resource languages. Instead, many MISC-tagged entities in the
2LDC2014E115 3LDC2015E13,LDC2015E90,LDC2015E83,LDC2015E91
CoNLL datasets have LOC tags in the REFLEX and LORELEI packages, e.g., Italian and Chinese. We modify a MISC-tagged word to LOC tag if it is grounded to an entity with location as a FreeBase type, and remove all the other MISC tags in the training data. This process of changing MISC tags is only done when we train on CoNLL documents and test on low-resource languages.
The only requirement to build the cross-lingual wikifier model is a multilingual Wikipedia dump, and it can be trivially applied to all languages in Wikipedia. The top section of Table 2 lists Wikipedia sizes in terms of articles,4 the number of titles linked to English titles, and the number of training and test mentions for each language.
Besides the English gazetteers used in Ratinov and Roth (2009), we collect gazetteers for each language using Wikipedia titles. A Wikipedia title is included in the list for person names if it contains FreeBase type person. Similarly, we also create a location list and an organization list for each language. The total number of names in the gazetteers of each language is listed in Table 2.We begin by showing that wikifier features help when we train and test on the same language. The middle section of Table 2 shows the results.
In the ‘Wikifier only’ row, we use only wikifier features and previous tags features. This is intended to show the predictive power of wikifier features alone. Without using any lexical features, it gets good scores on the languages that have a large Wikipedia. These numbers represent the quality of the cross-lingual wikifier in that language, which in turn is correlated with the size of Wikipedia and size of the intersection with English Wikipedia.
The next row, ‘Base features’, shows that lexical features are always better than wikifier features only. This squares with the common wisdom that lexical features are important for NER.
Adding gazetteers to the base features improves more than 3 points for every language except Bengali and Tamil. Since these two languages use entirely non-Latin scripts, the words will not match any names in the English gazetteers, which have higher coverage than other languages’ gazetteers.
Finally, the ‘+Wikifier’ row shows that our pro-
4From https://en.wikipedia.org/wiki/ List_of_Wikipedias, retrieved March 2016
6
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
Latin Script Non-Latin Script
APPROACH EN NL DE ES TR TL YO BN TA AVG
Wiki size 5.1M 1.9M 1.9M 1.3M 269K 64K 31K 42K 85K - En. intersection - 755K 964K 757K 169K 49K 30K 34K 51K - Gazetteer size 8.5M 579K 1M 943K 168K 54K 20K 29K 10K - Entities (train) 23.5K 18.8K 11.9K 13.3K 5.1K 4.6K 4.1K 8.8K 7.0K - Entities (test) 5.6K 3.6K 3.7K 3.9K 2.2K 3.4K 3.4K 3.5K 4.6K -
Monolingual Experiments
Wikifier only 72.90 57.38 51.95 59.33 52.63 51.41 33.63 45.96 37.83 51.45 Base Features 85.24 77.34 65.11 79.98 65.21 74.34 54.78 69.11 55.30 69.60 +Gazetteers 88.92 82.69 68.74 83.51 70.67 77.53 57.89 69.50 56.76 72.91 +Wikifier 89.47 84.90 72.97 84.25 73.50 78.18 59.27 70.62 60.00 74.80
Direct Transfer Experiments
Wikifier only 36.66 38.55 40.04 43.09 36.97 25.09 41.81 27.85 36.26 Base Features 44.10 25.24 41.81 30.50 50.45 32.48 2.30 1.74 28.58 +Gazetteers 49.66 35.06 55.04 30.90 64.07 34.42 3.14 0.30 34.07 +Wikifier 62.10 47.14 60.97 48.41 65.32 36.79 6.72 2.99 41.31
Täckström baseline 48.4 23.5 45.6 - - - - - - Täckström bitext clusters 58.4 40.4 59.3 - - - - - - Zhang et al. (2016) - - - 43.6 51.3 36.0 34.8 26.0 38.3
Table 2: Data sizes, monolingual experiments, and direct transfer experiments. Wiki size is the number of articles in Wikipedia. For monolingual experiments, we train the proposed model on the training data of the target languages. ‘Wikifier only’ uses the previous tags features also. For direct transfer experiments, all models are trained on CoNLL English training set. The rows marked Täckström come from (Täckström et al., 2012), and are the baseline and clustering result. The plus signs (+) signify cumulative addition. EN: English, NL: Dutch, DE: German, ES: Spanish, TR: Turkish, TL: Tagalog, YO: Yoruba, BN: Bengali, TA: Tamil.
posed features are valuable even in combination with strong features. It improves upon base features and gazetteer features for all 9 languages. These numbers may be less than state of the art because the features we use are designed for English, and may not capture lexical subtleties in every language. Nevertheless, they show that wikifier features have a non-trivial signal that has not been captured by other features.We evaluate our direct transfer experiments by training on English and testing on the target language. The results from these experiments are shown in the bottom section of Table 2.
The ‘Wikifier only’ row shows that the wikifier features alone preserve a signal across languages. Interestingly, for both Bengali and Tamil, this is the strongest signal, and gets the highest score. If the lexical features are included when we train the English model, the learning algorithm will give them too much emphasis, thus decreasing the importance of the wikifier features. Since Bengali and Tamil use non-Latin scripts, no lexical feature
in English will fire at test time. Thus, approaches that include base features perform poorly.
The results of ‘Base features’ can be viewed as a sort of language similarity to English, which, in this case, is related to lexical overlap and similarity between the scripts. Comparing to monolingual experiments, we can see that the lexical features become weak in the cross-lingual setting.
The gazetteer features are again shown to be very useful for almost all languages except Bengali and Tamil due to the reason explained in the monolingual experiment and to the inclusion of lexical features. For the rest of languages, the improvement from adding gazetteers is even more than the improvement in the monolingual setting.
For nearly every language, wikifier features help dramatically, which indicates that they are very good delexicalized features. It adds more than 10 points on Dutch, German, and Turkish.
The trend in Table 2 suggests the following strategy when we want to extract named entities in a new foreign language: It is better to include all features if the foreign language uses Latin script, since the names are likely mentioned using the
7
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
same way as in English. Otherwise, using wikifier features could be the best setting.
Täckström et al. (2012) also directly transfer an English NER model using the same setting as ours: train on the CoNLL English training set and predict on the test set of other three languages. We compare our baseline transfer model (Base Features) to the row denoted by “Täckström baseline”. Even though we do not use gold POS tags, we see that our results are comparable. The second Täckström row uses parallel text to induce multilingual word clustering. While this approach is orthogonal to ours, and could be used in tandem to get even better scores, we compare against it for lack of a more closely aligned scenario. We see that for each language, we significantly outperform their approach.
We note that our numbers are comparable to those reported for WIKI-2 in Nothman et al. (2012) for the CoNLL languages (with the exception of German, where theirs is higher). Their system requires language-specific heuristics to generate their silver-standard training data from Wikipedia articles. What they gain for single languages, they likely lose in generalizability. This approach is orthogonal to ours and we can also use their silverstandard data in training.
For the low-resource languages, we compare our direct transfer model with the expectation learning model proposed in Zhang et al. (2016). This model is not a direct transfer model, but it does not use any training data in the target languages either. Instead, for each target language, it generates patterns from parallel documents between English and the target language, a large monolingual corpus in the target language, and one-hour interaction with a native speaker of the target language. Note that they also use a crosslingual wikifier, but only for refining the entity types. On the other hand, in our model, the features from the wikifier are used in both detecting entity mention boundaries and the entity types. We can see that our approach performs better than their model on all five languages even though we assume much fewer resources. The difference is most significant on Turkish, Tagalog, and Bengali.One immediate question is why wikifier features are less helpful on the low-resource languages results than on the CoNLL languages? In this exper-
FEATURES SPANISH GERMAN #inter. F1 #inter. F1
Wikifier only 757K 40.04 964K 38.55 W.−FB query 757K 34.69 964K 28.27 W.−FB−50% inter. 379K 30.32 482K 27.24 W.−FB−90% inter. 76K 29.44 96K 25.94
Table 3: The F1 scores of using only wikifier features with removing the support from FreeBase and varying the number of titles linked to the English Wikipedia. ‘W.−FB query’ removes the component of querying FreeBase by the target language title from ‘Wikifier only’. ‘−X% inter.’ indicates removing X% of the interlanguage links with English titles. The column #inter. shows the number of titles that intersect with English.
iment, we show that smaller Wikipedia sizes result in worse Wikipedia features, which is the reason Yoruba has bad ‘Wikifier only’ results and the improvement from the wikifier features is much smaller.
The cross-lingual wikifier that we use in our system only grounds words to the intersection of the English and target language Wikipedia. Given a Wikipedia title in the target language, we first retrieve FreeBase ID by querying FreeBase API. If it fails, we find the corresponding English Wikipedia title via interlanguage links and then query the API with the English title. However, FreeBase does not contain entities in Yoruba, Bengali, and Tamil, so the first step will always fail for these three languages. We remove this step in the experiments of high-resource languages and the results are shown in the row ‘W.−FB query ’ of Table 3. We can see that the performance drops a lot, because many words have no features from FreeBase types.
Next, we randomly remove 50% and 90% of the interlanguage links to English titles. This will not only reduce the number of fired features from Wikipedia categories, but also FreeBase types since English titles are used to query FreeBase IDs. When 90% of interlanguage links are removed, the scores of Spanish and German are closer to Yoruba’s score (27.85).In all previous experiments, the training language is always English. In order to test the efficacy of training with languages other than English, we create a train/test matrix with all combinations of languages, as seen in Figure 2.
8
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
Figure 2: Different training/test language pairs. Scores shown are the F1 scores. The red boxes signify the best non-target training languages.
The vertical axis represents training language, and the horizontal axis represents test language. A darker color signifies a higher score. For example, if we train on Spanish (es) and test on Yoruba (yo), we get an F1 of 39.4. When the test language is Bengali (bn) or Tamil (ta), we only use wikifier features. For other test languages, all features are included. Note that we ignore all MISC tags in the CoNLL languages (en, nl, de, es) in evaluation, since there is no MISC tag in the low-resource languages. The diagonals represent the monolingual setting in which we use all features for all languages. Since we are interested in transferring a model, we ignore the diagonals, and identify the best training language for a given test language as the largest off-diagonal in each column. These are demarcated with red boxes.
English is the best for most languages, with the exception of Dutch, best for Spanish, and Spanish, best for Yoruba. It makes sense that highresource languages are better training languages because 1) there are more annotated training instances, 2) larger Wikipedia creates denser wikifier features, therefore providing better estimation of the weights to these features.
Table 4 shows the results of training on multiple languages. We use all features in this experiment. The row “EN” only trains the model on the English training documents, and the results are identical to those shown in Table 2. Adding
Spanish training data yields the best score on Yoruba, which agrees with Figure 2 where Spanish is the best training language for Yoruba. Using all CoNLL languages (EN+ES+NL+DE) adds more than 1 point F1 in average comparing to using English only. Finally, training on all but the test languages further improves the results.
This experiment shows that we can augment training data from other languages’ annotated documents. Although the performance only increases a little, it does not hurt most of the time.",,"Named Entity Recognition (NER) is the task of identifying and typing phrases that contain the names of persons, organizations, locations, and so on. It is an information extraction task that is important for understanding large bodies of text and
is considered an essential pre-processing stage in Natural Language Processing (NLP) and Information Retrieval systems.
NER is successful for languages which have a large amount of annotated data, but for languages with little to no annotated data, this task becomes very challenging. There are two common approaches to address the lack of training data problem. The first approach is to automatically generate annotated training data in the target language from Wikipedia articles or from parallel corpora. The performance of this method depends on the quality of the generated data and how well the language-specific features are explored. The second approach is to train a model on another language which has abundant training data, and then apply the model directly on test documents in the target language. This direct transfer technique relies on developing language-independent features. Note that these two approaches are orthogonal and can be used together.
In this paper, we focus on the direct transfer setting. We propose a cross-lingual NER model which is trained on annotated documents in one or multiple source languages, and can be applied to all languages in Wikipedia. The model depends on a cross-lingual wikifier, which only requires multilingual Wikipedia, no sentence-aligned or wordaligned parallel text is needed.
Recently, much attention has been given to cross-lingual wikification and entity linking research (Ji et al., 2015; Ji et al., 2016; Moro et al., 2014; Tsai and Roth, 2016). The key contribution of the current paper is the development of a method that makes use of cross-lingual wikification to generate language-independent features for NER, and showing how useful this can be to training NER models with no annotation in the target language. Given a mention (sub-string) from a document written in a foreign language, the goal
2
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
Schwierigkeiten beim nachvollziehenden Verstehen Albrecht Lehmann läßt Flüchtlinge und Vertriebene in Westdeutschland
Problem_solving Understanding Albert,_Duke_of_Prussia Jens_Lehmann Refugee Western_Germany
hobby media_genre media_common quotation_subject
person noble_person
person athlete
field_of_study literature_subject
location country
Wikipedia titles: FreeBase types:
NER Tags: Person Location Sentence:
Figure 1: An example of German sentence. We ground each word to the English Wikipedia by a crosslingual wikifier. A word is not linked if it is a stop word or the wikifier returns NIL. We can see that the FreeBase types are strong signals to NER even with imperfect disambiguation.
of cross-lingual wikification is to find the corresponding title in the English Wikipedia. Traditionally, wikification has been considered a downstream task of NER. That is, a named entity recognizer is firstly applied to identify the mentions of interest, and then the wikifier grounds the extracted mentions to Wikipedia entries. In contrast to this traditional pipeline, we show that the ability to disambiguate words is very useful in identifying named entities. By grounding every n-gram to the English Wikipedia, we can obtain useful clues regardless of the language used in testing documents.
Figure 1 shows an example of a German sentence. We use a cross-lingual wikifier to ground each word to the English Wikipedia. We can see that even though disambiguation is not perfect, the FreeBase types still provide valuable information. That is, although “Albrecht Lehmann” is not an entry in Wikipedia, the wikifier still links “Albrecht” and “Lehmann” to people. Since words in any language are grounded to the English Wikipedia, the corresponding Wikipedia categories and Freebase types can be used as language-independent features.
The proposed model significantly outperforms comparable direct transfer methods on Spanish, Dutch, and German CoNLL data. We also evaluate the model on five low-resource languages: Turkish, Tagalog, Yoruba, Bengali, and Tamil. Due to small sizes of Wikipedia, the overall performance is not as good as the CoNLL experiments. Nevertheless, the wikifier features still give significant improvement, and the proposed direct transfer model outperforms the state of the art, which assumes parallel text and some interaction with a native speaker of the target language. In addition, we show that the proposed languageindependent features not only perform well on the direct transfer scenario, but also improve monolingual models, which are trained on the target language. Another advantage of the proposed direct
transfer model is that we can train on documents from multiple languages together, and further improve the results.","There are three main branches of work for extending NLP systems to many languages: projection across parallel data, Wikipedia-based approaches, and direct transfer. Projection and direct transfer take advantage of the success of NLP tools on high-resource languages. Wikipediabased approaches exploit the fact that by editing Wikipedia, thousands of people have made annotations in hundreds of languages.",,"We propose a language-independent model for cross-lingual NER using a cross-lingual wikifier to disambiguate every n-grams. This model works on all languages in Wikipedia and the only requirement is a Wikipedia dump. We study a wide range of languages in both monolingual and crosslingual settings, and show significant improvements over strong baselines. An analysis shows that the quality of wikifier features depends on the Wikipedia size of the test language.
This work shows that if we can disambiguate words and phrases to the English Wikipedia, the typing information from Wikipedia categories and FreeBase are useful language-independent features for NER. However, there is other information from Wikipedia which we do not use, such as words from documents and relations between titles, which would require additional research.
In the future, we would like to experiment with including other techniques for multilingual NER that we discuss in Section 2 into our model, such as parallel projection and generating training data from Wikipedia automatically.
9
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935"
32,"Named Entity Recognition (NER) models for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on another, source, language (or multiple source languages).  We introduce a language independent method for NER, building on cross-lingual wikification, a technique that grounds words and phrases in non-English text into English Wikipedia entries. Thus, mentions in any language can be described using a set of categories and FreeBase types, yielding, as we show, strong language-independent features.  With this insight, we propose an NER model that can be applied to all languages in Wikipedia.                        When trained on English, our model outperforms comparable approaches on the standard CoNLL datasets (Spanish, German, and Dutch) and also performs very well on low-resource languages (e.g., Turkish, Tagalog, Yoruba, Bengali, and Tamil) that have significantly smaller Wikipedia. Moreover, our method allows us to train on multiple source languages, typically  improving NER results on the target languages. Finally, we show that our language-independent features can be used also to enhance monolingual NER systems, yielding improved results for all 9 languages.",Cross-Lingual Named Entity Recognition via Wikification,166,"This paper is concerned with cross-lingual direct transfer of NER models using
a very recent cross-lingual wikification model. In general, the key idea is not
highly innovative and creative, as it does not really propose any core new
technology. The contribution is mostly incremental, and marries the two
research paths: (1) direct transfer for downstream NLP tasks (such as NER,
parsing, or POS tagging), and (2) very recent developments in the cross-lingual
wikification technology. However, I pretty much liked the paper, as it is built
on a coherent and clear story with enough experiments and empirical evidence to
support its claims, with convincing results. I still have several comments
concerning the presentation of the work.

Related work: a more detailed description in related work on how this paper
relates to work of Kazama and Torisawa (2007) is needed. It is also required to
state a clear difference with other related NER system that in one way or
another relied on the encyclopaedic Wikipedia knowledge. The differences are
indeed given in the text, but they have to be further stressed to facilitate
reading and placing the work in context. 

Although the authors argue why they decided to leave out POS tags as features,
it would still be interesting to report experiments with POS tags features
similar to Tackstrom et al.: the reader might get an overview supported by
empirical evidence regarding the usefulness (or its lack) of such features for
different languages (i.e., for the languages for which universal POS are
available at least). 

Section 3.3 could contribute from a running example, as I am still not exactly
sure how the edited model from Tsai and Roth works now (i.e., the given
description is not entirely clear).

Since the authors mention several times that the approaches from Tackstrom et
al. (2012) and Nothman et al. (2012) are orthogonal to theirs and that they can
be combined with the proposed approach, it would be beneficial if they simply
reported some preliminary results on a selection of languages using the
combination of the models. It will add more flavour to the discussion. Along
the same line, although I do acknowledge that this is also orthogonal approach,
why not comparing with a strong projection baseline, again to put the results
into more even more context, and show the usefulness (or limitations) of
wikification-based approaches.

Why is Dutch the best training language for Spanish, and Spanish the best
language for Yoruba? Only a statistical coincidence or something more
interesting is going on there? A paragraph or two discussing these results in
more depth would be quite interesting.

Although the idea is sound, the results from Table 4 are not that convincing
with only small improvements detected (and not in all scenarios). A statistical
significance test reported for the results from Table 4 could help support the
claims.

Minor comments:

- Sect. 2.1: Projection can also be performed via methods that do not require
parallel data, which makes such models more widely applicable (even for
languages that do not have any parallel resources): e.g., see the work of
Peirsman and Pado (NAACL 2009) or Vulic and Moens (EMNLP 2013) which exploit
bilingual semantic spaces instead of direct alignment links to perform the
transfer.

- Several typos detected in the text, so the paper should gain quite a bit from
a more careful proofreading (e.g., first sentence of Section 3: ""as a the base
model""; This sentence is not 'parsable', Page 3: ""They avoid the traditional
pipeline of NER then EL by..."", ""to disambiguate every n-grams"" on Page 8)",,4,4,Poster,4,4,4,4,4,5,3,3,2016,"Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language.
There is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011).
Wang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data.Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Song and Roth, 2014), to generating parallel data (Smith et
3
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
al., 2010), to use in open information extraction (Wu and Weld, 2010). It has also been used to extract training data for NER, under the intuition that Wikipedia is already (partially) annotated with NER labels, in the form of links to pages. Nothman et al. (2012) generate silver-standard NER data from Wikipedia using link targets, and other heuristics. This can be gathered for any language in Wikipedia, but several of the heuristics depend on language-specific rules. Al-Rfou et al. (2015) generate training data from Wikipedia articles using a similar manner. The polyglot word embeddings (Al-Rfou et al., 2013) are used as features in their NER model. Although the features are delexicalized, the embeddings are unique to each language, and so the model cannot transfer.
Kim et al. (2012) use Wikipedia to generate parallel sentences with NE annotations. They propose a semi-CRF model for aligning entities in parallel sentences. Results are very strong on Wikipedia data. This is a hybrid approach in that it is supervised projection using Wikipedia.
Our work is most closely related to Kazama and Torisawa (2007). They do NER using Wikipedia category features for each mention. However, their method for wikifying text is not robust to ambiguity, and they only do monolingual NER.
Sil and Yates (2013) create a joint model for NER and entity linking (EL) in English. They avoid the traditional pipeline of NER then EL by overgenerating mentions in the first stage and using NER features to rank candidates. While the results are promising, the model is not scalable to other languages because it requires a trained NER system and NP chunker.The idea of direct transfer is to train a model in a high-resource setting using delexicalized features, that is, features that do not depend on word forms, and to directly apply it to text in a new language.
Täckström et al. (2012) experiments with direct transfer of dependency parsing and NER, and shows that using word cluster features can help, especially if the clusters are forced to conform across languages. The cross-lingual word clusters are induced using large parallel corpora.
Building on this work, Täckström (2012) focuses solely on NER, and includes experiments on self-training, and multi-source transfer for NER. Their experiments are orthogonal to ours, and
could be combined nicely. This work is closest to ours in terms of method, and so we compare against it in our experiments.
Our work falls under the umbrella of direct transfer methods combined with use of Wikipedia. We introduce wikifier features, which are truly delexicalized, and use Wikipedia as a source of information for each language.We use the state of the art English NER model from Ratinov and Roth (2009) as a the base model. This model approaches NER as a multiclass classification problem with greedy decoding, using the BIO labeling scheme. The underlying classifier is averaged perceptron.
Table 1 summarizes the features used in our model. These can be divided into a base set of standard features which are included in Ratinov and Roth (2009), a set of gazetteer features which are based on titles in multilingual Wikipedia, and our novel cross-lingual wikifier features. The base set of features can be further divided into nonlexical and lexical categories.Non-Lexical Features Ratinov and Roth (2009) uses a small number of non-lexical features. For example, the previous tag feature is useful in pre-
4
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
dicting I- tags, because the previous tag should never be an O. The tag context feature looks in a 1000 word history and gathers statistics over tags assigned to words [wi, wi+1, wi+2]. These features are included in all experiments.
In contrast with (Täckström et al., 2012), we do not use POS tags as features. We could not get the universal POS tags for all languages in our experiments, and an earlier experiment indicated that adding POS tags do not improve the performance due to the accuracy of tagger. Lexical Features Lexical features are very important for monolingual NER. In the direct transfer setting, lexical features are useful if the target language is close to the training language. We use a small number of simple features, including word forms, affixes, capitalization, and tag patterns. The latter feature looks at a small window of text (at most 2 tokens) before the word in question. If there is a named entity in the window, it makes a feature out of NETag+wi−2 + wi−1. Word type features simply indicate whether the word in question is all capitalized, is all digits, or is all letters.One of the larger performance improvements in Ratinov and Roth (2009) came from the use of gazetteers. We include gazetteers also in our model, except we gather them in each language from Wikipedia. As in Ratinov and Roth (2009), we use the gazetteers as features in the model. Specifically, we group gazetteers by topic, and use the name of the gazetteer file as the feature.
The method is to iteratively extend a short window to the right of the word in question. As the window increases in size, we search all gazetteers for occurrences of the phrase in the window. If we find a match, we add a feature to each word in the phrase according to its position in the phrase, either B for beginning, I for inside, or L for last. If the phrase is a single word, it is given a U feature.
This method generalizes gazetteers to unseen entities. For example, given the phrase “Bill and Melinda Gates Foundation”, “Bill” is marked as both B-PersonNames and B-Organizations, while “Foundation” is marked as L-Organizations. Imagine encountering at test time a fictional organization called “Dave and Sue Harris Foundation.” Although there is no gazetteer that contains this name, we have learned that “B-PersonName and B-PersonName B-PersonName Foundation” is a
strong signal for an organization.As shown in Figure 1, disambiguating words to Wikipedia entries allows us to obtain useful information for NER from the corresponding FreeBase types and Wikipedia categories. A crosslingual wikifier grounds words and phrases of non-English languages to the English Wikipedia, which provides language-independent features for transferring an NER model directly.
We use the system proposed in Tsai and Roth (2016), which grounds input strings to the intersection of the English Wikipedia and the target language Wikipedia. The only requirement of this system is the multilingual Wikipedia dump and it can be applied on all languages in Wikipedia.
Since we want to ground every n-gram (n ≤ 4) in the document, deviating from the normal usage that only considers few mentions of interest, we modify the system in the following two ways:
• The original candidate generation process queries the index by both whole input string and the individual tokens of the string. For the n-grams where n > 1, we generate title candidates only according to the whole string, not individual tokens. For instance, if it is allowed to generate title candidates based on individual tokens, the bigram “in Germany” will be linked to the title Germany thus wrongly considered as a named entity.
• The original ranking model includes the embeddings of other mentions in the document as features. It is clear that if we know what other important entities exist in the document, they provide useful clues to disambiguate a mention. However, if we want to wikify all n-grams, it makes no sense to include all of them as features, since the ranking model has already included features from TF-IDF weighted context words.
After wikifying every n-gram 1, we set the types of each n-gram as the coarse- and fine-grained FreeBase types and Wikipedia categories from the top 2 title candidates returned by wikifier. For each word wi, we use the types of wi, wi+1, and wi−1, and the types of the n-grams which contain wi as features. Moreover, we also include the
1We set n to 4 in all our experiments.
5
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
ranker features in wikifier from the top candidate as features. This could serve as a linker (Ratinov et al., 2011), which rejects the top prediction if it has a low confidence.In this section, we conduct experiments to validate and analyze the proposed NER model. First, we show that adding wikifier features improves results on monolingual NER. Second, we show that wikifier features are strong signals in direct transfer of a trained NER model across languages. Finally, we explore the importance of Wikipedia size to the quality of wikifier features and study using multiple source languages.We use data from CoNLL2002/2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The 4 languages represented are English, German, Spanish, and Dutch, each annotated using the IOB1 labeling scheme, which we convert to the BIO labeling scheme. All training is on the train set, and testing is on the test set. The evaluation metric for all experiments is phrase level F1, as explained in (Tjong Kim Sang, 2002).
In order to experiment on a broader range of languages, we also use data from the REFLEX (Simpson et al., 2008) and LORELEI projects. From LORELEI, we use Turkish,2 From REFLEX, we use Bengali, Tagalog, Tamil, and Yoruba.3 While Turkish, Tagalog, and Yoruba each have a few non-Latin characters, Bengali and Tamil are with an entirely non-Latin script. This is a major reason for inclusion in our experiments. We use the same set of test documents as used in Zhang et al. (2016). All other documents in the REFLEX and LORELEI packages are used as the training documents in our monolingual experiments. We refer to these five languages collectively as low-resource languages.
Besides PER, LOC, and ORG, some lowresource languages contain TIME tags and TTL tags, which represented titles in text, such as Secretary, President, or Minister. Since such words are not tagged in CoNLL training data, we opted to simply remove these tags. On the other hand, there is no MISC tag in the low-resource languages. Instead, many MISC-tagged entities in the
2LDC2014E115 3LDC2015E13,LDC2015E90,LDC2015E83,LDC2015E91
CoNLL datasets have LOC tags in the REFLEX and LORELEI packages, e.g., Italian and Chinese. We modify a MISC-tagged word to LOC tag if it is grounded to an entity with location as a FreeBase type, and remove all the other MISC tags in the training data. This process of changing MISC tags is only done when we train on CoNLL documents and test on low-resource languages.
The only requirement to build the cross-lingual wikifier model is a multilingual Wikipedia dump, and it can be trivially applied to all languages in Wikipedia. The top section of Table 2 lists Wikipedia sizes in terms of articles,4 the number of titles linked to English titles, and the number of training and test mentions for each language.
Besides the English gazetteers used in Ratinov and Roth (2009), we collect gazetteers for each language using Wikipedia titles. A Wikipedia title is included in the list for person names if it contains FreeBase type person. Similarly, we also create a location list and an organization list for each language. The total number of names in the gazetteers of each language is listed in Table 2.We begin by showing that wikifier features help when we train and test on the same language. The middle section of Table 2 shows the results.
In the ‘Wikifier only’ row, we use only wikifier features and previous tags features. This is intended to show the predictive power of wikifier features alone. Without using any lexical features, it gets good scores on the languages that have a large Wikipedia. These numbers represent the quality of the cross-lingual wikifier in that language, which in turn is correlated with the size of Wikipedia and size of the intersection with English Wikipedia.
The next row, ‘Base features’, shows that lexical features are always better than wikifier features only. This squares with the common wisdom that lexical features are important for NER.
Adding gazetteers to the base features improves more than 3 points for every language except Bengali and Tamil. Since these two languages use entirely non-Latin scripts, the words will not match any names in the English gazetteers, which have higher coverage than other languages’ gazetteers.
Finally, the ‘+Wikifier’ row shows that our pro-
4From https://en.wikipedia.org/wiki/ List_of_Wikipedias, retrieved March 2016
6
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
Latin Script Non-Latin Script
APPROACH EN NL DE ES TR TL YO BN TA AVG
Wiki size 5.1M 1.9M 1.9M 1.3M 269K 64K 31K 42K 85K - En. intersection - 755K 964K 757K 169K 49K 30K 34K 51K - Gazetteer size 8.5M 579K 1M 943K 168K 54K 20K 29K 10K - Entities (train) 23.5K 18.8K 11.9K 13.3K 5.1K 4.6K 4.1K 8.8K 7.0K - Entities (test) 5.6K 3.6K 3.7K 3.9K 2.2K 3.4K 3.4K 3.5K 4.6K -
Monolingual Experiments
Wikifier only 72.90 57.38 51.95 59.33 52.63 51.41 33.63 45.96 37.83 51.45 Base Features 85.24 77.34 65.11 79.98 65.21 74.34 54.78 69.11 55.30 69.60 +Gazetteers 88.92 82.69 68.74 83.51 70.67 77.53 57.89 69.50 56.76 72.91 +Wikifier 89.47 84.90 72.97 84.25 73.50 78.18 59.27 70.62 60.00 74.80
Direct Transfer Experiments
Wikifier only 36.66 38.55 40.04 43.09 36.97 25.09 41.81 27.85 36.26 Base Features 44.10 25.24 41.81 30.50 50.45 32.48 2.30 1.74 28.58 +Gazetteers 49.66 35.06 55.04 30.90 64.07 34.42 3.14 0.30 34.07 +Wikifier 62.10 47.14 60.97 48.41 65.32 36.79 6.72 2.99 41.31
Täckström baseline 48.4 23.5 45.6 - - - - - - Täckström bitext clusters 58.4 40.4 59.3 - - - - - - Zhang et al. (2016) - - - 43.6 51.3 36.0 34.8 26.0 38.3
Table 2: Data sizes, monolingual experiments, and direct transfer experiments. Wiki size is the number of articles in Wikipedia. For monolingual experiments, we train the proposed model on the training data of the target languages. ‘Wikifier only’ uses the previous tags features also. For direct transfer experiments, all models are trained on CoNLL English training set. The rows marked Täckström come from (Täckström et al., 2012), and are the baseline and clustering result. The plus signs (+) signify cumulative addition. EN: English, NL: Dutch, DE: German, ES: Spanish, TR: Turkish, TL: Tagalog, YO: Yoruba, BN: Bengali, TA: Tamil.
posed features are valuable even in combination with strong features. It improves upon base features and gazetteer features for all 9 languages. These numbers may be less than state of the art because the features we use are designed for English, and may not capture lexical subtleties in every language. Nevertheless, they show that wikifier features have a non-trivial signal that has not been captured by other features.We evaluate our direct transfer experiments by training on English and testing on the target language. The results from these experiments are shown in the bottom section of Table 2.
The ‘Wikifier only’ row shows that the wikifier features alone preserve a signal across languages. Interestingly, for both Bengali and Tamil, this is the strongest signal, and gets the highest score. If the lexical features are included when we train the English model, the learning algorithm will give them too much emphasis, thus decreasing the importance of the wikifier features. Since Bengali and Tamil use non-Latin scripts, no lexical feature
in English will fire at test time. Thus, approaches that include base features perform poorly.
The results of ‘Base features’ can be viewed as a sort of language similarity to English, which, in this case, is related to lexical overlap and similarity between the scripts. Comparing to monolingual experiments, we can see that the lexical features become weak in the cross-lingual setting.
The gazetteer features are again shown to be very useful for almost all languages except Bengali and Tamil due to the reason explained in the monolingual experiment and to the inclusion of lexical features. For the rest of languages, the improvement from adding gazetteers is even more than the improvement in the monolingual setting.
For nearly every language, wikifier features help dramatically, which indicates that they are very good delexicalized features. It adds more than 10 points on Dutch, German, and Turkish.
The trend in Table 2 suggests the following strategy when we want to extract named entities in a new foreign language: It is better to include all features if the foreign language uses Latin script, since the names are likely mentioned using the
7
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
same way as in English. Otherwise, using wikifier features could be the best setting.
Täckström et al. (2012) also directly transfer an English NER model using the same setting as ours: train on the CoNLL English training set and predict on the test set of other three languages. We compare our baseline transfer model (Base Features) to the row denoted by “Täckström baseline”. Even though we do not use gold POS tags, we see that our results are comparable. The second Täckström row uses parallel text to induce multilingual word clustering. While this approach is orthogonal to ours, and could be used in tandem to get even better scores, we compare against it for lack of a more closely aligned scenario. We see that for each language, we significantly outperform their approach.
We note that our numbers are comparable to those reported for WIKI-2 in Nothman et al. (2012) for the CoNLL languages (with the exception of German, where theirs is higher). Their system requires language-specific heuristics to generate their silver-standard training data from Wikipedia articles. What they gain for single languages, they likely lose in generalizability. This approach is orthogonal to ours and we can also use their silverstandard data in training.
For the low-resource languages, we compare our direct transfer model with the expectation learning model proposed in Zhang et al. (2016). This model is not a direct transfer model, but it does not use any training data in the target languages either. Instead, for each target language, it generates patterns from parallel documents between English and the target language, a large monolingual corpus in the target language, and one-hour interaction with a native speaker of the target language. Note that they also use a crosslingual wikifier, but only for refining the entity types. On the other hand, in our model, the features from the wikifier are used in both detecting entity mention boundaries and the entity types. We can see that our approach performs better than their model on all five languages even though we assume much fewer resources. The difference is most significant on Turkish, Tagalog, and Bengali.One immediate question is why wikifier features are less helpful on the low-resource languages results than on the CoNLL languages? In this exper-
FEATURES SPANISH GERMAN #inter. F1 #inter. F1
Wikifier only 757K 40.04 964K 38.55 W.−FB query 757K 34.69 964K 28.27 W.−FB−50% inter. 379K 30.32 482K 27.24 W.−FB−90% inter. 76K 29.44 96K 25.94
Table 3: The F1 scores of using only wikifier features with removing the support from FreeBase and varying the number of titles linked to the English Wikipedia. ‘W.−FB query’ removes the component of querying FreeBase by the target language title from ‘Wikifier only’. ‘−X% inter.’ indicates removing X% of the interlanguage links with English titles. The column #inter. shows the number of titles that intersect with English.
iment, we show that smaller Wikipedia sizes result in worse Wikipedia features, which is the reason Yoruba has bad ‘Wikifier only’ results and the improvement from the wikifier features is much smaller.
The cross-lingual wikifier that we use in our system only grounds words to the intersection of the English and target language Wikipedia. Given a Wikipedia title in the target language, we first retrieve FreeBase ID by querying FreeBase API. If it fails, we find the corresponding English Wikipedia title via interlanguage links and then query the API with the English title. However, FreeBase does not contain entities in Yoruba, Bengali, and Tamil, so the first step will always fail for these three languages. We remove this step in the experiments of high-resource languages and the results are shown in the row ‘W.−FB query ’ of Table 3. We can see that the performance drops a lot, because many words have no features from FreeBase types.
Next, we randomly remove 50% and 90% of the interlanguage links to English titles. This will not only reduce the number of fired features from Wikipedia categories, but also FreeBase types since English titles are used to query FreeBase IDs. When 90% of interlanguage links are removed, the scores of Spanish and German are closer to Yoruba’s score (27.85).In all previous experiments, the training language is always English. In order to test the efficacy of training with languages other than English, we create a train/test matrix with all combinations of languages, as seen in Figure 2.
8
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
Figure 2: Different training/test language pairs. Scores shown are the F1 scores. The red boxes signify the best non-target training languages.
The vertical axis represents training language, and the horizontal axis represents test language. A darker color signifies a higher score. For example, if we train on Spanish (es) and test on Yoruba (yo), we get an F1 of 39.4. When the test language is Bengali (bn) or Tamil (ta), we only use wikifier features. For other test languages, all features are included. Note that we ignore all MISC tags in the CoNLL languages (en, nl, de, es) in evaluation, since there is no MISC tag in the low-resource languages. The diagonals represent the monolingual setting in which we use all features for all languages. Since we are interested in transferring a model, we ignore the diagonals, and identify the best training language for a given test language as the largest off-diagonal in each column. These are demarcated with red boxes.
English is the best for most languages, with the exception of Dutch, best for Spanish, and Spanish, best for Yoruba. It makes sense that highresource languages are better training languages because 1) there are more annotated training instances, 2) larger Wikipedia creates denser wikifier features, therefore providing better estimation of the weights to these features.
Table 4 shows the results of training on multiple languages. We use all features in this experiment. The row “EN” only trains the model on the English training documents, and the results are identical to those shown in Table 2. Adding
Spanish training data yields the best score on Yoruba, which agrees with Figure 2 where Spanish is the best training language for Yoruba. Using all CoNLL languages (EN+ES+NL+DE) adds more than 1 point F1 in average comparing to using English only. Finally, training on all but the test languages further improves the results.
This experiment shows that we can augment training data from other languages’ annotated documents. Although the performance only increases a little, it does not hurt most of the time.",,"Named Entity Recognition (NER) is the task of identifying and typing phrases that contain the names of persons, organizations, locations, and so on. It is an information extraction task that is important for understanding large bodies of text and
is considered an essential pre-processing stage in Natural Language Processing (NLP) and Information Retrieval systems.
NER is successful for languages which have a large amount of annotated data, but for languages with little to no annotated data, this task becomes very challenging. There are two common approaches to address the lack of training data problem. The first approach is to automatically generate annotated training data in the target language from Wikipedia articles or from parallel corpora. The performance of this method depends on the quality of the generated data and how well the language-specific features are explored. The second approach is to train a model on another language which has abundant training data, and then apply the model directly on test documents in the target language. This direct transfer technique relies on developing language-independent features. Note that these two approaches are orthogonal and can be used together.
In this paper, we focus on the direct transfer setting. We propose a cross-lingual NER model which is trained on annotated documents in one or multiple source languages, and can be applied to all languages in Wikipedia. The model depends on a cross-lingual wikifier, which only requires multilingual Wikipedia, no sentence-aligned or wordaligned parallel text is needed.
Recently, much attention has been given to cross-lingual wikification and entity linking research (Ji et al., 2015; Ji et al., 2016; Moro et al., 2014; Tsai and Roth, 2016). The key contribution of the current paper is the development of a method that makes use of cross-lingual wikification to generate language-independent features for NER, and showing how useful this can be to training NER models with no annotation in the target language. Given a mention (sub-string) from a document written in a foreign language, the goal
2
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
Schwierigkeiten beim nachvollziehenden Verstehen Albrecht Lehmann läßt Flüchtlinge und Vertriebene in Westdeutschland
Problem_solving Understanding Albert,_Duke_of_Prussia Jens_Lehmann Refugee Western_Germany
hobby media_genre media_common quotation_subject
person noble_person
person athlete
field_of_study literature_subject
location country
Wikipedia titles: FreeBase types:
NER Tags: Person Location Sentence:
Figure 1: An example of German sentence. We ground each word to the English Wikipedia by a crosslingual wikifier. A word is not linked if it is a stop word or the wikifier returns NIL. We can see that the FreeBase types are strong signals to NER even with imperfect disambiguation.
of cross-lingual wikification is to find the corresponding title in the English Wikipedia. Traditionally, wikification has been considered a downstream task of NER. That is, a named entity recognizer is firstly applied to identify the mentions of interest, and then the wikifier grounds the extracted mentions to Wikipedia entries. In contrast to this traditional pipeline, we show that the ability to disambiguate words is very useful in identifying named entities. By grounding every n-gram to the English Wikipedia, we can obtain useful clues regardless of the language used in testing documents.
Figure 1 shows an example of a German sentence. We use a cross-lingual wikifier to ground each word to the English Wikipedia. We can see that even though disambiguation is not perfect, the FreeBase types still provide valuable information. That is, although “Albrecht Lehmann” is not an entry in Wikipedia, the wikifier still links “Albrecht” and “Lehmann” to people. Since words in any language are grounded to the English Wikipedia, the corresponding Wikipedia categories and Freebase types can be used as language-independent features.
The proposed model significantly outperforms comparable direct transfer methods on Spanish, Dutch, and German CoNLL data. We also evaluate the model on five low-resource languages: Turkish, Tagalog, Yoruba, Bengali, and Tamil. Due to small sizes of Wikipedia, the overall performance is not as good as the CoNLL experiments. Nevertheless, the wikifier features still give significant improvement, and the proposed direct transfer model outperforms the state of the art, which assumes parallel text and some interaction with a native speaker of the target language. In addition, we show that the proposed languageindependent features not only perform well on the direct transfer scenario, but also improve monolingual models, which are trained on the target language. Another advantage of the proposed direct
transfer model is that we can train on documents from multiple languages together, and further improve the results.","There are three main branches of work for extending NLP systems to many languages: projection across parallel data, Wikipedia-based approaches, and direct transfer. Projection and direct transfer take advantage of the success of NLP tools on high-resource languages. Wikipediabased approaches exploit the fact that by editing Wikipedia, thousands of people have made annotations in hundreds of languages.",,"We propose a language-independent model for cross-lingual NER using a cross-lingual wikifier to disambiguate every n-grams. This model works on all languages in Wikipedia and the only requirement is a Wikipedia dump. We study a wide range of languages in both monolingual and crosslingual settings, and show significant improvements over strong baselines. An analysis shows that the quality of wikifier features depends on the Wikipedia size of the test language.
This work shows that if we can disambiguate words and phrases to the English Wikipedia, the typing information from Wikipedia categories and FreeBase are useful language-independent features for NER. However, there is other information from Wikipedia which we do not use, such as words from documents and relations between titles, which would require additional research.
In the future, we would like to experiment with including other techniques for multilingual NER that we discuss in Section 2 into our model, such as parallel projection and generating training data from Wikipedia automatically.
9
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935"
33,"We review the task of Sentence Pair Scor- ing, popular in the literature in various forms â viewed as Answer Sentence Se- lection, Semantic Text Scoring, Next Ut- terance Ranking, Recognizing Textual En- tailment, Paraphrasing or e.g. a component of Memory Networks.  We argue that all such tasks are similar from the model perspective and propose new baselines by comparing the perfor- mance of common IR metrics and popu- lar convolutional, recurrent and attention- based neural models across many Sen- tence Pair Scoring tasks and datasets. We discuss the problem of evaluating ran- domized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored bench- marks. We introduce a unified open source software framework with easily pluggable models and tasks, which enables us to experiment with multi-task reusability of trained sentence models.",Sentence Pair Scoring: Towards Unified Framework for Text Comprehension,176,"This paper proposes the new (to my knowledge) step of proposing to treat a
number of sentence pair scoring tasks (e.g. Answer Set Scoring, RTE,
Paraphrasing,
among others) as instances of a more general task of understanding semantic
relations
between two sentences. Furthermore, they investigate the potential of learning
generally-
applicable neural network models for the family of tasks. I find this to be an
exciting
proposal that's worthy of both presentation at CoNLL and further discussion and
investigation.

The main problem I have with the paper is that it in fact feels unfinished. It
should be
accepted for publication only with the proviso that a number of updates will be
made
for the final version:
1 - the first results table needs to be completed
2 - given the large number of individual results, the written discussion of
results
is terribly short. Much more interpretation and discussion of the results is
sorely needed.
3 - the abstract promises presentation of a new, more challenging dataset which
the paper
does not seem to deliver. This incongruity needs to be resolved.
4 - the results vary quite a bit across different tasks - could some
investigation be made into
how and why the models fail for some of the tasks, and how and why they succeed
for others?
Even if no solid answer is found, it would be interesting to hear the authors'
position regarding
whether this is a question of modeling or rather dissimilarity between the
tasks. Does it really
work to group them into a unified whole?
5 - please include example instances of the various datasets used, including
both prototypical
sentence pairs and pairs which pose problems for classification
6 - the Ubu. RNN transfer learning model is recommended for new tasks, but is
this because
of the nature of the data (is it a more general task) or rather the size of the
dataset? How can
we determine an answer to that question?

Despite the unpolished nature of the paper, though, it's an exciting approach
that
could generate much interesting discussion, and I'd be happy to see it
published
IN A MORE FINISHED FORM.
I do recognize that this view may not be shared by other reviewers!

Some minor points about language:
* ""weigh"" and ""weighed"" are consistently used in contexts that rather require
""weight"" and
""weighted""
* there are several misspellings of ""sentence"" (as ""sentene"")
* what is ""interpunction""?
* one instance of ""world overlap"" instead of ""word overlap""",,4,4,Poster,4,4,3,2,4,5,4,4,2016,"The tasks we are aware of that can be phrased as f2-type problems are listed below. In general, we primarily focus on tasks that have reasonably large and realistically complex datasets freely available. On the contrary, we have explicitly avoided datasets that have licence restrictions on availability or commercial usage.Given a factoid question and a set of candidate answer-bearing sentences in encyclopedic style, the first task is to rank higher sentences that are more likely to contain the answer to the question. As it is fundamentally an Information Retrival task in nature, the model performance is commonly evaluated in terms of Mean Average Precision (MAP) and Mean Reciprocial Rank (MRR).
This task is popular in the NLP research community thanks to the dataset introduced in (Wang et al., 2007) (which we refer to as wang), with six papers published between February 2015 and 2016 alone and neural models substantially improving over classical approaches based primarily on parse tree edits.1 It is possibly the main research testbed for f2-style task models. This task has also immediate applications e.g. in Question Answering systems.
In the context of practical applications, the sofar standard wang dataset has several downsides we observed when tuning and evaluating our models, illustrated numerically in Fig. 1 — the set of candidate sentences is often very small and quite uneven (which also makes rank-based measures
1http://aclweb.org/aclwiki/index.php? title=Question_Answering_(State_of_the_ art)
unstable) and the total number of individual sentence pairs as well as questions is relatively small. Furthermore, the validation and test set are very small, which makes for noisy performance measurements; the splits also seem quite different in the nature of questions since we see minimum correlation between performance on the validation and test sets, which calls the parameter tuning procedures and epoch selection for early stopping into question. Alternative datasets WikiQA (Yang et al., 2015) and InsuranceQA (Tan et al., 2015) were proposed, but are encumbered by licence restrictions. Furthermore, we speculate that they may suffer from many of the problems above2 (even if they are somewhat larger). To alleviate the problems listed above, we are introducing a new dataset yodaqa/large2470 based on an extension of the curatedv2 question dataset (introduced in (Baudiš and Šedivý, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudiš, 2015) from English Wikipedia and labelled by matching the gold standard answers in the passages.3
Motivated by another problem related to the YodaQA system, we also introduce another dataset wqmprop, where s0 are again question sentences, but s1 are English labels of properties that make a path within the Freebase knowledge base that connects an entity linked in the question to the correct answer. This task (Property Selection) can be evaluated identically to the previous task, and solutions often involving Convolutional Neural Networks have been studied in the Question Answering literature (Yih et al., 2015) (Xu et al., 2016). Our sentences have been derived from the WebQuestions dataset (Berant et al., 2013) extended with the moviesE dataset questions (originally introduced in (Baudiš and Šedivý, 2015)); the property paths are based on the Freebase knowledge graph dump, generated based on entity linking and exploration procedure of YodaQA v1.5.4
Fig. 1 compares the critical characteristics of
2Moreover, InsuranceQA is effectively a classification task rather than a ranking task, which we do not find as appealing in the context of practical applications.
3Note that the wang and yodaqa datasets however share a common ancestry regarding the set of questions and there may be some overlaps, even across train and test splits. Therefore, mixing training and evaluation on wang and yodaqa datasets within a single model instance is not advisable.
4https://github.com/brmson/ dataset-factoid-webquestions branch movies
the datasets. Furthermore, as apparent below, the baseline performances on the newly proposed datasets are much lower, which suggests that future model improvements will be more apparent in evaluation.(Lowe et al., 2015) proposed the new large-scale real-world Ubuntu Dialogue dataset for an f2-style task of ranking candidates for the next utterance in a chat dialog, given the dialog context. The technical formulation of the task is the same as for Answer Sentence Selection, but semantically, choosing the best followup has different concerns than choosing an answer-bearing sentence. Recall at top-ranked 1, 2 or 5 utterances out of either 2 or 10 candidates is reported; we also propose reporting the utterance MRR as a more aggregate measure. The newly proposed Ubuntu Dialogue dataset is based on IRC chat logs of the Ubuntu community technical support channels and contains casually typed interactions regarding computer-related problems.5 While the training set consists of individual labelled pairs, during evaluation 10 followups to given message(s) are ranked. The sequences might be over 200 tokens long.
Our primary motivation for using this dataset is its size. The numerical characteristics of this dataset are shown in Table 1.6 We use the v2 version of the dataset.7 Research published on this dataset so far relies on simple neural models. (Lowe et al., 2015) (Kadlec et al., 2015)One of the classic tasks at the boundary of Natural Language Processing and Artificial Intelligence is the inference problem of Recognizing Textual Entailment (Dagan et al., 2006) — given a pair of a factual sentence and a hypothesis sentence, we are to determine whether the hypothesis represents a contradiction, entailment or is neutral (cannot be proven or disproven).
We include two current popular machine learning datasets for this task. The Stanford Natural Language Inference SNLI dataset (Bowman et al.,
5In a manner, they resemble tweet data, but without the length restriction and with heavily technical jargon, interspersed command sequences etc.
6As in past papers, we use only the first 1M pairs (10%) of the training set.
7https://github.com/rkadlec/ ubuntu-ranking-dataset-creator
2015) consists of 570k English sentence pairs with the facts based on image captions, and 10k + 10k of the pairs held out as validation and test sets. The SICK-2014 dataset (Marelli et al., 2014) was introduced as Task 1 of the SemEval 2014 conference and in contrast to SNLI, it is geared at specifically benchmarking semantic compositional methods, aiming to capture only similarities on purely language and common knowledge level, without relying on domain knowledge, and there are no named entities or multi-word idioms; it consists of 4500 training pairs, 500 validation pairs and 4927 testing pairs. For the SICK-2014 dataset, we also report results on the Semantic Textual Similarity. This task originates in the STS track of the SemEval conferences (Agirre et al., 2015) and involves scoring pairs of sentences from 0 to 5 with the objective of maximizing correlation (Pearson’s r) with manually annotated gold standard.As our goal is a universal text comprehension model, we focus on neural network models architecture-wise. We assume that the sequence is transformed usingN -dimensional word embeddings on input, and employ models that produce a pair of sentence embeddings E0, E1 from the sequences of word embeddings e0, e1. Unless noted otherwise, a Siamese architecture is used that shares weights among both sentenes. A scorer module that compares the E0, E1 sentence embeddings to produce a scalar result is connected to the model; for specific task-model configurations, we use either the dot-product module E0 · ET1 (representing non-normalized vector angle, as in e.g. (Yu et al., 2014) or (Weston et al., 2014)) or theMLPmodule that takes elementwise product and sum of the embeddings and feeds them to a two-layer perceptron with hidden layer of width 2N (as in e.g. (Tai et al., 2015)).8 For the STS task, we follow this by score regression using class interpolation as in (Tai et al., 2015). When training for a ranking task (Answer Sentence Selection), we use the bipartite ranking version of Ranknet (Burges et al., 2005) as the objective; when training for STS task, we use Pearson’s r formula as the objective; for binary classification
8The motivation is to capture both angle and euclid distance in multiple weighed sums. Past literature uses absolute difference rather than sum, but both performed equally in our experiments and we adopted sum for technical reasons.
tasks, we use the binary crossentropy objective.In order to anchor the reported performance, we report several basic methods. Weighed word overlaps metrics TF-IDF and BM25 (Robertson et al., 1995) are inspired by IR research and provide strong baselines for many tasks. We treat s0 as the query and s1 as the document, counting the number of common words and weighing them appropriately. IDF is determined on the training set.
The avg metric represents the baseline method when using word embeddings that proved successful e.g. in (Yu et al., 2014) or (Weston et al., 2014), simply taking the mean vector of the word embedding sequence and training an U weight matrix N×2N that projects both embeddings to the same vector space, Ei = tanh(U · ēi), where the MLP scorer compares them. During training, p = 1/3 standard (elementwise) dropout is applied on the input embeddings.
A simple extension of the above are the DAN Deep Averaging Networks (Iyyer et al., 2015), which were shown to adequately replace much more complex models in some tasks. Two dense perceptron layers are stacked between the mean and projection, relu is used instead of tanh as the non-linearity, and word-level dropout is used instead of elementwise dropout.RNN with memory units are popular models for processing sentenes (Tan et al., 2015) (Lowe et al., 2015) (Bowman et al., 2015). We use a bidirectional network with 2N GRU memory units9 (Cho et al., 2014) in each direction; the final unit states are summed across the per-direction GRUs
9While the LSTM architecture is more popular, we have found the GRU results are equivalent while the number of parameters is reduced.
to yield a 2N vector representation of the sentence. Like in the avg baseline, a projection matrix is applied on this representation and final vectors compared by an MLP scorer. We have found that applying massive dropout p = 4/5 both on the input and output of the network helps to avoid overfitting even early in the training.CNN with sentence-wide pooling layer are also popular models for processing sentences (Yu et al., 2014) (Tan et al., 2015) (Severyn and Moschitti, 2015) (He et al., 2015) (Kadlec et al., 2015). We apply a multi-channel convolution (Kim, 2014) with single-token channel of N convolutions and 2, 3, 4 and 5-token channels of N/2 convolutions each, relu transfer function, max-pooling over the whole sentence, and as above a projection to shared space and an MLP scorer. Dropout is not applied.The RNN-CNN model aims to combine both recurrent and convolutional networks by using the memory unit states in each token as the new representation of the token which is then fed to the convolutional network. Inspired by (Tan et al., 2015), the aim of this model is to allow the RNN to model long-term dependencies and model contextual representations of words, while taking advantage of the CNN and pooling operation for crisp selection of the gist of the sentence. We use the same parameters as for the individual models, but with no dropout and reducing the number of parameters by using only N memory units per direction.The idea of attention models is to attend preferrentially to some parts of the sentence when building its representation (Hermann et al., 2015) (Tan et al., 2015) (dos Santos et al., 2016) (Rocktäschel
et al., 2015). There are many ways to model attention, we adopt the (Tan et al., 2015) model attn1511 as a conceptually simple and easy to implement baseline. It asymmetrically extends the RNN-CNN model by extra links from s0 CNN output to the post-recurrent representation of each s1 token, determining an attention level for each token by weighed sum of the token vector elements, focusing on the relevant s1 segment by transforming the attention levels using softmax and multiplying the token representations by the attention levels before they are fed to the convolutional network.
Convolutional network weights are not shared between the two sentences and the convolutional network output is not projected before applying the MLP scorer. The CNN used here is singlechannel with 2N convolution filters 3 tokens wide.
4 Model Performance
4.1 dataset-sts framework
To easily implement models, dataset loaders and task adapters in a modular fashion so that any model can be easily run on any f2-type task, we have created a new software package dataset-sts that integrates a variety of datasets, a Python dataset adapter PySTS and a Python library for easy construction of deep neural NLP models for semantic sentence pair scoring KeraSTS that uses the Keras machine learning library (Chollet, 2015). The framework is available for other researchers as open source on GitHub.10We use N = 300 dimensional GloVe embeddings matrix pretrained on Wikipedia 2014 + Gigaword 5 (Pennington et al., 2014) that we keep adaptable during training; words in the training set not included in the pretrained model are initialized by random vectors uniformly sampled from [−0.25,+0.25] to match the embedding standard deviation.
Word overlap is an important feature in many f2-type tasks (Yu et al., 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available. As a workaround, ensemble of world overlap count and neural model score is typically used to produce
10URL redacted but included as supplementary material
the final score. In line with this idea, in the Answer Sentence Selection wang and large2470 datasets, we use the BM25 overlap baseline as an additional input to the MLP scoring module, and prune the scored samples to top 20 based on BM25.11 Furthermore, we extend the embedding of each input token by several extra dimensions carrying boolean flags— bigram overlap, unigram overlap (except stopwords and interpunction), and whether the token starts with a capital letter or is a number. Particular hyperparameters are tuned primarily on the yodaqa/large2470 dataset unless noted otherwise in the respective results table caption. We apply 10−4 L2 regularization and use Adam optimization with standard parameters (Kingma and Ba, 2014). In the answer selection tasks, we train on 1/4 of the dataset in each epoch. After training, we use the epoch with best validation performance; sadly, we typically observe heavy overfitting as training progresses and rarely use a model from later than a couple of epochs.To confirm the hypothesis that our models learn a generic task akin to some form of text comprehension, we trained a model on the large Ubuntu Dialogue dataset (Next Utterance Ranking task) and transferred the weights and retrained the model instance on other tasks. We used the RNN model for the experiment in a configuration with dot-product scorer and smaller dimensionality (which works much better on the Ubuntu dataset). This configuration is shown in the respective result tables as Ubu. RNN and it consistently ranks as the best or among the best classifiers, dramatically outperforing the baseline RNN model.15
During our experiments, we have noticed that it is important not to apply dropout during re-
13Over larger number of samples, this estimate converges to the normal distribution confidence levels. Note that the confidence interval determines the range of the true expected evaluation, not evaluation of any measured sample.
14We have tried simple averaging of predictions (as per (Kadlec et al., 2015)), but the benefit was small and inconsistent.
15The RNN configuration used for the transfer, when trained only on the target task, is not shown in the tables but has always been worse than the baseline RNN configuration.
training if it wasn’t applied during the source model training, to balance the dataset labels, and we used the RMSprop training procedure since Adam’s learning rate annealing schedule might not be appropriate for weight re-training. We have also tried freezing the weights of some layers, but this never yielded a significant improvement.Due to the very wide scope of the f2-problem scope, we leave some popular tasks and datasets as future work. A popular instance of sentence pair scoring is the question answering task of the Memory Networks (supported by the baBi dataset) (Weston et al., 2015). A realistic large question Paraphrasing dataset based on the AskUbuntu Stack Overflow forum had been recently proposed (Lei et al., 2015).16 In a multi-lingual context, sentence-level MT Quality Estimation is a meta-task with several available datasets.17 While the tasks of Semantic Textual Similarity (supported by a dataset from the STS track of the SemEval conferences (Agirre et al., 2015)) and Paraphrasing (based on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) right now) are available within our framework, we do not report the results here as the models lag behind the state-of-art significantly and
16The task resembles paraphrasing, but is evaluated as an Information Retrieval task much closer to Answer Sentence Selection.
17http://www.statmt.org/wmt15/ quality-estimation-task.html
show little difference in results. Advancing the models to be competitive remains future work. A generalization of our proposed architecture could be applied to the Hypothesis Evidencing task of binary classification of a hypothesis sentence s0 based on a number of memory sentences s1, for example within the MCText (Richardson et al., 2013) dataset. We also did not include several major classes of models in our initial evaluation. Most notably, this includes serial RNNs with attention as used e.g. for the RTE task (Rocktäschel et al., 2015), and the skip-thoughts method of sentence embedding. (Kiros et al., 2015)
We believe that the Ubuntu Dialogue Dataset results demonstrate that the time is ripe to push the research models further towards the real-world by allowing for wider sentence variability and less explicit supervision. But in particular, we believe that new models should be developed and tested on tasks with long sentences and wide vocabulary. In terms of models, recent work in many NLP domains (dos Santos et al., 2016) (Cheng et al.,
2016) (Kumar et al., 2015) clearly points towards various forms of attention modelling to remove the bottleneck of having to compress the full spectrum of semantics into a single vector of fixed dimensionality. In this paper, we have shown the benefit of training a model on a single dataset and then applying it on another dataset. One open question is whether we could jointly train a model on multiple tasks simultaneously (even if they do not share some output layers). Another option would be to include extra supervision similar to the token overlap features that we already employ; for example, in the new Answer Sentence Selection task datasets, we can explicitly mark the actual tokens representing the answer.redacted","We report model performance averaged across 16 training runs (with different seeds). A consideration we must emphasize is that randomness plays a large role in neural models both in terms of randomized weight initialization and stochastic dropout. For example, the typical methodology for reporting results on the wang dataset is to evaluate and report a single test run after tuning on the dev set,12 but wang test MRR has empirical standard deviation of 0.025 across repeated runs of our attn1511 model, which is more than twice the gap between every two successive papers pushing the state-of-art on this dataset! See the ∗- marked sample in Fig. 2 for a practical example of this phenomenon. Furthermore, on more complex tasks (Answer Sentence Selection in particular, see Fig. 1) the validation set performance is not a great approximator for test set performance and a strategy like picking the training run with best validation performance would lead just to overfitting on the validation set. To allow comparison between models (and with 11This reduces the number of (massively irrelevant) training samples, but we observed no adverse effects of that, while it speeds up training greatly and models well a typical Information Retrieval scenario where fast pre-scoring of candidates is essential.
12Confirmed by personal communication with paper authors.
future models), we therefore report also 95% confidence intervals for each model performance estimate, as determined from the empirical standard deviation using Student’s t-distribution.13In Fig. 2 to 4, we show the cross-task performance of our models. We can observe an effect analogous to what has been described in (Kadlec et al., 2015)—when the dataset is smaller, CNNmodels are preferrable, while larger dataset allows RNN models to capture the text comprehension task better. IR baselines provide strong competition and finding new ways to ensemble them with models should prove beneficial in the future.14 This is especially apparent in the new Answer Sentence Selection datasets that have very large number of sentence candidates per question. The attention mechanism also has the highest impact in this kind of Information Retrieval task.
While our models clearly yet lag behind the state-of-art on the RTE and STS tasks, it establishes the new baseline on the Ubuntu Dialogue dataset and it is not possible to statistically determine its relation to state-of-art on the wang Answer Sentence Selection dataset.","AnNLPmachine learning task often involves classifying a sequence of tokens such as a sentence or a document, i.e. approximating a function f1(s) ∈ [0, 1] (where f1 may determine a domain, sentiment, etc.). But there is a large class of problems that involve classifying a pair of sentences, f2(s0, s1) ∈ R (where s0, s1 are sequences of tokens, typically sentences).
Typically, the function f2 represents some sort of semantic similarity, that is whether (or how much) the two sequences are semantically related.
This formulation allows f2 to be a measure for tasks as different as topic relatedness, paraphrasing, degree of entailment, a pointwise ranking task for answer-bearing sentences or next utterance classification. In this work, we adopt the working assumption that there exist certain universal f2 type measures that may be successfuly applied to a wide variety of semantic similarity tasks — in the case of neural network models trained to represent universal semantic comprehension of sentences and adapted to the given task by just fine-tuning or adapting the output neural layer (in terms of architecture or just weights). Our argument for preferring f2 to f1 in this pursuit is the fact that the other sentence in the pair is essentially a very complex label when training the sequence model, which can therefore discern semantically rich structures and dependencies. Determining and demonstrating such universal semantic comprehension models across multiple tasks remains a few steps ahead, since the research landscape is fragmented in this regard. Model research is typically reported within the context of just a single f2-type task, each dataset requires sometimes substantial engineering work before measurements are possible, and results are reported in ways that make meaningful model comparisons problematic. Our main aims are as follows. (A) Unify research within a single framework that employs task-independent models and task-specific adaptation modules. (B) Improve the methodology of model evaluation in terms of statistics, comparing with strong non-neural IR baselines, and introducing new datasets with better characteristics. (C) Demonstrate the feasibility of pursuing universal, task-independent f2 models, showing that even simple neural models learn universal semantic comprehension by employing cross-task transfer learning.
The paper is structured as follows. In Sec. 2, we outline possible specific f2 tasks and available datasets; in Sec. 3, we survey the popular nonneural and neural baselines in the context of these tasks; finally, in Sec. 4, we present model-task evaluations within a unified framework to establish the watermark for future research as well as gain insight into the suitability of models across a variety of tasks. In Sec. 5, we demonstrate that transfer learning across tasks is helpful to powerfully seed models. We conclude with Sec. 6, summarizing our findings and outlining several future research directions.",,,"We have unified a variety of tasks in a single scientific framework of sentence pair scoring, and demonstrated a platform for general modelling of this problem and aggregate benchmarking of these models across many datasets. Promising initial transfer learning results suggest that a quest for generic neural model capable of task-independent text comprehension is becoming a meaningful pursuit. The open source nature of our framework and the implementation choice of a popular and extensible deep learning library allows for high reusability of our research and easy extensions with further more advanced models. Based on our benchmarks, as a primary model for applications on new f2-type tasks, we can recommend either the RNN-CNN model or transfer learning based on the Ubu. RNN model."
34,"Coreference resolution for event mentions enables extraction systems to process document-level information. Current systems in this area base their decisions on rich semantic features from various knowledge bases, thus restricting them to domains where such external sources are available. We propose a model for this task which does not rely on such features but instead utilizes sentential features coming from convolutional neural networks. Two such networks first process coreference candidates and their respective context, thereby generating latent-feature representations which are tuned towards event aspects relevant for a linking decision. These representations are augmented with lexical-level and pairwise features, and serve as input to a trainable similarity function producing a coreference score. Our model achieves state-of-the-art performance on two datasets, one of which is publicly available. An error analysis points out directions for further research.",Event Linking with Sentential Features from Convolutional Neural Networks,13,"This paper models event linking using CNNs. Given event mentions, the authors
generate vector representations based on word embeddings passed through a CNN
and followed by max-pooling. They also concatenate the resulting
representations with several word embeddings around the mention. Together with
certain pairwise features, they produce a vector of similarities using a
single-layer neural network, and compute a coreference score. 
The model is tested on an ACE dataset and an expanded version with performance
comparable to previous feature-rich systems.
The main contribution of the paper, in my opinion, is in developing a neural
approach for entity linking that combines word embeddings with several
linguistic features. It is interesting to find out that just using the word
embeddings is not sufficient for good performance. Fortunately, the linguistic
features used are limited and do not require manually-crafted external
resources.  

Experimental setting
- It appears that gold trigger words are used rather than predicted ones. The
authors make an argument why this is reasonable, although I still would have
liked to see performance with predicted triggers. This is especially
problematic as one of the competitor systems used predicted triggers, so the
comparison isn't fair. 
- The fact that different papers use different train/test splits is worrisome.
I would encourage the authors to stick to previous splits as much as possible. 

Unclear points
- The numbers indicating that cross-sentential information is needed are
convincing. However, the last statement in the second paragraph (lines 65-70)
was not clear to me.
- Embeddings for positions are said to be generaties ""in a way similar to word
embeddings"". How exactly? Are they randomly initialized? Are they lexicalized?
It is not clear to me why a relative position next to one word should have the
same embedding as a relative position next to a different word.
- How exactly are left vs right neighbors used to create the representation
(lines 307-311)? Does this only affect the max-pooling operation?
- The word embeddings of one word before and one word after the trigger words
are appended to it. This seems a bit arbitrary. Why one word before and after
and not some other choice?  
- It is not clear how the event-mention representation v_e (line 330) is used?
In the following sections only v_{sent+lex} appear to be used, not v_e.
- How are pairwise features used in section 3.2? Most features are binary, so I
assume they are encoded as a binary vector, but what about the distance feature
for example? And, are these kept fixed during training?

Other issues and suggestions
- Can the approach be applied to entity coreference resolution as well? This
would allow comparing with more previous work and popular datasets like
OntoNotes. 
- The use of a square function as nonlinearity is interesting. Is it novel? Do
you think it has applicability in other tasks?
- Datasets: one dataset is publicly available, but results are also presented
with ACE++, which is not. Do you have plans to release it? It would help other
researchers compare new methods. At least, it would have been good to see a
comparison to the feature-rich systems also on this dataset.
- Results: some of the numbers reported in the results are quite close.
Significance testing would help substantiating the comparisons.
- Related work: among the work on (entity) coreference resolution, one might
mention the neural network approach by Wiseman et al. (2015)  

Minor issues
- line 143, ""that"" is redundant. 
- One of the baselines is referred to as ""same type"" in table 6, but ""same
event"" in the text (line 670).        

Refs
- Learning Anaphoricity and Antecedent Ranking Features for Coreference
Resolution. Sam Wiseman, Alexander M. Rush, Jason Weston, and Stuart M.
Shieber. ACL 2015.",,4,3,Poster,4,5,4,4,4,5,3,3,2016,"We follow the notion of events from the ACE 2005 dataset (LDC, 2005; Walker et al., 2006). Consider the following example:
British bank Barclays had agreed to buy Spanish rival Banco Zaragozano for 1.14 billion euros. The combination of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses and will happen this year, in contrast to Barclays’ postponed merger with Lloyds.1
Processing these sentences in a prototypical, ACE-style information extraction (IE) pipeline would involve (a) the recognition of entity mentions. In the example, mentions of entities are underlined. Next, (b) words in the text are processed as to whether they elicit an event reference, i.e., event triggers are identified and their semantic type is classified. The above sentences contain three event mentions with type Business.MergeOrg, shown in boldface. The task of event extraction further requires that (c) participants of recognized events are determined among the entity mentions in the same sentence, i.e., an event’s arguments are identified and their semantic role wrt. to the event is classified. The three recognized event mentions are:
E1: buy(British bank Barclays, Spanish rival Banco Zaragozano, 1.14 billion euros) E2: combination(Barclays Spain, Zaragozano, this year) E3: merger(Barclays, Lloyds)
Often, an IE system involves (d) a disambiguation step of the entity mentions against one another in the same document. This allows to identify that the three mentions of “Barclays” in the text as referring to the same real-world entity. The analogous task on the level of event mentions is called (e) event linking (or: event coreference resolution) and is the focus of this paper. Specifically, the task is
1Based on an example in (Araki and Mitamura, 2015).
to determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date).This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture.
Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples:
• lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexico-semantic resources (WordNet, FrameNet) and other datasets (VerbOcean corpus), enrichtment of arguments with alternative names from external sources (DBpedia, Geonames)
While lexical, discourse, and intrinsic-semantic features are available in virtually all application scenarios of event extraction/linking, and even syntactic parsing is no longer considered an expensive feature source, semantic features from external knowledge sources pose a significant burden on the application of event processing systems, as these sources are created at high cost and come with limited domain coverage.
Fortunately, recent work has explored the use of a new feature class, sentential features, for tackling relation-/event-extraction related tasks with neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). These approaches have shown that processing sentences with neural models yields representations suitable for IE, which motivates their use in our approach.
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
... had agreed to buy Spanish rival ... -3 -2 -1 0 +1 +2
to buy Spanish
Embedding look-up for words and positions Convolution layer Piecewise max-pooling Concat. Hidden layer
Generation of event-mention representation …
Sentence-level feature generation
Embedding look-up for trigger & context
Trigger-local feature generation
Model part (a).
● type compatibility ● position in discourse ● realis match ● argument overlap
Distributed similarity
The combination of the banking operations ...
... had agreed to buy Spanish rival ...
2x model part (a) Concat. Logistic regression
Coreference scoring
Pairwise features
Trigger-local & sentencelevel features
Model part (b).
Figure 1: The two parts of the model. The first part computes a representation for a single event mention. The second part is fed with two such event-mention representations plus a number of pairwise features for the input event-mention pair, and calculates a coreference score.
Data properties A preliminary analysis of one dataset used in our experiments (ACE++; see Section 5) further motivates the design of our model. We found that 50.97 % of coreferential event-mentions pairs share no arguments, either by mentioning distinct argument roles or because one/both mentions have no annotated arguments. Furthermore, 47.29 % of positive event-mention pairs have different trigger words. It is thus important to not solely rely on intrinsic event properties in order to model event mentions, but to additionally take the surrounding sentence’s semantics into account. Another observation regards the distance of coreferential event mentions in a document. 55.42% are more than five sentences apart. This indicates that a locality-based heuristic would not perform well and also encourages the use of sentential features for making coreference decisions.The architecture of the model (Figure 1) is split into two parts. The first one aims at adequately representing individual event mentions. As is common in literature, words of the whole sentence of an input event mention are represented as real-valued vectors viw of a fixed size dw, with i being a word’s position in the sentence. These word embeddings
are updated during model training and are stored in a matrix Ww ∈ Rdw×|V |; |V | being the vocabulary size of the dataset.
Furthermore, we take the relative position of tokens with respect to the mention into account, as suggested by (Collobert et al., 2011; Zeng et al., 2014). The rationale is that while the absolute position of learned features in a sentence might not be relevant for an event-related decision, the position of them wrt. the event mention is. Embeddings v(·)p of size dp for relative positions are generated in a way similar to word embeddings. Embeddings for words and positions are concatenated into vectors v (·) t of size dt = dw + dp. A sentence with s words is thus represented by a matrix of dimensions s×dt. This matrix serves as input to a convolution layer.
In order to compress the semantics of s words into a sentence-level feature vector with constant size, the convolution layer applies dc filters to each window of n consecutive words, calculating dc features for each n-gram of a sentence. For a single filter and particular window, this operation is defined as
vic = relu(wc · vi:i+n−1t + bc), (1)
where wc ∈ Rn∗dt is a filter, vi:i+n−1t is the flattened concatenation of vectors v(·)t for words at
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
positions i through i+n−1, bc is a bias, and relu is the activation function of a rectified linear unit. In Figure 1, dc = 3 and n = 2.
In order to identify the most indicative features in the sentence and to introduce invariance for the absolute position of these, we feed the n-gram representations to a max-pooling layer, which identifies the maximum value for each filter. We treat n-grams on each side of the trigger word separately, which allows the model to handle multiple event mentions per sentence, similar in spirit to (Chen et al., 2015; Zeng et al., 2015). The pooling is defined as
vj,km = max (v i c), (2)
where 1 ≤ j ≤ dc designates a feature, k ∈ {left, right} corresponds to a sentence part, and i runs through the convolution windows of k. The output of this step are sentential features vsent ∈ R2∗dc of the input event mention.
Additionally, we provide the network with trigger-local, lexical-level features by concatenating vsent with the word embeddings v (·) w of the trigger word and its left and right neighbor, resulting in vsent+lex ∈ R2∗dc+3∗dw . This encourages the model to take the lexical semantics of the trigger into account, as these can be a strong indicator for coreference. The result is processed by an additional hidden layer, generating the final event-mention representation ve with size de used for the subsequent event-linking decision:
ve = tanh(Wevsent+lex + be). (3)The second part of the model (Figure 1 (b)) processes the representations for two event mentions, and augments these with pairwise comparison features to determine the compatibility of the event mentions. The following features are used, in parentheses we give the feature value for the pair E1, E2 from the example in Section 1: • Coarse-grained and/or fine-grained event type agree-
ment (yes, yes) • Antecedent event is in first sentence (yes) • (Bagged) distance between event mentions in #sen-
tences/#intermediate event mentions (1, 0) • Agreement in event modality (yes) • Overlap in arguments (two shared arguments)
The concatenation of these vectors (vsent+lex+pairw) is processed by a single-layer neural network which calculates a distributed similarity of size dsim for the two event mentions:
vsim = square(Wsimvsent+lex+pairw + bsim). (4)
1: procedure GENERATECLUSTERS(Pd, score): 2: Pd = {(mi,mj)}i,j 3: score : Pd 7→ (0, 1) 4: Cd ← {(mi,mj) ∈ Pd : score(mi,mj) > 0.5} 5: while ∃(mi,mk), (mk,mj) ∈ Cd : (mi,mj) 6∈ Cd do 6: Cd ← Cd ∪ {(mi,mj)} 7: return Cd
Figure 3: Generation of event clusters Cd for a document d based on the coreference scores from the model. Pd is the set of all event-mention pairs from a document, as implemented in Figure 2.
The use of the square function as the network’s non-linearity is backed by the intuition that for measuring similarity, an invariance under polarity changes is desirable. Having dsim similarity dimensions allows the model to learn multiple similarity facets in parallel; in our experiments, this setup outperformed model variants with different activation functions as well as a cosine-similarity based comparison.
To calculate the final output of the model, vsim is fed to a logistic regression classifier, whose output serves as the coreference score:
score = σ(Woutvsim + bout) (5)
We train the model parameters
θ = {Ww,Wp, {wc}, {bc},We, be,Wsim, bsim,Wout, bout} (6)
by minimizing the logistic loss over shuffled minibatches with gradient descent using Adam (Kingma and Ba, 2014).We investigated two alternatives for the generation of examples from documents with recognized event mentions. Figure 2 shows the strategy we found to perform best, which iterates over the event mentions of a document and pairs each mention (the “anaphors”) with all preceding ones (the “antecedent” candidates). This strategy applies to both training and inference time. Soon et al. (2001) propose an alternative strategy, which during training
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
ACE ACE++
# documents 599 1950 # event instances 3617 7520 # event mentions 4728 9956
Table 1: Dataset properties.
creates positive examples only for the closest actual antecedent of an anaphoric event mention with intermediate event mentions serving as negative antecedent candidates. In our experiments, this strategy performed worse than the less elaborate algorithm in Figure 2.
The pairwise coreference decisions of our model induce a clustering of a document’s event mentions. In order to force the model to output a consistent view on a given document, a strategy for resolving conflicting decisions is needed. We followed the strategy detailed in Figure 3, which builds the transitive closure of all positive links. Additionally, we experimented with Ng and Gardent (2002)’s “BestLink” strategy, which discards all but the highest-scoring antecedent of an anaphoric event mention. Liu et al. (2014) reported that for event linking, BestLink outperforms naive transitive closure, however, in our experiments (Section 5) we come to a different conclusion.We implemented our model using the TensorFlow framework (Abadi et al., 2015, v0.6), and chose the ACE 2005 dataset (Walker et al., 2006, later: ACE) as our main testbed. The annotation of this corpus focuses on the event types Conflict.Attack, Movement.Transport, and Life.Die reporting about terrorist attacks, movement of goods and people, and deaths of people; but also contains many more related event types as well as mentions of businessrelevant and judicial events. The corpus consists of merely 599 documents, which is why we create a second dataset that encompasses these documents and additionally contains 1351 more web documents annotated in an analogous fashion. We refer to this second dataset as ACE++. Both datasets are split 9:1 into a development (dev) and test partition; we further split dev 9:1 into a training (train) and validation (valid) partition.2 Table 1 lists statistics for the datasets.
There are a number of architectural alternatives 2The list of documents in ACEvalid/ACEtest is published
here: https://git.io/vwEEP.
in the model as well as hyperparameters to optimize. Besides the size of intermediate representations in the model (dw, dp, dc, de, dsim), we experimented with different convolution window sizes n, activation functions for the similarity-function layer in model part (b), whether to use the dual pooling and final hidden layer in model part (a), whether to apply regularization with `2 penalties or Dropout, and parameters to Adam (η, β1, β2, ). We started our exploration of this space of possibilities from previously reported hyperparameter values (Zhang and Wallace, 2015; Chen et al., 2015) and followed a combined strategy of random sampling from the hyperparameter space (180 points) and line search. Optimization was done by training on ACE++train and evaluating on ACE ++ valid. The final settings we used for all following experiments are listed in Table 2. Ww is initialized with pre-trained embeddings of (Mikolov et al., 2013)3, all other model parameters are randomly initialized. Model training is run for 2000 epochs, after which the best model on the respective valid partition is selected.Table 3 depicts the performance of our model, trained on ACEtrain, on ACEtest, along with the performance of state-of-the-art systems from the literature. From the wide range of proposed metrics for the evaluation of coreference resolution, we believe
3https://code.google.com/archive/p/ word2vec/
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
BLANC B-CUBED MUC Positive links
4 ∗ (Precision / Recall / F1 score) in % This paper 71.80 75.16 73.31 90.52 86.12 88.26 61.54 45.16 52.09 47.89 56.20 51.71 (Liu et al., 2014) 70.88 70.01 70.43 89.90 88.86 89.38 53.42 48.75 50.98 55.86 40.52 46.97 (Bejan and Harabagiu, 2010) — — — 83.4 84.2 83.8 — — — 43.3 47.1 45.1 (Sangeetha and Arock, 2012) — — — — — 87.7 — — — — — —
Table 3: Event-linking performance of our model & competitors on ACE. Best value per metric in bold.
BLANC (Recasens and Hovy, 2011) has the highest validity, as it balances the impact of positive and negative event-mention links in a document. Negative links and consequently singleton event mentions are more common in this dataset (more than 90 % of links are negative). As Recasens and Hovy (2011) point out, the informativeness of metrics like MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), and the naive positivelink metric suffers from such imbalance. We still add these metrics for completeness, and because BLANC is not available for all systems.
Unfortunately, there are two caveats to this comparison. Firstly, while a 9:1 train/test split is the commonly accepted way of using ACE, the exact documents in the partitions vary from system to system. Secondly, published methods follow different strategies regarding preprocessing components. While all systems in Table 3 use gold-annotated event-mention triggers, Bejan and Harabagiu (2010) and Liu et al. (2014) use a semantic-role labeler and other tools instead of gold-argument information. We argue that using gold-annotated event mentions is reasonable in order to mitigate error propagation along extraction pipeline and make performance values for the task at hand more informative.
We beat Liu et al. (2014)’s system in terms of F1 score on BLANC, MUC, and positive-links, while their system performs better in terms of B-CUBED. Even when taking into account the caveats mentioned above, it seems justified to assess that our model performs in general on-par with their stateof-the-art system. Their approach involves randomforest classification with best-link clustering and propagation of attributes between event mentions, and is grounded on a manifold of external feature sources, i.e., it uses a “rich set of 105 semantic features”. Thus, their approach is strongly tied to domains where these semantic features are available and is potentially hard to port to other text kinds. In contrast, our approach does not depend
Model Dataset BLANC
(P/R/F1 in %)
1) Section 3 ACE 71.80 75.16 73.31 2) Sec. 3 + BestLink ACE 75.68 69.72 72.19
3) Section 3 ACE++ 73.22 83.21 76.90 4) Sec. 3 + BestLink ACE++ 74.24 68.86 71.09
Table 4: Impact of data amount and clustering.
on resources with restricted domain availability. Bejan and Harabagiu (2010) propose a nonparametric Bayesian model with standard lexicallevel features and WordNet-based similarity between event elements. We outperform their system in terms of B-CUBED and positive-links, which indicates that their system tends to over-merge event mentions, i.e., has a bias against singletons. They use a slightly bigger variant of ACE with 46 additional documents in their experiments.
Sangeetha and Arock (2012) hand-craft a similarity metric for event mentions based on the number of shared entities in the respective sentences, lexical terms, synsets in WordNet, which serves as input to a mincut-based cluster identification. Their system performs well in terms of B-cubed F1, however their paper provides few details about the exact experimental setup.
Another approach with results on ACE was presented by Chen et al. (2009), who employ a maximum-entropy classifier with agglomerative clustering and lexical, discourse, and semantic features, e.g., also a WordNet-based similarity measure. However, they report performance using a threshold optimized on the test set, thus we decided to not include the performance here.
5.2 Further evaluation on ACE and ACE++
We now look at several aspects of the model performance to gain further insights about it’s behavior.
Impact of dataset size and clustering strategy Table 4 shows the impact of increasing the amount of training data (ACE → ACE++). This increase
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Pw Loc Sen Dataset BLANC
(P/R/F1 in %)
1) X ACE++ 57.45 68.16 56.69 2) X X ACE++ 62.24 76.23 64.12 3) X X X ACE++ 73.22 83.21 76.90 4) X X ACE++ 82.60 70.71 74.97 5) X X ACE++ 59.67 66.25 61.28 6) X ACE++ 58.38 55.85 56.70
Table 5: Impact of feature classes; Pw ≡ pairwise features, Loc ≡ trigger-local lexical features, Sen ≡ sentential features.
Model Dataset BLANC
(P/R/F1 in %)
Section 3 ACE++ 73.22 83.21 76.90 All singletons ACE++ 45.29 50.00 47.53 One instance ACE++ 4.71 50.00 8.60 Same type ACE++ 62.73 84.75 61.35
Table 6: Event-linking performance of our model against naive baselines.
(rows 1, 3) leads to a boost in recall, from 75.16% to 83.21%, at the cost of a small decrease in precision. This indicates that the model can generalize much better using this additional training data.
Looking into the use of the alternative clustering strategy BestLink recommended by Liu et al. (2014), we can make the expected observation of a precision improvement (row 1 vs. 2; row 3 vs. 4), due to fewer positive links being used before the transitive-closure clustering takes place. This is however outweighed by a large decline in recall, resulting in a lower F1 score (73.31→ 72.19; 76.90 → 71.09). The better performance of BestLink in Liu et al.’s model suggests that our model already weeds out many low confidence links in the classification step, which makes a downstream filtering unnecessary in terms of precision, and even counter-productive in terms of recall.
Impact of feature classes Table 5 shows our model’s performance when particular feature classes are removed from the model (with retraining), with row 3 corresponding to the full model as described in Section 3. Unsurprisingly, classifying examples with just pairwise features (row 1) results in the worst performance, and adding first trigger-local lexical features (row 2), then sentential features (row 3) subsequently raises both precision and recall. Just using pairwise features and sentential ones (row 4), boosts precision,
which is counter-intuitive at first, but may be explained by a different utilization of the sententialfeature part of the model during training. This part is then adapted to focus more on the trigger-word aspect, meaning the sentential features degrade to trigger-local features. While this allows to reach higher precision (recall that Section 3 finds that more than fifty percent of positive examples have trigger-word agreement), it substantially limits the model’s ability to learn other coreference-relevant aspects of event-mention pairs, leading to low recall. Further considering rows 5 & 6, we can conclude that all feature classes indeed positively contribute to the overall model performance.
Baselines The result of applying three naive baselines to ACE++ is shown in Table 6. The all singletons/one instance baselines predict every input link to be negative/positive, respectively. In particular the all-singletons baseline performs well, due to the large fraction of singleton event mentions in the dataset. The third baseline, same event, predicts a positive link whenever there is agreement on the event type, namely, it ignores the possibility that there could be multiple event mentions of the same type in a document which do not refer to the same real-world event, e.g., referring to different terrorist attacks. This baseline also performs quite well, in particular in terms of recall, but shows low precision.
Error analysis We manually investigated a sample of 100 false positives and 100 false negatives from ACE++ in order to get an understanding of system errors.
It turns out that a significant portion of the false negatives would involve the resolution of a pronoun to a previous event mention, a very hard and yet unsolved problem. Consider the following examples:
• “It’s crazy that we’re bombing Iraq. It sickens me.” • “Some of the slogans sought to rebut war supporters’
arguments that the protests are unpatriotic. [...] Nobody questions whether this is right or not.
In both examples, the event mentions (trigger words in bold font) are gold-annotated as coreferential, but our model failed to recognize this.
Another observation is that for 17 false negatives, we found analogous cases among the sampled false positives where annotators made a different annotation decision. Consider these examples:
• The 1860 Presidential Election. [...] Lincoln won a plurality with about 40% of the vote.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
• She lost her seat in the 1997 election.
Each bullet point has two event mentions (in bold font) taken from the same document and referring to the same event type, i.e., Personnel.Elect. While in the first example, the annotators identified the mentions as coreferential, the second pair of mentions is not annotated as such. Analogously, 22 out of the 100 analyzed false positives were cases where the misclassification of the system was plausible to a human rater. This exemplifies that this task has many boundary cases were a positive/negative decision is hard to make even for expert annotators, thus putting the overall performance of all models in Table 3 in perspective.","This section elaborates on the conducted experiments. First, we compare our approach to state-ofart systems on dataset ACE, after which we report experiments on ACE++, where we contrast variations of our model to gain insights about the impact of the utilized feature classes. We conclude this section with an error analysis.","Event extraction aims at detecting mentions of realworld events and their arguments in text documents of different domains, e.g., news articles. The subsequent task of event linking is concerned with resolving coreferences between recognized event mentions in a document, and is the focus of this paper.
Several studies investigate event linking and related problems such as relation mentions spanning multiple sentences. Swampillai and Stevenson (2010) find that 28.5 % of binary relation mentions in the MUC 6 dataset are affected, as are 9.4 % of
relation mentions in the ACE corpus from 2003. Ji and Grishman (2011) estimate that 15 % of slot fills in the training data for the “TAC 2010 KBP Slot Filling” task require cross-sentential inference. To confirm these numbers, we analyzed the event annotation of the ACE 2005 corpus and found that approximately 23 % of the event mentions are incomplete on the argument level, with respect to the information in other mentions of the same event instance in the respective document. These numbers suggest that event linking is an important task.
Previous approaches for modeling event mentions in context of coreference resolution (Bejan and Harabagiu, 2010; Sangeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level.
Our contributions in this paper are as follows: We design a system for event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisions for pairs of event mentions based on the previously generated representations. This approach does not
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
rely on external semantic features, but rather employs a combination of local and sentential features to describe individual event mentions, and combines these intermediate event representations with standard pairwise features for the coreference decision. The model achieves state-of-the-art performance in our experiments on two datasets, one of which is publicly available. Furthermore, we present an analysis of the system errors to identify directions for further research.","We briefly point out other relevant approaches and efforts from the vast amount of literature.
Event coreference In addition to the competitors mentioned in Section 5, approaches for event linking were presented, e.g., by Chen and Ji (2009), who determine link scores with hand-crafted compatibility metrics for event mention pairs and a maximum-entropy model, and feed these to a spectral clustering algorithm. A variation of the eventcoreference resolution task extends the scope to cross-document relations. Cybulska and Vossen (2015) approach this task with various classification models and propose to use a type-specific granularity hierarchy for feature values. Lee et al. (2012) further extend the task definition by jointly resolving entity and event coreference, through several iterations of mention-cluster merge operations. Sachan et al. (2015) describe an active-learning based method for the same problem, where they derive a clustering of entities/events by incorporating bits of human judgment as constraints into the objective function. Araki and Mitamura (2015) simultaneously identify event triggers and disambiguate them wrt. one another with a structured-perceptron algorithm.
Resources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermin-
gles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain.
Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011).
In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015).",,"Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences.
As next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument setting or for joint trigger identification and coreference resolution. Furthermore, the generation of sentential features from other types of neural networks seems promising. Regarding our medium-term research agenda, we would like to investigate if the model can benefit from more finegrained information about the discourse structure underlying a text. This could guide the model when encountering the problematic case of pronoun resolution, described in the error analysis.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
35,"Coreference resolution for event mentions enables extraction systems to process document-level information. Current systems in this area base their decisions on rich semantic features from various knowledge bases, thus restricting them to domains where such external sources are available. We propose a model for this task which does not rely on such features but instead utilizes sentential features coming from convolutional neural networks. Two such networks first process coreference candidates and their respective context, thereby generating latent-feature representations which are tuned towards event aspects relevant for a linking decision. These representations are augmented with lexical-level and pairwise features, and serve as input to a trainable similarity function producing a coreference score. Our model achieves state-of-the-art performance on two datasets, one of which is publicly available. An error analysis points out directions for further research.",Event Linking with Sentential Features from Convolutional Neural Networks,13,"This paper presents a model for the task of event entity linking, where they
propose to use sentential features from CNNs in place of external knowledge
sources which earlier methods have used. They train a two-part model: the first
part learns an event mention representation, and the second part learns to
calculate a coreference score given two event entity mentions.

The paper is well-written, well-presented and is easy to follow. I rather like
the analysis done on the ACE corpus regarding the argument sharing between
event coreferences. Furthermore, the analysis on the size impact of the
dataset is a great motivation for creating their ACE++ dataset. However, there
are a few
major issues that need to be addressed:

- The authors fail to motivate and analyze the pros and cons of using CNN for
generating mention representations. It is not discussed why they chose CNN and
there are no comparisons to the other models (e.g., straightforwardly an RNN).
Given that the improvement their model makes according various metrics against
the
state-of-the-art is only 2 or 3 points on F1 score, there needs to be more
evidence that this architecture is indeed superior.

- It is not clear what is novel about the idea of tackling event linking with
sentential features, given that using CNN in this fashion for a classification
task is not new. The authors could explicitly point out and mainly compare to
any existing continuous space methods for event linking. The choice of methods
in Table 3 is not thorough enough.

- There is no information regarding how the ACE++ dataset is collected. A major
issue with the ACE dataset is its limited number of event types, making it too
constrained and biased. It is important to know what event types ACE++ covers.
This can also help support the claim in Section 5.1 that 'other approaches are
strongly tied to the domain where these semantic features are availableâ¦our
approach does not depend on resources with restrictedâ¦', you need to show
that those earlier methods fail on some dataset that you succeed on. Also,
for enabling any meaningful comparison in future, the authors should think
about making this dataset publicly available.

Some minor issues:
- I would have liked to see the performance of your model without gold
references in Table 3 as well.

- It would be nice to explore how this model can or cannot be augmented with a
vanilla coreference resolution system. For the specific example in line 687,
the off-the-shelf CoreNLP system readily links 'It' to 'bombing', which can be
somehow leveraged in an event entity linking baseline.

- Given the relatively small size of the ACE dataset, I think having a
compelling model requires testing on the other available resources as well.
This further motivates working on entity and event coreference simultaneously.
I also believe that testing on EventCorefBank in parallel with ACE is
essential. 

- Table 5 shows that the pairwise features have been quite effective, which
signals that feature engineering may still be crucial for having a competitive
model (at least on the scale of the ACE dataset). One would wonder which
features were the most effective, and why not report how the current set was
chosen and what else was tried.",,3,3,Oral Presentation,5,3,4,4,3,5,2,3,2016,"We follow the notion of events from the ACE 2005 dataset (LDC, 2005; Walker et al., 2006). Consider the following example:
British bank Barclays had agreed to buy Spanish rival Banco Zaragozano for 1.14 billion euros. The combination of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses and will happen this year, in contrast to Barclays’ postponed merger with Lloyds.1
Processing these sentences in a prototypical, ACE-style information extraction (IE) pipeline would involve (a) the recognition of entity mentions. In the example, mentions of entities are underlined. Next, (b) words in the text are processed as to whether they elicit an event reference, i.e., event triggers are identified and their semantic type is classified. The above sentences contain three event mentions with type Business.MergeOrg, shown in boldface. The task of event extraction further requires that (c) participants of recognized events are determined among the entity mentions in the same sentence, i.e., an event’s arguments are identified and their semantic role wrt. to the event is classified. The three recognized event mentions are:
E1: buy(British bank Barclays, Spanish rival Banco Zaragozano, 1.14 billion euros) E2: combination(Barclays Spain, Zaragozano, this year) E3: merger(Barclays, Lloyds)
Often, an IE system involves (d) a disambiguation step of the entity mentions against one another in the same document. This allows to identify that the three mentions of “Barclays” in the text as referring to the same real-world entity. The analogous task on the level of event mentions is called (e) event linking (or: event coreference resolution) and is the focus of this paper. Specifically, the task is
1Based on an example in (Araki and Mitamura, 2015).
to determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date).This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture.
Event features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples:
• lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexico-semantic resources (WordNet, FrameNet) and other datasets (VerbOcean corpus), enrichtment of arguments with alternative names from external sources (DBpedia, Geonames)
While lexical, discourse, and intrinsic-semantic features are available in virtually all application scenarios of event extraction/linking, and even syntactic parsing is no longer considered an expensive feature source, semantic features from external knowledge sources pose a significant burden on the application of event processing systems, as these sources are created at high cost and come with limited domain coverage.
Fortunately, recent work has explored the use of a new feature class, sentential features, for tackling relation-/event-extraction related tasks with neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). These approaches have shown that processing sentences with neural models yields representations suitable for IE, which motivates their use in our approach.
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
... had agreed to buy Spanish rival ... -3 -2 -1 0 +1 +2
to buy Spanish
Embedding look-up for words and positions Convolution layer Piecewise max-pooling Concat. Hidden layer
Generation of event-mention representation …
Sentence-level feature generation
Embedding look-up for trigger & context
Trigger-local feature generation
Model part (a).
● type compatibility ● position in discourse ● realis match ● argument overlap
Distributed similarity
The combination of the banking operations ...
... had agreed to buy Spanish rival ...
2x model part (a) Concat. Logistic regression
Coreference scoring
Pairwise features
Trigger-local & sentencelevel features
Model part (b).
Figure 1: The two parts of the model. The first part computes a representation for a single event mention. The second part is fed with two such event-mention representations plus a number of pairwise features for the input event-mention pair, and calculates a coreference score.
Data properties A preliminary analysis of one dataset used in our experiments (ACE++; see Section 5) further motivates the design of our model. We found that 50.97 % of coreferential event-mentions pairs share no arguments, either by mentioning distinct argument roles or because one/both mentions have no annotated arguments. Furthermore, 47.29 % of positive event-mention pairs have different trigger words. It is thus important to not solely rely on intrinsic event properties in order to model event mentions, but to additionally take the surrounding sentence’s semantics into account. Another observation regards the distance of coreferential event mentions in a document. 55.42% are more than five sentences apart. This indicates that a locality-based heuristic would not perform well and also encourages the use of sentential features for making coreference decisions.The architecture of the model (Figure 1) is split into two parts. The first one aims at adequately representing individual event mentions. As is common in literature, words of the whole sentence of an input event mention are represented as real-valued vectors viw of a fixed size dw, with i being a word’s position in the sentence. These word embeddings
are updated during model training and are stored in a matrix Ww ∈ Rdw×|V |; |V | being the vocabulary size of the dataset.
Furthermore, we take the relative position of tokens with respect to the mention into account, as suggested by (Collobert et al., 2011; Zeng et al., 2014). The rationale is that while the absolute position of learned features in a sentence might not be relevant for an event-related decision, the position of them wrt. the event mention is. Embeddings v(·)p of size dp for relative positions are generated in a way similar to word embeddings. Embeddings for words and positions are concatenated into vectors v (·) t of size dt = dw + dp. A sentence with s words is thus represented by a matrix of dimensions s×dt. This matrix serves as input to a convolution layer.
In order to compress the semantics of s words into a sentence-level feature vector with constant size, the convolution layer applies dc filters to each window of n consecutive words, calculating dc features for each n-gram of a sentence. For a single filter and particular window, this operation is defined as
vic = relu(wc · vi:i+n−1t + bc), (1)
where wc ∈ Rn∗dt is a filter, vi:i+n−1t is the flattened concatenation of vectors v(·)t for words at
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
positions i through i+n−1, bc is a bias, and relu is the activation function of a rectified linear unit. In Figure 1, dc = 3 and n = 2.
In order to identify the most indicative features in the sentence and to introduce invariance for the absolute position of these, we feed the n-gram representations to a max-pooling layer, which identifies the maximum value for each filter. We treat n-grams on each side of the trigger word separately, which allows the model to handle multiple event mentions per sentence, similar in spirit to (Chen et al., 2015; Zeng et al., 2015). The pooling is defined as
vj,km = max (v i c), (2)
where 1 ≤ j ≤ dc designates a feature, k ∈ {left, right} corresponds to a sentence part, and i runs through the convolution windows of k. The output of this step are sentential features vsent ∈ R2∗dc of the input event mention.
Additionally, we provide the network with trigger-local, lexical-level features by concatenating vsent with the word embeddings v (·) w of the trigger word and its left and right neighbor, resulting in vsent+lex ∈ R2∗dc+3∗dw . This encourages the model to take the lexical semantics of the trigger into account, as these can be a strong indicator for coreference. The result is processed by an additional hidden layer, generating the final event-mention representation ve with size de used for the subsequent event-linking decision:
ve = tanh(Wevsent+lex + be). (3)The second part of the model (Figure 1 (b)) processes the representations for two event mentions, and augments these with pairwise comparison features to determine the compatibility of the event mentions. The following features are used, in parentheses we give the feature value for the pair E1, E2 from the example in Section 1: • Coarse-grained and/or fine-grained event type agree-
ment (yes, yes) • Antecedent event is in first sentence (yes) • (Bagged) distance between event mentions in #sen-
tences/#intermediate event mentions (1, 0) • Agreement in event modality (yes) • Overlap in arguments (two shared arguments)
The concatenation of these vectors (vsent+lex+pairw) is processed by a single-layer neural network which calculates a distributed similarity of size dsim for the two event mentions:
vsim = square(Wsimvsent+lex+pairw + bsim). (4)
1: procedure GENERATECLUSTERS(Pd, score): 2: Pd = {(mi,mj)}i,j 3: score : Pd 7→ (0, 1) 4: Cd ← {(mi,mj) ∈ Pd : score(mi,mj) > 0.5} 5: while ∃(mi,mk), (mk,mj) ∈ Cd : (mi,mj) 6∈ Cd do 6: Cd ← Cd ∪ {(mi,mj)} 7: return Cd
Figure 3: Generation of event clusters Cd for a document d based on the coreference scores from the model. Pd is the set of all event-mention pairs from a document, as implemented in Figure 2.
The use of the square function as the network’s non-linearity is backed by the intuition that for measuring similarity, an invariance under polarity changes is desirable. Having dsim similarity dimensions allows the model to learn multiple similarity facets in parallel; in our experiments, this setup outperformed model variants with different activation functions as well as a cosine-similarity based comparison.
To calculate the final output of the model, vsim is fed to a logistic regression classifier, whose output serves as the coreference score:
score = σ(Woutvsim + bout) (5)
We train the model parameters
θ = {Ww,Wp, {wc}, {bc},We, be,Wsim, bsim,Wout, bout} (6)
by minimizing the logistic loss over shuffled minibatches with gradient descent using Adam (Kingma and Ba, 2014).We investigated two alternatives for the generation of examples from documents with recognized event mentions. Figure 2 shows the strategy we found to perform best, which iterates over the event mentions of a document and pairs each mention (the “anaphors”) with all preceding ones (the “antecedent” candidates). This strategy applies to both training and inference time. Soon et al. (2001) propose an alternative strategy, which during training
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
ACE ACE++
# documents 599 1950 # event instances 3617 7520 # event mentions 4728 9956
Table 1: Dataset properties.
creates positive examples only for the closest actual antecedent of an anaphoric event mention with intermediate event mentions serving as negative antecedent candidates. In our experiments, this strategy performed worse than the less elaborate algorithm in Figure 2.
The pairwise coreference decisions of our model induce a clustering of a document’s event mentions. In order to force the model to output a consistent view on a given document, a strategy for resolving conflicting decisions is needed. We followed the strategy detailed in Figure 3, which builds the transitive closure of all positive links. Additionally, we experimented with Ng and Gardent (2002)’s “BestLink” strategy, which discards all but the highest-scoring antecedent of an anaphoric event mention. Liu et al. (2014) reported that for event linking, BestLink outperforms naive transitive closure, however, in our experiments (Section 5) we come to a different conclusion.We implemented our model using the TensorFlow framework (Abadi et al., 2015, v0.6), and chose the ACE 2005 dataset (Walker et al., 2006, later: ACE) as our main testbed. The annotation of this corpus focuses on the event types Conflict.Attack, Movement.Transport, and Life.Die reporting about terrorist attacks, movement of goods and people, and deaths of people; but also contains many more related event types as well as mentions of businessrelevant and judicial events. The corpus consists of merely 599 documents, which is why we create a second dataset that encompasses these documents and additionally contains 1351 more web documents annotated in an analogous fashion. We refer to this second dataset as ACE++. Both datasets are split 9:1 into a development (dev) and test partition; we further split dev 9:1 into a training (train) and validation (valid) partition.2 Table 1 lists statistics for the datasets.
There are a number of architectural alternatives 2The list of documents in ACEvalid/ACEtest is published
here: https://git.io/vwEEP.
in the model as well as hyperparameters to optimize. Besides the size of intermediate representations in the model (dw, dp, dc, de, dsim), we experimented with different convolution window sizes n, activation functions for the similarity-function layer in model part (b), whether to use the dual pooling and final hidden layer in model part (a), whether to apply regularization with `2 penalties or Dropout, and parameters to Adam (η, β1, β2, ). We started our exploration of this space of possibilities from previously reported hyperparameter values (Zhang and Wallace, 2015; Chen et al., 2015) and followed a combined strategy of random sampling from the hyperparameter space (180 points) and line search. Optimization was done by training on ACE++train and evaluating on ACE ++ valid. The final settings we used for all following experiments are listed in Table 2. Ww is initialized with pre-trained embeddings of (Mikolov et al., 2013)3, all other model parameters are randomly initialized. Model training is run for 2000 epochs, after which the best model on the respective valid partition is selected.Table 3 depicts the performance of our model, trained on ACEtrain, on ACEtest, along with the performance of state-of-the-art systems from the literature. From the wide range of proposed metrics for the evaluation of coreference resolution, we believe
3https://code.google.com/archive/p/ word2vec/
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
BLANC B-CUBED MUC Positive links
4 ∗ (Precision / Recall / F1 score) in % This paper 71.80 75.16 73.31 90.52 86.12 88.26 61.54 45.16 52.09 47.89 56.20 51.71 (Liu et al., 2014) 70.88 70.01 70.43 89.90 88.86 89.38 53.42 48.75 50.98 55.86 40.52 46.97 (Bejan and Harabagiu, 2010) — — — 83.4 84.2 83.8 — — — 43.3 47.1 45.1 (Sangeetha and Arock, 2012) — — — — — 87.7 — — — — — —
Table 3: Event-linking performance of our model & competitors on ACE. Best value per metric in bold.
BLANC (Recasens and Hovy, 2011) has the highest validity, as it balances the impact of positive and negative event-mention links in a document. Negative links and consequently singleton event mentions are more common in this dataset (more than 90 % of links are negative). As Recasens and Hovy (2011) point out, the informativeness of metrics like MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), and the naive positivelink metric suffers from such imbalance. We still add these metrics for completeness, and because BLANC is not available for all systems.
Unfortunately, there are two caveats to this comparison. Firstly, while a 9:1 train/test split is the commonly accepted way of using ACE, the exact documents in the partitions vary from system to system. Secondly, published methods follow different strategies regarding preprocessing components. While all systems in Table 3 use gold-annotated event-mention triggers, Bejan and Harabagiu (2010) and Liu et al. (2014) use a semantic-role labeler and other tools instead of gold-argument information. We argue that using gold-annotated event mentions is reasonable in order to mitigate error propagation along extraction pipeline and make performance values for the task at hand more informative.
We beat Liu et al. (2014)’s system in terms of F1 score on BLANC, MUC, and positive-links, while their system performs better in terms of B-CUBED. Even when taking into account the caveats mentioned above, it seems justified to assess that our model performs in general on-par with their stateof-the-art system. Their approach involves randomforest classification with best-link clustering and propagation of attributes between event mentions, and is grounded on a manifold of external feature sources, i.e., it uses a “rich set of 105 semantic features”. Thus, their approach is strongly tied to domains where these semantic features are available and is potentially hard to port to other text kinds. In contrast, our approach does not depend
Model Dataset BLANC
(P/R/F1 in %)
1) Section 3 ACE 71.80 75.16 73.31 2) Sec. 3 + BestLink ACE 75.68 69.72 72.19
3) Section 3 ACE++ 73.22 83.21 76.90 4) Sec. 3 + BestLink ACE++ 74.24 68.86 71.09
Table 4: Impact of data amount and clustering.
on resources with restricted domain availability. Bejan and Harabagiu (2010) propose a nonparametric Bayesian model with standard lexicallevel features and WordNet-based similarity between event elements. We outperform their system in terms of B-CUBED and positive-links, which indicates that their system tends to over-merge event mentions, i.e., has a bias against singletons. They use a slightly bigger variant of ACE with 46 additional documents in their experiments.
Sangeetha and Arock (2012) hand-craft a similarity metric for event mentions based on the number of shared entities in the respective sentences, lexical terms, synsets in WordNet, which serves as input to a mincut-based cluster identification. Their system performs well in terms of B-cubed F1, however their paper provides few details about the exact experimental setup.
Another approach with results on ACE was presented by Chen et al. (2009), who employ a maximum-entropy classifier with agglomerative clustering and lexical, discourse, and semantic features, e.g., also a WordNet-based similarity measure. However, they report performance using a threshold optimized on the test set, thus we decided to not include the performance here.
5.2 Further evaluation on ACE and ACE++
We now look at several aspects of the model performance to gain further insights about it’s behavior.
Impact of dataset size and clustering strategy Table 4 shows the impact of increasing the amount of training data (ACE → ACE++). This increase
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Pw Loc Sen Dataset BLANC
(P/R/F1 in %)
1) X ACE++ 57.45 68.16 56.69 2) X X ACE++ 62.24 76.23 64.12 3) X X X ACE++ 73.22 83.21 76.90 4) X X ACE++ 82.60 70.71 74.97 5) X X ACE++ 59.67 66.25 61.28 6) X ACE++ 58.38 55.85 56.70
Table 5: Impact of feature classes; Pw ≡ pairwise features, Loc ≡ trigger-local lexical features, Sen ≡ sentential features.
Model Dataset BLANC
(P/R/F1 in %)
Section 3 ACE++ 73.22 83.21 76.90 All singletons ACE++ 45.29 50.00 47.53 One instance ACE++ 4.71 50.00 8.60 Same type ACE++ 62.73 84.75 61.35
Table 6: Event-linking performance of our model against naive baselines.
(rows 1, 3) leads to a boost in recall, from 75.16% to 83.21%, at the cost of a small decrease in precision. This indicates that the model can generalize much better using this additional training data.
Looking into the use of the alternative clustering strategy BestLink recommended by Liu et al. (2014), we can make the expected observation of a precision improvement (row 1 vs. 2; row 3 vs. 4), due to fewer positive links being used before the transitive-closure clustering takes place. This is however outweighed by a large decline in recall, resulting in a lower F1 score (73.31→ 72.19; 76.90 → 71.09). The better performance of BestLink in Liu et al.’s model suggests that our model already weeds out many low confidence links in the classification step, which makes a downstream filtering unnecessary in terms of precision, and even counter-productive in terms of recall.
Impact of feature classes Table 5 shows our model’s performance when particular feature classes are removed from the model (with retraining), with row 3 corresponding to the full model as described in Section 3. Unsurprisingly, classifying examples with just pairwise features (row 1) results in the worst performance, and adding first trigger-local lexical features (row 2), then sentential features (row 3) subsequently raises both precision and recall. Just using pairwise features and sentential ones (row 4), boosts precision,
which is counter-intuitive at first, but may be explained by a different utilization of the sententialfeature part of the model during training. This part is then adapted to focus more on the trigger-word aspect, meaning the sentential features degrade to trigger-local features. While this allows to reach higher precision (recall that Section 3 finds that more than fifty percent of positive examples have trigger-word agreement), it substantially limits the model’s ability to learn other coreference-relevant aspects of event-mention pairs, leading to low recall. Further considering rows 5 & 6, we can conclude that all feature classes indeed positively contribute to the overall model performance.
Baselines The result of applying three naive baselines to ACE++ is shown in Table 6. The all singletons/one instance baselines predict every input link to be negative/positive, respectively. In particular the all-singletons baseline performs well, due to the large fraction of singleton event mentions in the dataset. The third baseline, same event, predicts a positive link whenever there is agreement on the event type, namely, it ignores the possibility that there could be multiple event mentions of the same type in a document which do not refer to the same real-world event, e.g., referring to different terrorist attacks. This baseline also performs quite well, in particular in terms of recall, but shows low precision.
Error analysis We manually investigated a sample of 100 false positives and 100 false negatives from ACE++ in order to get an understanding of system errors.
It turns out that a significant portion of the false negatives would involve the resolution of a pronoun to a previous event mention, a very hard and yet unsolved problem. Consider the following examples:
• “It’s crazy that we’re bombing Iraq. It sickens me.” • “Some of the slogans sought to rebut war supporters’
arguments that the protests are unpatriotic. [...] Nobody questions whether this is right or not.
In both examples, the event mentions (trigger words in bold font) are gold-annotated as coreferential, but our model failed to recognize this.
Another observation is that for 17 false negatives, we found analogous cases among the sampled false positives where annotators made a different annotation decision. Consider these examples:
• The 1860 Presidential Election. [...] Lincoln won a plurality with about 40% of the vote.
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
• She lost her seat in the 1997 election.
Each bullet point has two event mentions (in bold font) taken from the same document and referring to the same event type, i.e., Personnel.Elect. While in the first example, the annotators identified the mentions as coreferential, the second pair of mentions is not annotated as such. Analogously, 22 out of the 100 analyzed false positives were cases where the misclassification of the system was plausible to a human rater. This exemplifies that this task has many boundary cases were a positive/negative decision is hard to make even for expert annotators, thus putting the overall performance of all models in Table 3 in perspective.","This section elaborates on the conducted experiments. First, we compare our approach to state-ofart systems on dataset ACE, after which we report experiments on ACE++, where we contrast variations of our model to gain insights about the impact of the utilized feature classes. We conclude this section with an error analysis.","Event extraction aims at detecting mentions of realworld events and their arguments in text documents of different domains, e.g., news articles. The subsequent task of event linking is concerned with resolving coreferences between recognized event mentions in a document, and is the focus of this paper.
Several studies investigate event linking and related problems such as relation mentions spanning multiple sentences. Swampillai and Stevenson (2010) find that 28.5 % of binary relation mentions in the MUC 6 dataset are affected, as are 9.4 % of
relation mentions in the ACE corpus from 2003. Ji and Grishman (2011) estimate that 15 % of slot fills in the training data for the “TAC 2010 KBP Slot Filling” task require cross-sentential inference. To confirm these numbers, we analyzed the event annotation of the ACE 2005 corpus and found that approximately 23 % of the event mentions are incomplete on the argument level, with respect to the information in other mentions of the same event instance in the respective document. These numbers suggest that event linking is an important task.
Previous approaches for modeling event mentions in context of coreference resolution (Bejan and Harabagiu, 2010; Sangeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level.
Our contributions in this paper are as follows: We design a system for event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisions for pairs of event mentions based on the previously generated representations. This approach does not
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
rely on external semantic features, but rather employs a combination of local and sentential features to describe individual event mentions, and combines these intermediate event representations with standard pairwise features for the coreference decision. The model achieves state-of-the-art performance in our experiments on two datasets, one of which is publicly available. Furthermore, we present an analysis of the system errors to identify directions for further research.","We briefly point out other relevant approaches and efforts from the vast amount of literature.
Event coreference In addition to the competitors mentioned in Section 5, approaches for event linking were presented, e.g., by Chen and Ji (2009), who determine link scores with hand-crafted compatibility metrics for event mention pairs and a maximum-entropy model, and feed these to a spectral clustering algorithm. A variation of the eventcoreference resolution task extends the scope to cross-document relations. Cybulska and Vossen (2015) approach this task with various classification models and propose to use a type-specific granularity hierarchy for feature values. Lee et al. (2012) further extend the task definition by jointly resolving entity and event coreference, through several iterations of mention-cluster merge operations. Sachan et al. (2015) describe an active-learning based method for the same problem, where they derive a clustering of entities/events by incorporating bits of human judgment as constraints into the objective function. Araki and Mitamura (2015) simultaneously identify event triggers and disambiguate them wrt. one another with a structured-perceptron algorithm.
Resources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermin-
gles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain.
Other A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011).
In addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015).",,"Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences.
As next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument setting or for joint trigger identification and coreference resolution. Furthermore, the generation of sentential features from other types of neural networks seems promising. Regarding our medium-term research agenda, we would like to investigate if the model can benefit from more finegrained information about the discourse structure underlying a text. This could guide the model when encountering the problematic case of pronoun resolution, described in the error analysis.
9
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899"
36,"We present UDP, an unsupervised parsing algorithm for Universal Dependencies (UD) based on  PageRank and a small set of specific dependency head rules. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual UD parsing. It is distinctly robust to domain change across languages.",Parsing for Universal Dependencies without training,151,"This paper describes a new deterministic dependency parsing algorithm and
analyses its behaviour across a range of languages.
The core of the algorithm is a set of rules defining permitted dependencies
based on POS tags.
The algorithm starts by ranking words using a slightly biased PageRank over a
graph with edges defined by the permitted dependencies.
Stepping through the ranking, each word is linked to the closest word that will
maintain a tree and is permitted by the head rules and a directionality
constraint.

Overall, the paper is interesting and clearly presented, though seems to differ
only slightly from Sogaard (2012), ""Unsupervised Dependency Parsing without
Training"".
I have a few questions and suggestions:

Head Rules (Table 1) - It would be good to have some analysis of these rules in
relation to the corpus.
For example, in section 3.1 the fact that they do not always lead to a
connected graph is mentioned, but not how frequently it occurs, or how large
the components typically are.

I was surprised that head direction was chosen using the test data rather than
training or development data.
Given how fast the decision converges (10-15 sentences), this is not a major
issue, but a surprising choice.

How does tie-breaking for words with the same PageRank score work?
Does it impact performance significantly, or are ties rare enough that it
doesn't have an impact?

The various types of constraints (head rules, directionality, distance) will
lead to upper bounds on possible performance of the system.
It would be informative to include oracle results for each constraint, to show
how much they hurt the maximum possible score.
That would be particularly helpful for guiding future work in terms of where to
try to modify this system.

Minor:

- 4.1, ""we obtain [the] rank""

- Table 5 and Table 7 have columns in different orders. I found the Table 7
arrangement clearer.

- 6.1, ""isolate the [contribution] of both""",,3,4,Poster,4,5,3,4,4,5,2,2,2016,"Our approach does not use any training or unlabeled data. We have used the English treebank during development to assess the contribution of individual head rules, and to tune PageRank parameters (Sec. 3.1) and function-word directionality (Sec. 3.2). Adposition direction is calculated on the fly on test data. In the following, we refer to our UD parser as UDP.Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), but our system fares best strictly using the dependency rules in Table 1 to build the graph.
We build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB. This strategy does not always yield connected graphs, and we use a teleport probability of 0.05 to ensure PR convergence. We chose this
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
value incrementally in intervals of 0.01 during development until we found the smallest value that guaranteed PR convergence. A high teleport probability is undesirable, because the resulting stationary distribution can be almost uniform. We did not have to re-adjust this value when running on the actual test data.
The the main idea behind our personalized PR approach is the observation that ranking is only relevant for content words.2 PR can incorporate a priori knowledge of the relevance of nodes by means of personalization, namely giving more weight to certain nodes. Intuitively, the higher the rank of a word, the closer it should be to the root node, i.e., the main predicate of the sentence is the node that should have the highest PR, making it the dependent of the root node (Fig. 1, lines 4-5). We use PR personalization to give 5 times more weight (over an otherwise uniform distribution) to the node that is estimated to be main predicate, i.e., the first verb or the first content word if there are no verbs.Head direction is an important syntactic trait. Indeed, the UD feature inventory contains a trait to distinguish adposition between pre- and postpositions. Instead of relying on this feature from the treebanks, which is not always provided, we estimate the frequency of ADP-NOMINAL vs. NOMINAL-ADP bigrams.3 This estimation requires very few examples to converge (10-15 sentences), and we calculate it directly on test data.
If a language has more ADP-NOMINAL bigrams, we consider all its ADP to be prepositions (and thus dependent of elements at their right). Otherwise, we consider them to be postpositions.
For other function words, we have determined on the English dev data whether to make them strictly right- or left-attaching, or to allow either direction: AUX, DET, and SCONJ are right-attaching, while CONJ and PUNCT are leftattaching. There are no direction constraints for the rest.Fig. 1 shows the tree-decoding algorithm. It has two blocks, namely a first block (3-11) where we assign the head of content words according to
2ADJ, NOUN, PROPN, and VERB mark content words. 3NOMINAL= {NOUN, PROPN, PRON}
1: H = ∅; D = ∅ 2: C = 〈c1, ...cm〉; F = 〈f1, ...fm〉 3: for c ∈ C do 4: if |H| = 0 then 5: h = root 6: else 7: h =argminj∈H {γ(j, c) | δ(j, c)∧κ(j, c)} 8: end if 9: H = H ∪ {c} 10: D = D ∪ {(h, c)} 11: end for 12: for f ∈ F do 13: h =argminj∈H {γ(j, f) | δ(j, f) ∧ κ(j, f)} 14: D = D ∪ {(h, f)} 15: end for 16: return D
Figure 1: Two-step decoding algorithm for UDP.
ADJ −→ ADV VERB −→ ADV, AUX, NOUN, PROPN, PRON, SCONJ NOUN, PROPN −→ ADP, DET, NUM NOUN, PROPN −→ ADJ, NOUN, PROPN
Table 1: UD dependency rules
their PageRank and the constraints of the dependency rules, and a second block (12-15) where we assign the head of function words according to their proximity, direction of attachment, and dependency rules. The algorithm requires:
1. The PR-sorted list of content words C. 2. The set of function words F . 3. A set H for the current possible heads, and
a set D for the dependencies assigned at each iteration, which we represent as headdependent tuples (h, d). 4. A symbol root for the root node. 5. A function γ(n,m) that gives the linear dis-
tance between two nodes. 6. A function κ(h, d) that returns whether the
dependency (h, d) has a valid attachment direction given the POS of the d (cf. Sec. 3.2). 7. A function δ(h, d) that determines whether (h, d) is licensed by the rules in Table 1.
The head assignations in lines 7 and 13 read as follow: the head h of a word (either c or f ) is the closest element of the current list of heads (H) that has the right direction (κ) and respects the POSdependency rules (δ). These assignations have a back-off option to ensure the final D is a tree. If the conditions determined by κ and δ are too strict,
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
357
358
359
360
361
362
363
364
365
366
367
368
369
378
379
380
381
382
383
384
385
386
392
393
394
395
396
397
398
399
i.e. if the set of possible heads is empty, we drop the δ head-rule constraint and recalculate the closest possible head that respects the directionality imposed by κ. If the set is empty again, we drop both constraints and assign the closest head.
Lines 4 and 5 enforce the single-root constraint. To enforce the leaf status of function nodes, the algorithm first attaches all content words (C), and then all function words (F ) in the second block where H is not updated, thereby ensuring leafness for all f ∈ F . The order of head attachment is not monotonic wrt. PR between the first and second block, and can yield non-projectivities. Nevertheless, it still is a one-pass algorithm. Decoding runs in less than O(n2), namely O(n× |C|). However, running PR incurs the main computation cost.This section exemplifies a full run of UDP for the example sentence “They also had a special connection to some extremists”, an actual clause from the English test data.Given an input sentence and its POS tags, we obtain rank of each word by building a graph using head rules and running PR on it. Table 2 provides the sentence, the POS of each word, the number of incoming edges for each word after building the graph with the head rules from Sec. 3.1, and the personalization vector for PR on this sentence. Note that all nodes have the same personalization weight, except the estimated main predicate, the verb “had”.
Word: They also had a special connection to some extremists POS: PRON ADV VERB DET ADJ NOUN ADP DET NOUN
Personalization: 1 1 5 1 1 1 1 1 1 Incoming edges: 0 0 4 0 1 5 0 0 5
Table 2: Words, POS, Personalization and incoming edges for the example sentence.
Table 3 shows the directed multigraph used for PR in detail. We can see e.g. that the four incoming edges for the verb “come” from the two nouns, plus from the adverb “also” and the pronoun “They”.
After running PR, we obtain the following ranking for content words: C = 〈had,connection,extremists,special〉 Even though the verb has four incoming edges and the nouns have six each, the personalization makes the verb the highest-ranked word.Once C is calculated, we can follow the algorithm in Fig. 1 to obtain a dependency parse. Table 4 shows a trace of the algorithm, given C and F : C = 〈had,connection,extremists,special〉 F = {They, also, a, to, some}
The first four iterations calculate the head of content words following their PR, and the following iterations attach the function words in F .
Finally, Fig. 2 shows the resulting dependency tree. Full lines are assigned in the first block (content dependents), dotted lines are assigned in the second block (function dependents). The edge labels indicate in which iteration the algorithm has assigned each dependency.
Note that the algorithm is deterministic for a certain input POS sequence. Any 10-token sentence with the POS labels shown in Table 2 would yield the same dependency tree.4
4The resulting trees always pass the validation script in
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
Language BLG UDPG MSDG MSDP UDPP UDPN
Ancient Greek 42.2 L 43.4 48.6 46.5 41.6 27.0 Arabic 34.8 R 47.8 52.8 52.6 47.6 41.0 Basque 47.8 R 45.0 51.2 49.3 43.1 22.8
Bulgarian 54.9 R 70.5 78.7 76.6 68.1 27.1 Church Slavonic 53.8 L 59.2 61.8 59.8 59.2 35.2
Croatian 41.6 L 56.7 69.1 65.6 54.5 25.2 Czech 46.5 R 61.0 69.5 67.6 59.3 25.3
Danish 47.3 R 57.9 70.2 65.6 53.8 26.9 Dutch 36.1 L 49.5 57.0 59.2 50.0 24.1
English 46.2 R 53.0 62.1 59.9 51.4 27.9 Estonian 73.2 R 70.0 73.4 66.1 65.0 25.3 Finnish 43.8 R 45.1 52.9 50.4 43.1 21.6 French 47.1 R 64.5 72.7 70.6 62.1 36.3 German 48.2 R 60.6 66.9 62.5 57.0 24.2 Gothic 50.2 L 57.5 61.7 59.2 55.8 34.1 Greek 45.7 R 58.5 68.0 66.4 57.0 29.3
Hebrew 41.8 R 55.4 62.0 58.6 52.8 35.7 Hindi 43.9 R 46.3 34.6 34.5 45.7 27.0 Hungarian 53.1 R 56.7 58.4 56.8 54.8 22.7 Indonesian 44.6 L 60.6 63.6 61.0 58.4 35.3
Irish 47.5 R 56.6 62.5 61.3 53.9 35.8 Italian 50.6 R 69.4 77.1 75.2 67.9 37.6 Latin 49.4 L 56.2 59.8 54.9 52.4 37.1 Norwegian 49.1 R 61.7 70.8 67.3 58.6 29.8 Persian 37.8 L 55.7 57.8 55.6 53.6 33.9 Polish 60.8 R 68.4 75.6 71.7 65.7 34.6 Portuguese 45.8 R 65.7 72.8 71.4 64.9 33.5 Romanian 52.7 R 63.7 69.2 64.0 58.9 32.1
Slovene 50.6 R 63.6 74.7 71.0 56.0 24.3 Spanish 48.2 R 63.9 72.9 70.7 62.1 35.0 Swedish 52.4 R 62.8 72.2 67.2 58.5 25.3
Tamil 41.4 R 34.2 44.2 39.5 32.1 20.3
Average 47.8 57.5 63.9 61.2 55.3 29.9
Table 5: UAS for baseline with gold POS (BLG) with direction (L/R) for backoff attachments, UDP with gold POS (UDPG) and predicted POS (UDPP ), PR with naive content-function POS (UDPN ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP , respectively). BL values higher than UDPG are underlined, and UDPG values higher than MSDG are in boldface.This section describes the data and metrics used to assess the performance of UDP, as well as the systems we compare against. We evaluate on the test sections of the UD1.2 treebanks (Nivre et al., 2015) that contain word forms. If there is more than one treebank per language, we use the treebank that has the canonical language name (e.g. Finnish instead of Finnish-FTB).
It is common to evaluate unsupervised dependency parsing using alternative metrics like undirected attachment score or neutralized edge direction, or to evaluate short sentences separately (Schwartz et al., 2011; Gelling et al., 2012). In contrast, we use standard unlabeled attachment score (UAS) and evaluate on all sentences of the
github.com/UniversalDependencies/tools
canonical UD test sets.We compare our UDP system with the performance of a rule-based baseline that uses the head rules in Table 5. The baseline identifies the first verb (or first content word if there are no verbs) as the main predicate, and assigns heads to all words according to the rules in Table 1.
We have selected the set of head rules to maximize precision on the development set, and they do not provide full coverage. The system makes any word not covered by the rules (e.g., a word with a POS such as X or SYM) either dependent of their left or right neighbor, depending on the estimated runtime parameter.
We report the best head direction and its score for each language in Table 5. This baseline finds the head of each token based on its closest possible head, or on its immediate left or right neighbor if there is no head rule for the POS at hand, which means that this system does not necessarily yield well-formed tress. Each token receives a head, and while the structures are single-rooted, they are not necessarily connected.
Note that we do not include results for the DMV model by Klein and Manning (2004), as it has been outperformed by a system similar to ours (Søgaard, 2012b). The usual adjacency baseline for unsupervised dependency parsing, where all words depend on their left or right neighbor, fares much worse than our baseline (20% UAS below on average) even when we make an oracle pick for the best per-language direction. Therefore we do not report those scores.The performance of UDP depends on PageRank to score content words, and on two-step decoding to ensure the leaf status of function words. In this section we isolate the constribution of both parts. We do so by comparing the performance of BL, UDP, and UDPNoPR, a version of UDP where we disable PR and rank content words according to their reading order, i.e. the first word in the ranking is the first word to be read, regardless of the specific language’s script direction
The baseline BL described in 5.1 already ensures function words are leaf nodes, because they have no listed dependent POS in the head rules. The task of the decoding steps is mainly to ensure the resulting structures are well-formed dependency trees.
However, if we measure the difference between UDPNoPR and BL, we observe that UDPNoPR contributes with 4 UAS points on average over the baseline. Nevertheless, the baseline is oracleinformed about the language’s best branching direction, a property that UDP does not have. Instead, the decoding step determines head direction as described in Section 3.2.
Complementary, we can measure the contribution of PR by observing the difference between regular UDP and UDPNoPR. The latter scores on average 9 UAS points lower than UDP. These 9 points are strictly determined by the better attachment of content words.UD is a constantly-improving effort, and not all v1.2 treebanks have the same level of formalism compliance. Thus, the interpretation of, e.g., the AUX-VERB or DET-PRON distinctions might differ. However, we do not incorporate these differences in our analysis and consider all treebanks equally compliant.
The root accuracy scores oscillate around an average of 69%, with Arabic and Tamil (26%) and Estonian (93%) as outliers. Given the PR personalization (Sec. 3.1), UDP has a strong bias for chosing the first verb as main predicate. However, without personalization, performance drops 2% on average. This improvement is consistent even for verb-final languages like Hindi. Moreover, our personalization strategy makes PR converge a whole order of magnitude faster.
The bigram heuristic to determine adposition
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Language BLG MSDG UDPG MSDP UDPP
Bulgarian 50.1±2.4 73.5 ±3.5 69.7±1.8 71.3±3.3 66.9±3.2 Croatian+Serbian 42.1±0.7 66.0±3.0 57.8±1.4 62.1±3.0 54.4 ±2.0
English 42.2±2.8 60.1±6.2 53.9±2.5 57.3±4.3 52.0 ±3.3 Italian 50.3±1.2 70.0±5.4 70.1±3.3 68.1±6.0 68.7±3.9
Average Std. 1.8 4.5 2.5 4.2 3.1
Table 6: Average language-wise domain evaluation. We report average UAS and standard deviation per language. The bottom row provides the average standard deviation for each system.
direction succeeds at identifying the predominant pre- or postposition preference for all languages (average ADP UAS of 75%). The fixed direction for the other functional POS is largely effective, with few exceptions, e.g., DET is consistently right-attaching on all treebanks except Basque (average overall DET UAS of 84%, 32% for Basque). These alternations could also be estimated from the data in a manner similar to ADP. Our rules do not make nouns eligible heads for verbs. As a result, the system cannot infer relative clauses. We have excluded the NOUN → VERB head rule during development because it makes the hierarchical relation between verbs and nouns less conclusive.
We have not excluded punctuation from the evaluation. Indeed, the UAS for the PUNCT is low (an average of 21%, standard deviation of 9.6), even lower than the otherwise problematic CONJ. Even though conjunctions are pervasive and identifying their scope is one of the usual challenges for parsers, the average UAS for CONJ is much larger (an average of 38%, standard deviation of 13.5) than for PUNCT. Both POS show large standard deviations, which indicates great variability. This variability can be caused by linguistic properties of the languages or evaluation datasets, but also by differences in annotation convention.Models with fewer parameters are less likely to be overfit for a certain dataset. In our case, a system with few, general rules is less likely to make attachment decisions that are very particular of a certain language or dataset. Plank and van Noord (2010) have shown that rule-based parsers can be more stable to domain shift. We explore if their finding holds for UDP as well, by testing on i) the UD development data as a readily available proxy for domain shift, and ii) manually curated domain splits of select UD test sets.
Language Domain BLG MSDG UDPG MSDP UDPP Bulgarian bulletin 48.3 67.5 67.4 65.4 61.5 legal 47.9 76.9 69.2 73.0 68.6 literature 53.6 74.2 69.0 72.8 66.6 news 49.3 74.6 70.2 73.0 68.2 various 51.4 74.2 72.5 72.6 69.5
Croatian news 41.2 62.4 57.9 61.8 52.2 wiki 41.9 64.8 55.8 58.2 56.3
English answers 44.1 61.6 55.9 59.5 53.7 email 42.8 58.8 52.1 57.1 56.3 newsgroup 41.7 55.5 49.7 52.9 51.1 reviews 47.4 66.8 54.9 63.9 52.2 weblog 43.3 51.6 50.9 49.8 53.8 magazine† 41.4 60.9 55.6 58.4 53.3 bible† 38.4 56.2 56.2 56.8 48.6 questions† 38.7 69.7 55.6 60.5 47.2
Italian europarl 50.8 64.1 70.6 62.7 69.7 legal 51.1 67.9 69.0 64.4 67.2 news 49.4 68.9 67.5 67.0 65.3 questions 48.7 80.0 77.0 79.1 76.1 various 49.7 67.8 69.0 65.3 67.6 wiki 51.8 71.2 68.1 70.3 66.6
Serbian news 42.8 68.0 58.8 65.6 53.3 wiki 42.4 68.9 58.8 62.8 55.8
Table 7: Evaluation across domains. UAS for baseline with gold POS (BLG), UDP with gold POS (UDPG) and predicted POS (UDPP ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP ). English datasets marked with † are in-house annotated. Lowest results per language underlined. Bold: UDP outperforms MSD.
Development sets We have used the English development data to choose which relations would be included as head rules in the final system (Cf. Table 1). It would be possible that some of the rules are indeed more befitting for the English data or for that particular section.
However, if we regard the results for UDPG in Table 5, we can see that there are 24 languages (out of 32) for which the parser performs better than for English. This result indicates that the head rules are general enough to provide reasonable parses for languages other than the one chosen for development.
If we run UDPG on the development sections for the other languages, we find the results are very consistent. Any language scores on average ±1 UAS with regards to the test section. There is no clear tendency for either section being easier to parse with our system.
Cross-domain test sets To further assess the cross-domain robustness, we retrieved the domain (genre) splits6 from the test sections of the UD
6The data splits are freely available at http://ANONYMIZED
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
treebanks where the domain information is available as sentence metadata: from Bulgarian, Croatian, and Italian. We also include a UD-compliant Serbian dataset which is not included in the UD release but which is based on the same parallel corpus as Croatian and has the same domain splits (Agić and Ljubešić, 2015). When averaging we pool Croatian and Serbian together as they come from the same dataset. Also, we use a tagger trained on the Croatian UD training data for tagging Serbian.
For English, we have obtained the data splits in the test section matching the sentences from the original distribution of the English Web Treebank. In addition to these already available data sets, we have annotated three different datasets to asses domain variation more extensively, namely the first 50 verses of the King James Bible, 50 sentences from a magazine, and 75 sentences from the test split in QuestionBank (Judge et al., 2006). We include the third dataset to evaluate strictly on questions, which we could do already in Italian. While the answers domain in English is made up of text from the Yahoo! Answers forum, only one fourth of the sentences are questions. Note these three small datasets are not included in the results on the canonical test sections in Table 5.7
Table 6 summarizes the per-language average score and standard deviation, as well as the macroaveraged standard deviation across languages. UDP has a much lower standard deviation across domains compared to MSD. This holds across languages. We attribute this higher stability to UDP being developed to satisfy a set of general properties of the UD syntactic formalism, instead of being a data-driven method more sensitive to sampling bias. This holds for both the gold-POS and predicted-POS setup. The differences in standard deviation are unsurprisingly smaller in the predicted POS setup. In general, the rule-based UPD is less sensitive to domain shifts than the datadriven MSD counterpart, confirming earlier findings (Plank and van Noord, 2010).
Table 7 gives the detailed scores per language and domain. From the scores we can see that presidental bulletin, legal and weblogs are amongst the hardest domains to parse. However, the systems often do not agree on which domain is hardest, with the exception of Bulgarian
7The three in-house annotated datasets are freely available at http://ANONYMIZED
bulletin. More importantly, this might even change between gold and predicted POS, highlighting the importance of evaluating systems beyond gold POS. Interestingly, for the Italian data and some of the hardest domains UDP outperforms MSD, confirming that it is a robust baseline.","Our system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word’s frequency. Words that belong to the 100 most frequent word types of the input test section receive the FUNCTION tag.
Finally, we compare our system to a supervised cross-lingual system (MSD). It is a multi-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
source delexicalized transfer parser, referred to as multi-dir in the original paper by McDonald et al. (2011). For this baseline we train TurboParser (Martins et al., 2013) on a delexicalized training set of 20k sentences, sampled uniformly from the UD training data excluding the target language. MSD is a competitive baseline in crosslingual transfer parsing work. This gives us an indication how our system compares to standard cross-lingual parsers.Table 5 shows that UDP is a competitive system; because UDPG is remarkably close to the supervised MSDG system, with an average difference of 6.4%, even outperforming MSDG on one language (Hindi).
More interestingly, on the evaluation scenario with predicted POS we observe that our system drops only marginally (2.2%) compared to MSD (2.7%). In the least robust rule-based setup, the error propagation rate from POS to dependency would be doubled, as either a wrongly tagged head or dependent would break the dependency rules. However, with an average POS accuracy by TnT of 94.1%, the error propagation is 0.37, i.e each POS error causes 0.37 additional dependency errors. In contrast, for the MSD system this error propagation is 0.46, thus higher.5
For the extreme POS scenario, content vs. function POS (CF), the drop in performance for UDP is however very large. But this might be a too crude evaluation setup. Nevertheless, UDP, the simple unsupervised system with PageRank, outperforms the adjacency baselines (BL) by 4% on average on the two type-based naive POS tag scenario. This difference indicates that even with very deficient POS tags, UDP can provide better structures.","Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns. Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences.
The Universal Dependencies (UD) project (Nivre et al., 2015) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for cross-lingual dependency parsing research (McDonald et al., 2013). Furthermore, we expect that such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach.
Our system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In par-
ticular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014).
Contributions Our method goes beyond the existing work on rule-aided unsupervised dependency parsing by a) adapting dependency head rules to UD-compliant POS relations, b) incorporating the UD restriction of function words being leaves, c) using personalized PageRank to improve main predicate identification, and d) making it completely free of language-specific parameters by estimating adposition attachment direction directly on test data.
We evaluate our system on 32 languages1 in three setups, depending on the reliability of available POS tags, and compare to a multi-source delexicalized transfer system. In addition, we evaluate the systems’ sensitivity to domain change for a subset of UD languages for which domain information was retrievable. The results expose a solid and competitive system for all UD languages. Our unsupervised parser compares favorably to delexicalized parsing, while being more robust to domain change across languages.","Over the recent years, cross-lingual linguistic structure prediction based on model transfer or projection of POS tags and dependency trees has become a relevant line of work (Das and Petrov, 2011; McDonald et al., 2011). These works mostly use supervised learning and different target language adaptation techniques.
1Out of 33 languages in UD v1.2. We exclude Japanese because the treebank is distributed without word forms and hence we can not provide results on predicted POS.
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015).
The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extend delexicalization to involve multiple sourceside parsers. This line of work depends on applying uniform POS and dependency representations (McDonald et al., 2013).
Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich Indo-European languages. Agić et al. (2015) expose some of the biases in their proposal for realistic cross-lingual POS tagging, as they emphasize the lack of perfect sentence and word splitting for truly low-resource languages. Johannsen et al. (2016) introduce joint projection of POS and dependencies from multiple sources while sharing the outlook on bias removal in real-world multilingual processing.
Cross-lingual learning, realistic or not, depends entirely on the availability of data: for the sources, for the targets, or most often for both sets of languages. Moreover, it typically does not exploit the constraints placed on the linguistic structures through the formalism, and it does so by design. With the emergence of UD as the practical standard for cross-language annotation of POS and syntactic dependencies, we argue for an approach that takes a fresh angle on both aspects. Namely, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploit-
ing the UD constraints on building POS and dependency annotations.
These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of unsupervised parsers falls far behind the approaches involving any sort of supervision.
Our work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b). In particular, we make use of Søgaard’s (2012b) PageRank method to rank words before decoding. Our system, however, has two key differences: i) the usage of PageRank personalization, and of ii) two-step decoding to treat content and function words differently according to the UD formalism. Through these differences, even without any training data, we parse nearly as well as a delexicalized transfer parser, and with increased stability to domain change.","In this section we provide a further error analysis of the UDP parser. We examine the contribution to the overal results of using PageRank to score content words, the behavior of the system across different parts of speech, and we assess the robustness of UDP when parsing text from different domains.
5Err. prop. = (E(ParseP )−E(ParseG))/E(POSP ), where E(x) = 1−Accuracy(x).","We have presented UDP, an unsupervised dependency parser for Universal Dependencies that makes use of personalized PageRank and a small set of head-dependent rules. The parser requires no training data and estimates adpositon direction directly from test data. We achieve competitive performance on all but two UD languages, and even beat a multi-source delexicalized parser (MSD) on Hindi. We evaluated the parser on three POS setups and across domains. Our results show that UDP is less affected by deteriorating POS tags than MSD, and is more resilient to domain changes. Both the parser and the in-domain annotated test sets are freely available.8
Further work includes extending the parser to handle multiword expressions, coordination, and proper names. Moreover, our usage of PR could be expanded to directly score the potential dependency edges—e.g., by means of edge reification— instead of words. Finally, we only considered unlabeled attachment, however, our system could easily be augmented with partial edge labeling."
37,"We present UDP, an unsupervised parsing algorithm for Universal Dependencies (UD) based on  PageRank and a small set of specific dependency head rules. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual UD parsing. It is distinctly robust to domain change across languages.",Parsing for Universal Dependencies without training,151,"The authors proposed an unsupervised algorithm for Universal Dependencies that
does not require training. The tagging is based on PageRank for the words and a
small amount of hard-coded rules.
The article is well written, very detailed and the intuition behind all prior
information being added to the model is explained clearly.
I think that the contribution is substantial to the field of unsupervised
parsing, and the possibilities for future work presented by the authors give
rise to additional research.",,4,5,Oral Presentation,5,5,5,4,4,5,4,4,2016,"Our approach does not use any training or unlabeled data. We have used the English treebank during development to assess the contribution of individual head rules, and to tune PageRank parameters (Sec. 3.1) and function-word directionality (Sec. 3.2). Adposition direction is calculated on the fly on test data. In the following, we refer to our UD parser as UDP.Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), but our system fares best strictly using the dependency rules in Table 1 to build the graph.
We build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB. This strategy does not always yield connected graphs, and we use a teleport probability of 0.05 to ensure PR convergence. We chose this
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
value incrementally in intervals of 0.01 during development until we found the smallest value that guaranteed PR convergence. A high teleport probability is undesirable, because the resulting stationary distribution can be almost uniform. We did not have to re-adjust this value when running on the actual test data.
The the main idea behind our personalized PR approach is the observation that ranking is only relevant for content words.2 PR can incorporate a priori knowledge of the relevance of nodes by means of personalization, namely giving more weight to certain nodes. Intuitively, the higher the rank of a word, the closer it should be to the root node, i.e., the main predicate of the sentence is the node that should have the highest PR, making it the dependent of the root node (Fig. 1, lines 4-5). We use PR personalization to give 5 times more weight (over an otherwise uniform distribution) to the node that is estimated to be main predicate, i.e., the first verb or the first content word if there are no verbs.Head direction is an important syntactic trait. Indeed, the UD feature inventory contains a trait to distinguish adposition between pre- and postpositions. Instead of relying on this feature from the treebanks, which is not always provided, we estimate the frequency of ADP-NOMINAL vs. NOMINAL-ADP bigrams.3 This estimation requires very few examples to converge (10-15 sentences), and we calculate it directly on test data.
If a language has more ADP-NOMINAL bigrams, we consider all its ADP to be prepositions (and thus dependent of elements at their right). Otherwise, we consider them to be postpositions.
For other function words, we have determined on the English dev data whether to make them strictly right- or left-attaching, or to allow either direction: AUX, DET, and SCONJ are right-attaching, while CONJ and PUNCT are leftattaching. There are no direction constraints for the rest.Fig. 1 shows the tree-decoding algorithm. It has two blocks, namely a first block (3-11) where we assign the head of content words according to
2ADJ, NOUN, PROPN, and VERB mark content words. 3NOMINAL= {NOUN, PROPN, PRON}
1: H = ∅; D = ∅ 2: C = 〈c1, ...cm〉; F = 〈f1, ...fm〉 3: for c ∈ C do 4: if |H| = 0 then 5: h = root 6: else 7: h =argminj∈H {γ(j, c) | δ(j, c)∧κ(j, c)} 8: end if 9: H = H ∪ {c} 10: D = D ∪ {(h, c)} 11: end for 12: for f ∈ F do 13: h =argminj∈H {γ(j, f) | δ(j, f) ∧ κ(j, f)} 14: D = D ∪ {(h, f)} 15: end for 16: return D
Figure 1: Two-step decoding algorithm for UDP.
ADJ −→ ADV VERB −→ ADV, AUX, NOUN, PROPN, PRON, SCONJ NOUN, PROPN −→ ADP, DET, NUM NOUN, PROPN −→ ADJ, NOUN, PROPN
Table 1: UD dependency rules
their PageRank and the constraints of the dependency rules, and a second block (12-15) where we assign the head of function words according to their proximity, direction of attachment, and dependency rules. The algorithm requires:
1. The PR-sorted list of content words C. 2. The set of function words F . 3. A set H for the current possible heads, and
a set D for the dependencies assigned at each iteration, which we represent as headdependent tuples (h, d). 4. A symbol root for the root node. 5. A function γ(n,m) that gives the linear dis-
tance between two nodes. 6. A function κ(h, d) that returns whether the
dependency (h, d) has a valid attachment direction given the POS of the d (cf. Sec. 3.2). 7. A function δ(h, d) that determines whether (h, d) is licensed by the rules in Table 1.
The head assignations in lines 7 and 13 read as follow: the head h of a word (either c or f ) is the closest element of the current list of heads (H) that has the right direction (κ) and respects the POSdependency rules (δ). These assignations have a back-off option to ensure the final D is a tree. If the conditions determined by κ and δ are too strict,
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
357
358
359
360
361
362
363
364
365
366
367
368
369
378
379
380
381
382
383
384
385
386
392
393
394
395
396
397
398
399
i.e. if the set of possible heads is empty, we drop the δ head-rule constraint and recalculate the closest possible head that respects the directionality imposed by κ. If the set is empty again, we drop both constraints and assign the closest head.
Lines 4 and 5 enforce the single-root constraint. To enforce the leaf status of function nodes, the algorithm first attaches all content words (C), and then all function words (F ) in the second block where H is not updated, thereby ensuring leafness for all f ∈ F . The order of head attachment is not monotonic wrt. PR between the first and second block, and can yield non-projectivities. Nevertheless, it still is a one-pass algorithm. Decoding runs in less than O(n2), namely O(n× |C|). However, running PR incurs the main computation cost.This section exemplifies a full run of UDP for the example sentence “They also had a special connection to some extremists”, an actual clause from the English test data.Given an input sentence and its POS tags, we obtain rank of each word by building a graph using head rules and running PR on it. Table 2 provides the sentence, the POS of each word, the number of incoming edges for each word after building the graph with the head rules from Sec. 3.1, and the personalization vector for PR on this sentence. Note that all nodes have the same personalization weight, except the estimated main predicate, the verb “had”.
Word: They also had a special connection to some extremists POS: PRON ADV VERB DET ADJ NOUN ADP DET NOUN
Personalization: 1 1 5 1 1 1 1 1 1 Incoming edges: 0 0 4 0 1 5 0 0 5
Table 2: Words, POS, Personalization and incoming edges for the example sentence.
Table 3 shows the directed multigraph used for PR in detail. We can see e.g. that the four incoming edges for the verb “come” from the two nouns, plus from the adverb “also” and the pronoun “They”.
After running PR, we obtain the following ranking for content words: C = 〈had,connection,extremists,special〉 Even though the verb has four incoming edges and the nouns have six each, the personalization makes the verb the highest-ranked word.Once C is calculated, we can follow the algorithm in Fig. 1 to obtain a dependency parse. Table 4 shows a trace of the algorithm, given C and F : C = 〈had,connection,extremists,special〉 F = {They, also, a, to, some}
The first four iterations calculate the head of content words following their PR, and the following iterations attach the function words in F .
Finally, Fig. 2 shows the resulting dependency tree. Full lines are assigned in the first block (content dependents), dotted lines are assigned in the second block (function dependents). The edge labels indicate in which iteration the algorithm has assigned each dependency.
Note that the algorithm is deterministic for a certain input POS sequence. Any 10-token sentence with the POS labels shown in Table 2 would yield the same dependency tree.4
4The resulting trees always pass the validation script in
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
Language BLG UDPG MSDG MSDP UDPP UDPN
Ancient Greek 42.2 L 43.4 48.6 46.5 41.6 27.0 Arabic 34.8 R 47.8 52.8 52.6 47.6 41.0 Basque 47.8 R 45.0 51.2 49.3 43.1 22.8
Bulgarian 54.9 R 70.5 78.7 76.6 68.1 27.1 Church Slavonic 53.8 L 59.2 61.8 59.8 59.2 35.2
Croatian 41.6 L 56.7 69.1 65.6 54.5 25.2 Czech 46.5 R 61.0 69.5 67.6 59.3 25.3
Danish 47.3 R 57.9 70.2 65.6 53.8 26.9 Dutch 36.1 L 49.5 57.0 59.2 50.0 24.1
English 46.2 R 53.0 62.1 59.9 51.4 27.9 Estonian 73.2 R 70.0 73.4 66.1 65.0 25.3 Finnish 43.8 R 45.1 52.9 50.4 43.1 21.6 French 47.1 R 64.5 72.7 70.6 62.1 36.3 German 48.2 R 60.6 66.9 62.5 57.0 24.2 Gothic 50.2 L 57.5 61.7 59.2 55.8 34.1 Greek 45.7 R 58.5 68.0 66.4 57.0 29.3
Hebrew 41.8 R 55.4 62.0 58.6 52.8 35.7 Hindi 43.9 R 46.3 34.6 34.5 45.7 27.0 Hungarian 53.1 R 56.7 58.4 56.8 54.8 22.7 Indonesian 44.6 L 60.6 63.6 61.0 58.4 35.3
Irish 47.5 R 56.6 62.5 61.3 53.9 35.8 Italian 50.6 R 69.4 77.1 75.2 67.9 37.6 Latin 49.4 L 56.2 59.8 54.9 52.4 37.1 Norwegian 49.1 R 61.7 70.8 67.3 58.6 29.8 Persian 37.8 L 55.7 57.8 55.6 53.6 33.9 Polish 60.8 R 68.4 75.6 71.7 65.7 34.6 Portuguese 45.8 R 65.7 72.8 71.4 64.9 33.5 Romanian 52.7 R 63.7 69.2 64.0 58.9 32.1
Slovene 50.6 R 63.6 74.7 71.0 56.0 24.3 Spanish 48.2 R 63.9 72.9 70.7 62.1 35.0 Swedish 52.4 R 62.8 72.2 67.2 58.5 25.3
Tamil 41.4 R 34.2 44.2 39.5 32.1 20.3
Average 47.8 57.5 63.9 61.2 55.3 29.9
Table 5: UAS for baseline with gold POS (BLG) with direction (L/R) for backoff attachments, UDP with gold POS (UDPG) and predicted POS (UDPP ), PR with naive content-function POS (UDPN ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP , respectively). BL values higher than UDPG are underlined, and UDPG values higher than MSDG are in boldface.This section describes the data and metrics used to assess the performance of UDP, as well as the systems we compare against. We evaluate on the test sections of the UD1.2 treebanks (Nivre et al., 2015) that contain word forms. If there is more than one treebank per language, we use the treebank that has the canonical language name (e.g. Finnish instead of Finnish-FTB).
It is common to evaluate unsupervised dependency parsing using alternative metrics like undirected attachment score or neutralized edge direction, or to evaluate short sentences separately (Schwartz et al., 2011; Gelling et al., 2012). In contrast, we use standard unlabeled attachment score (UAS) and evaluate on all sentences of the
github.com/UniversalDependencies/tools
canonical UD test sets.We compare our UDP system with the performance of a rule-based baseline that uses the head rules in Table 5. The baseline identifies the first verb (or first content word if there are no verbs) as the main predicate, and assigns heads to all words according to the rules in Table 1.
We have selected the set of head rules to maximize precision on the development set, and they do not provide full coverage. The system makes any word not covered by the rules (e.g., a word with a POS such as X or SYM) either dependent of their left or right neighbor, depending on the estimated runtime parameter.
We report the best head direction and its score for each language in Table 5. This baseline finds the head of each token based on its closest possible head, or on its immediate left or right neighbor if there is no head rule for the POS at hand, which means that this system does not necessarily yield well-formed tress. Each token receives a head, and while the structures are single-rooted, they are not necessarily connected.
Note that we do not include results for the DMV model by Klein and Manning (2004), as it has been outperformed by a system similar to ours (Søgaard, 2012b). The usual adjacency baseline for unsupervised dependency parsing, where all words depend on their left or right neighbor, fares much worse than our baseline (20% UAS below on average) even when we make an oracle pick for the best per-language direction. Therefore we do not report those scores.The performance of UDP depends on PageRank to score content words, and on two-step decoding to ensure the leaf status of function words. In this section we isolate the constribution of both parts. We do so by comparing the performance of BL, UDP, and UDPNoPR, a version of UDP where we disable PR and rank content words according to their reading order, i.e. the first word in the ranking is the first word to be read, regardless of the specific language’s script direction
The baseline BL described in 5.1 already ensures function words are leaf nodes, because they have no listed dependent POS in the head rules. The task of the decoding steps is mainly to ensure the resulting structures are well-formed dependency trees.
However, if we measure the difference between UDPNoPR and BL, we observe that UDPNoPR contributes with 4 UAS points on average over the baseline. Nevertheless, the baseline is oracleinformed about the language’s best branching direction, a property that UDP does not have. Instead, the decoding step determines head direction as described in Section 3.2.
Complementary, we can measure the contribution of PR by observing the difference between regular UDP and UDPNoPR. The latter scores on average 9 UAS points lower than UDP. These 9 points are strictly determined by the better attachment of content words.UD is a constantly-improving effort, and not all v1.2 treebanks have the same level of formalism compliance. Thus, the interpretation of, e.g., the AUX-VERB or DET-PRON distinctions might differ. However, we do not incorporate these differences in our analysis and consider all treebanks equally compliant.
The root accuracy scores oscillate around an average of 69%, with Arabic and Tamil (26%) and Estonian (93%) as outliers. Given the PR personalization (Sec. 3.1), UDP has a strong bias for chosing the first verb as main predicate. However, without personalization, performance drops 2% on average. This improvement is consistent even for verb-final languages like Hindi. Moreover, our personalization strategy makes PR converge a whole order of magnitude faster.
The bigram heuristic to determine adposition
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Language BLG MSDG UDPG MSDP UDPP
Bulgarian 50.1±2.4 73.5 ±3.5 69.7±1.8 71.3±3.3 66.9±3.2 Croatian+Serbian 42.1±0.7 66.0±3.0 57.8±1.4 62.1±3.0 54.4 ±2.0
English 42.2±2.8 60.1±6.2 53.9±2.5 57.3±4.3 52.0 ±3.3 Italian 50.3±1.2 70.0±5.4 70.1±3.3 68.1±6.0 68.7±3.9
Average Std. 1.8 4.5 2.5 4.2 3.1
Table 6: Average language-wise domain evaluation. We report average UAS and standard deviation per language. The bottom row provides the average standard deviation for each system.
direction succeeds at identifying the predominant pre- or postposition preference for all languages (average ADP UAS of 75%). The fixed direction for the other functional POS is largely effective, with few exceptions, e.g., DET is consistently right-attaching on all treebanks except Basque (average overall DET UAS of 84%, 32% for Basque). These alternations could also be estimated from the data in a manner similar to ADP. Our rules do not make nouns eligible heads for verbs. As a result, the system cannot infer relative clauses. We have excluded the NOUN → VERB head rule during development because it makes the hierarchical relation between verbs and nouns less conclusive.
We have not excluded punctuation from the evaluation. Indeed, the UAS for the PUNCT is low (an average of 21%, standard deviation of 9.6), even lower than the otherwise problematic CONJ. Even though conjunctions are pervasive and identifying their scope is one of the usual challenges for parsers, the average UAS for CONJ is much larger (an average of 38%, standard deviation of 13.5) than for PUNCT. Both POS show large standard deviations, which indicates great variability. This variability can be caused by linguistic properties of the languages or evaluation datasets, but also by differences in annotation convention.Models with fewer parameters are less likely to be overfit for a certain dataset. In our case, a system with few, general rules is less likely to make attachment decisions that are very particular of a certain language or dataset. Plank and van Noord (2010) have shown that rule-based parsers can be more stable to domain shift. We explore if their finding holds for UDP as well, by testing on i) the UD development data as a readily available proxy for domain shift, and ii) manually curated domain splits of select UD test sets.
Language Domain BLG MSDG UDPG MSDP UDPP Bulgarian bulletin 48.3 67.5 67.4 65.4 61.5 legal 47.9 76.9 69.2 73.0 68.6 literature 53.6 74.2 69.0 72.8 66.6 news 49.3 74.6 70.2 73.0 68.2 various 51.4 74.2 72.5 72.6 69.5
Croatian news 41.2 62.4 57.9 61.8 52.2 wiki 41.9 64.8 55.8 58.2 56.3
English answers 44.1 61.6 55.9 59.5 53.7 email 42.8 58.8 52.1 57.1 56.3 newsgroup 41.7 55.5 49.7 52.9 51.1 reviews 47.4 66.8 54.9 63.9 52.2 weblog 43.3 51.6 50.9 49.8 53.8 magazine† 41.4 60.9 55.6 58.4 53.3 bible† 38.4 56.2 56.2 56.8 48.6 questions† 38.7 69.7 55.6 60.5 47.2
Italian europarl 50.8 64.1 70.6 62.7 69.7 legal 51.1 67.9 69.0 64.4 67.2 news 49.4 68.9 67.5 67.0 65.3 questions 48.7 80.0 77.0 79.1 76.1 various 49.7 67.8 69.0 65.3 67.6 wiki 51.8 71.2 68.1 70.3 66.6
Serbian news 42.8 68.0 58.8 65.6 53.3 wiki 42.4 68.9 58.8 62.8 55.8
Table 7: Evaluation across domains. UAS for baseline with gold POS (BLG), UDP with gold POS (UDPG) and predicted POS (UDPP ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP ). English datasets marked with † are in-house annotated. Lowest results per language underlined. Bold: UDP outperforms MSD.
Development sets We have used the English development data to choose which relations would be included as head rules in the final system (Cf. Table 1). It would be possible that some of the rules are indeed more befitting for the English data or for that particular section.
However, if we regard the results for UDPG in Table 5, we can see that there are 24 languages (out of 32) for which the parser performs better than for English. This result indicates that the head rules are general enough to provide reasonable parses for languages other than the one chosen for development.
If we run UDPG on the development sections for the other languages, we find the results are very consistent. Any language scores on average ±1 UAS with regards to the test section. There is no clear tendency for either section being easier to parse with our system.
Cross-domain test sets To further assess the cross-domain robustness, we retrieved the domain (genre) splits6 from the test sections of the UD
6The data splits are freely available at http://ANONYMIZED
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
treebanks where the domain information is available as sentence metadata: from Bulgarian, Croatian, and Italian. We also include a UD-compliant Serbian dataset which is not included in the UD release but which is based on the same parallel corpus as Croatian and has the same domain splits (Agić and Ljubešić, 2015). When averaging we pool Croatian and Serbian together as they come from the same dataset. Also, we use a tagger trained on the Croatian UD training data for tagging Serbian.
For English, we have obtained the data splits in the test section matching the sentences from the original distribution of the English Web Treebank. In addition to these already available data sets, we have annotated three different datasets to asses domain variation more extensively, namely the first 50 verses of the King James Bible, 50 sentences from a magazine, and 75 sentences from the test split in QuestionBank (Judge et al., 2006). We include the third dataset to evaluate strictly on questions, which we could do already in Italian. While the answers domain in English is made up of text from the Yahoo! Answers forum, only one fourth of the sentences are questions. Note these three small datasets are not included in the results on the canonical test sections in Table 5.7
Table 6 summarizes the per-language average score and standard deviation, as well as the macroaveraged standard deviation across languages. UDP has a much lower standard deviation across domains compared to MSD. This holds across languages. We attribute this higher stability to UDP being developed to satisfy a set of general properties of the UD syntactic formalism, instead of being a data-driven method more sensitive to sampling bias. This holds for both the gold-POS and predicted-POS setup. The differences in standard deviation are unsurprisingly smaller in the predicted POS setup. In general, the rule-based UPD is less sensitive to domain shifts than the datadriven MSD counterpart, confirming earlier findings (Plank and van Noord, 2010).
Table 7 gives the detailed scores per language and domain. From the scores we can see that presidental bulletin, legal and weblogs are amongst the hardest domains to parse. However, the systems often do not agree on which domain is hardest, with the exception of Bulgarian
7The three in-house annotated datasets are freely available at http://ANONYMIZED
bulletin. More importantly, this might even change between gold and predicted POS, highlighting the importance of evaluating systems beyond gold POS. Interestingly, for the Italian data and some of the hardest domains UDP outperforms MSD, confirming that it is a robust baseline.","Our system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word’s frequency. Words that belong to the 100 most frequent word types of the input test section receive the FUNCTION tag.
Finally, we compare our system to a supervised cross-lingual system (MSD). It is a multi-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
source delexicalized transfer parser, referred to as multi-dir in the original paper by McDonald et al. (2011). For this baseline we train TurboParser (Martins et al., 2013) on a delexicalized training set of 20k sentences, sampled uniformly from the UD training data excluding the target language. MSD is a competitive baseline in crosslingual transfer parsing work. This gives us an indication how our system compares to standard cross-lingual parsers.Table 5 shows that UDP is a competitive system; because UDPG is remarkably close to the supervised MSDG system, with an average difference of 6.4%, even outperforming MSDG on one language (Hindi).
More interestingly, on the evaluation scenario with predicted POS we observe that our system drops only marginally (2.2%) compared to MSD (2.7%). In the least robust rule-based setup, the error propagation rate from POS to dependency would be doubled, as either a wrongly tagged head or dependent would break the dependency rules. However, with an average POS accuracy by TnT of 94.1%, the error propagation is 0.37, i.e each POS error causes 0.37 additional dependency errors. In contrast, for the MSD system this error propagation is 0.46, thus higher.5
For the extreme POS scenario, content vs. function POS (CF), the drop in performance for UDP is however very large. But this might be a too crude evaluation setup. Nevertheless, UDP, the simple unsupervised system with PageRank, outperforms the adjacency baselines (BL) by 4% on average on the two type-based naive POS tag scenario. This difference indicates that even with very deficient POS tags, UDP can provide better structures.","Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns. Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences.
The Universal Dependencies (UD) project (Nivre et al., 2015) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for cross-lingual dependency parsing research (McDonald et al., 2013). Furthermore, we expect that such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach.
Our system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In par-
ticular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014).
Contributions Our method goes beyond the existing work on rule-aided unsupervised dependency parsing by a) adapting dependency head rules to UD-compliant POS relations, b) incorporating the UD restriction of function words being leaves, c) using personalized PageRank to improve main predicate identification, and d) making it completely free of language-specific parameters by estimating adposition attachment direction directly on test data.
We evaluate our system on 32 languages1 in three setups, depending on the reliability of available POS tags, and compare to a multi-source delexicalized transfer system. In addition, we evaluate the systems’ sensitivity to domain change for a subset of UD languages for which domain information was retrievable. The results expose a solid and competitive system for all UD languages. Our unsupervised parser compares favorably to delexicalized parsing, while being more robust to domain change across languages.","Over the recent years, cross-lingual linguistic structure prediction based on model transfer or projection of POS tags and dependency trees has become a relevant line of work (Das and Petrov, 2011; McDonald et al., 2011). These works mostly use supervised learning and different target language adaptation techniques.
1Out of 33 languages in UD v1.2. We exclude Japanese because the treebank is distributed without word forms and hence we can not provide results on predicted POS.
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015).
The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extend delexicalization to involve multiple sourceside parsers. This line of work depends on applying uniform POS and dependency representations (McDonald et al., 2013).
Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich Indo-European languages. Agić et al. (2015) expose some of the biases in their proposal for realistic cross-lingual POS tagging, as they emphasize the lack of perfect sentence and word splitting for truly low-resource languages. Johannsen et al. (2016) introduce joint projection of POS and dependencies from multiple sources while sharing the outlook on bias removal in real-world multilingual processing.
Cross-lingual learning, realistic or not, depends entirely on the availability of data: for the sources, for the targets, or most often for both sets of languages. Moreover, it typically does not exploit the constraints placed on the linguistic structures through the formalism, and it does so by design. With the emergence of UD as the practical standard for cross-language annotation of POS and syntactic dependencies, we argue for an approach that takes a fresh angle on both aspects. Namely, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploit-
ing the UD constraints on building POS and dependency annotations.
These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of unsupervised parsers falls far behind the approaches involving any sort of supervision.
Our work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b). In particular, we make use of Søgaard’s (2012b) PageRank method to rank words before decoding. Our system, however, has two key differences: i) the usage of PageRank personalization, and of ii) two-step decoding to treat content and function words differently according to the UD formalism. Through these differences, even without any training data, we parse nearly as well as a delexicalized transfer parser, and with increased stability to domain change.","In this section we provide a further error analysis of the UDP parser. We examine the contribution to the overal results of using PageRank to score content words, the behavior of the system across different parts of speech, and we assess the robustness of UDP when parsing text from different domains.
5Err. prop. = (E(ParseP )−E(ParseG))/E(POSP ), where E(x) = 1−Accuracy(x).","We have presented UDP, an unsupervised dependency parser for Universal Dependencies that makes use of personalized PageRank and a small set of head-dependent rules. The parser requires no training data and estimates adpositon direction directly from test data. We achieve competitive performance on all but two UD languages, and even beat a multi-source delexicalized parser (MSD) on Hindi. We evaluated the parser on three POS setups and across domains. Our results show that UDP is less affected by deteriorating POS tags than MSD, and is more resilient to domain changes. Both the parser and the in-domain annotated test sets are freely available.8
Further work includes extending the parser to handle multiword expressions, coordination, and proper names. Moreover, our usage of PR could be expanded to directly score the potential dependency edges—e.g., by means of edge reification— instead of words. Finally, we only considered unlabeled attachment, however, our system could easily be augmented with partial edge labeling."
38,"We present UDP, an unsupervised parsing algorithm for Universal Dependencies (UD) based on  PageRank and a small set of specific dependency head rules. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual UD parsing. It is distinctly robust to domain change across languages.",Parsing for Universal Dependencies without training,151,"This paper presents a way to parse trees (namely the universal dependency
treebanks) by relying only on POS and by using a modified version of the
PageRank to give more way to some meaningful words (as opposed to stop words).

This idea is interesting though very closed to what was done in SÃ¸gaard
(2012)'s paper. The personalization factor giving more weight to the main
predicate is nice but it would have been better to take it to the next level.
As far as I can tell, the personalization is solely used for the main predicate
and its weight of 5 seems arbitrary.

Regarding the evaluation and the detailed analyses, some charts would have been
beneficial, because it is sometimes hard to get the gist out of the tables.
Finally, it would have been interesting to get the scores of the POS tagging in
the prediction mode to be able to see if the degradation in parsing performance
is heavily correlated to the degradation in tagging performance (which is what
we expect).

All in all, the paper is interesting but the increment over the work of
SÃ¸gaard (2012) is small.

Smaller issues:
-------------------

l. 207 : The the main idea -> The main idea",,3,4,Poster,4,3,3,4,4,5,3,2,2016,"Our approach does not use any training or unlabeled data. We have used the English treebank during development to assess the contribution of individual head rules, and to tune PageRank parameters (Sec. 3.1) and function-word directionality (Sec. 3.2). Adposition direction is calculated on the fly on test data. In the following, we refer to our UD parser as UDP.Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), but our system fares best strictly using the dependency rules in Table 1 to build the graph.
We build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB. This strategy does not always yield connected graphs, and we use a teleport probability of 0.05 to ensure PR convergence. We chose this
3
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
value incrementally in intervals of 0.01 during development until we found the smallest value that guaranteed PR convergence. A high teleport probability is undesirable, because the resulting stationary distribution can be almost uniform. We did not have to re-adjust this value when running on the actual test data.
The the main idea behind our personalized PR approach is the observation that ranking is only relevant for content words.2 PR can incorporate a priori knowledge of the relevance of nodes by means of personalization, namely giving more weight to certain nodes. Intuitively, the higher the rank of a word, the closer it should be to the root node, i.e., the main predicate of the sentence is the node that should have the highest PR, making it the dependent of the root node (Fig. 1, lines 4-5). We use PR personalization to give 5 times more weight (over an otherwise uniform distribution) to the node that is estimated to be main predicate, i.e., the first verb or the first content word if there are no verbs.Head direction is an important syntactic trait. Indeed, the UD feature inventory contains a trait to distinguish adposition between pre- and postpositions. Instead of relying on this feature from the treebanks, which is not always provided, we estimate the frequency of ADP-NOMINAL vs. NOMINAL-ADP bigrams.3 This estimation requires very few examples to converge (10-15 sentences), and we calculate it directly on test data.
If a language has more ADP-NOMINAL bigrams, we consider all its ADP to be prepositions (and thus dependent of elements at their right). Otherwise, we consider them to be postpositions.
For other function words, we have determined on the English dev data whether to make them strictly right- or left-attaching, or to allow either direction: AUX, DET, and SCONJ are right-attaching, while CONJ and PUNCT are leftattaching. There are no direction constraints for the rest.Fig. 1 shows the tree-decoding algorithm. It has two blocks, namely a first block (3-11) where we assign the head of content words according to
2ADJ, NOUN, PROPN, and VERB mark content words. 3NOMINAL= {NOUN, PROPN, PRON}
1: H = ∅; D = ∅ 2: C = 〈c1, ...cm〉; F = 〈f1, ...fm〉 3: for c ∈ C do 4: if |H| = 0 then 5: h = root 6: else 7: h =argminj∈H {γ(j, c) | δ(j, c)∧κ(j, c)} 8: end if 9: H = H ∪ {c} 10: D = D ∪ {(h, c)} 11: end for 12: for f ∈ F do 13: h =argminj∈H {γ(j, f) | δ(j, f) ∧ κ(j, f)} 14: D = D ∪ {(h, f)} 15: end for 16: return D
Figure 1: Two-step decoding algorithm for UDP.
ADJ −→ ADV VERB −→ ADV, AUX, NOUN, PROPN, PRON, SCONJ NOUN, PROPN −→ ADP, DET, NUM NOUN, PROPN −→ ADJ, NOUN, PROPN
Table 1: UD dependency rules
their PageRank and the constraints of the dependency rules, and a second block (12-15) where we assign the head of function words according to their proximity, direction of attachment, and dependency rules. The algorithm requires:
1. The PR-sorted list of content words C. 2. The set of function words F . 3. A set H for the current possible heads, and
a set D for the dependencies assigned at each iteration, which we represent as headdependent tuples (h, d). 4. A symbol root for the root node. 5. A function γ(n,m) that gives the linear dis-
tance between two nodes. 6. A function κ(h, d) that returns whether the
dependency (h, d) has a valid attachment direction given the POS of the d (cf. Sec. 3.2). 7. A function δ(h, d) that determines whether (h, d) is licensed by the rules in Table 1.
The head assignations in lines 7 and 13 read as follow: the head h of a word (either c or f ) is the closest element of the current list of heads (H) that has the right direction (κ) and respects the POSdependency rules (δ). These assignations have a back-off option to ensure the final D is a tree. If the conditions determined by κ and δ are too strict,
4
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
357
358
359
360
361
362
363
364
365
366
367
368
369
378
379
380
381
382
383
384
385
386
392
393
394
395
396
397
398
399
i.e. if the set of possible heads is empty, we drop the δ head-rule constraint and recalculate the closest possible head that respects the directionality imposed by κ. If the set is empty again, we drop both constraints and assign the closest head.
Lines 4 and 5 enforce the single-root constraint. To enforce the leaf status of function nodes, the algorithm first attaches all content words (C), and then all function words (F ) in the second block where H is not updated, thereby ensuring leafness for all f ∈ F . The order of head attachment is not monotonic wrt. PR between the first and second block, and can yield non-projectivities. Nevertheless, it still is a one-pass algorithm. Decoding runs in less than O(n2), namely O(n× |C|). However, running PR incurs the main computation cost.This section exemplifies a full run of UDP for the example sentence “They also had a special connection to some extremists”, an actual clause from the English test data.Given an input sentence and its POS tags, we obtain rank of each word by building a graph using head rules and running PR on it. Table 2 provides the sentence, the POS of each word, the number of incoming edges for each word after building the graph with the head rules from Sec. 3.1, and the personalization vector for PR on this sentence. Note that all nodes have the same personalization weight, except the estimated main predicate, the verb “had”.
Word: They also had a special connection to some extremists POS: PRON ADV VERB DET ADJ NOUN ADP DET NOUN
Personalization: 1 1 5 1 1 1 1 1 1 Incoming edges: 0 0 4 0 1 5 0 0 5
Table 2: Words, POS, Personalization and incoming edges for the example sentence.
Table 3 shows the directed multigraph used for PR in detail. We can see e.g. that the four incoming edges for the verb “come” from the two nouns, plus from the adverb “also” and the pronoun “They”.
After running PR, we obtain the following ranking for content words: C = 〈had,connection,extremists,special〉 Even though the verb has four incoming edges and the nouns have six each, the personalization makes the verb the highest-ranked word.Once C is calculated, we can follow the algorithm in Fig. 1 to obtain a dependency parse. Table 4 shows a trace of the algorithm, given C and F : C = 〈had,connection,extremists,special〉 F = {They, also, a, to, some}
The first four iterations calculate the head of content words following their PR, and the following iterations attach the function words in F .
Finally, Fig. 2 shows the resulting dependency tree. Full lines are assigned in the first block (content dependents), dotted lines are assigned in the second block (function dependents). The edge labels indicate in which iteration the algorithm has assigned each dependency.
Note that the algorithm is deterministic for a certain input POS sequence. Any 10-token sentence with the POS labels shown in Table 2 would yield the same dependency tree.4
4The resulting trees always pass the validation script in
5
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
Language BLG UDPG MSDG MSDP UDPP UDPN
Ancient Greek 42.2 L 43.4 48.6 46.5 41.6 27.0 Arabic 34.8 R 47.8 52.8 52.6 47.6 41.0 Basque 47.8 R 45.0 51.2 49.3 43.1 22.8
Bulgarian 54.9 R 70.5 78.7 76.6 68.1 27.1 Church Slavonic 53.8 L 59.2 61.8 59.8 59.2 35.2
Croatian 41.6 L 56.7 69.1 65.6 54.5 25.2 Czech 46.5 R 61.0 69.5 67.6 59.3 25.3
Danish 47.3 R 57.9 70.2 65.6 53.8 26.9 Dutch 36.1 L 49.5 57.0 59.2 50.0 24.1
English 46.2 R 53.0 62.1 59.9 51.4 27.9 Estonian 73.2 R 70.0 73.4 66.1 65.0 25.3 Finnish 43.8 R 45.1 52.9 50.4 43.1 21.6 French 47.1 R 64.5 72.7 70.6 62.1 36.3 German 48.2 R 60.6 66.9 62.5 57.0 24.2 Gothic 50.2 L 57.5 61.7 59.2 55.8 34.1 Greek 45.7 R 58.5 68.0 66.4 57.0 29.3
Hebrew 41.8 R 55.4 62.0 58.6 52.8 35.7 Hindi 43.9 R 46.3 34.6 34.5 45.7 27.0 Hungarian 53.1 R 56.7 58.4 56.8 54.8 22.7 Indonesian 44.6 L 60.6 63.6 61.0 58.4 35.3
Irish 47.5 R 56.6 62.5 61.3 53.9 35.8 Italian 50.6 R 69.4 77.1 75.2 67.9 37.6 Latin 49.4 L 56.2 59.8 54.9 52.4 37.1 Norwegian 49.1 R 61.7 70.8 67.3 58.6 29.8 Persian 37.8 L 55.7 57.8 55.6 53.6 33.9 Polish 60.8 R 68.4 75.6 71.7 65.7 34.6 Portuguese 45.8 R 65.7 72.8 71.4 64.9 33.5 Romanian 52.7 R 63.7 69.2 64.0 58.9 32.1
Slovene 50.6 R 63.6 74.7 71.0 56.0 24.3 Spanish 48.2 R 63.9 72.9 70.7 62.1 35.0 Swedish 52.4 R 62.8 72.2 67.2 58.5 25.3
Tamil 41.4 R 34.2 44.2 39.5 32.1 20.3
Average 47.8 57.5 63.9 61.2 55.3 29.9
Table 5: UAS for baseline with gold POS (BLG) with direction (L/R) for backoff attachments, UDP with gold POS (UDPG) and predicted POS (UDPP ), PR with naive content-function POS (UDPN ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP , respectively). BL values higher than UDPG are underlined, and UDPG values higher than MSDG are in boldface.This section describes the data and metrics used to assess the performance of UDP, as well as the systems we compare against. We evaluate on the test sections of the UD1.2 treebanks (Nivre et al., 2015) that contain word forms. If there is more than one treebank per language, we use the treebank that has the canonical language name (e.g. Finnish instead of Finnish-FTB).
It is common to evaluate unsupervised dependency parsing using alternative metrics like undirected attachment score or neutralized edge direction, or to evaluate short sentences separately (Schwartz et al., 2011; Gelling et al., 2012). In contrast, we use standard unlabeled attachment score (UAS) and evaluate on all sentences of the
github.com/UniversalDependencies/tools
canonical UD test sets.We compare our UDP system with the performance of a rule-based baseline that uses the head rules in Table 5. The baseline identifies the first verb (or first content word if there are no verbs) as the main predicate, and assigns heads to all words according to the rules in Table 1.
We have selected the set of head rules to maximize precision on the development set, and they do not provide full coverage. The system makes any word not covered by the rules (e.g., a word with a POS such as X or SYM) either dependent of their left or right neighbor, depending on the estimated runtime parameter.
We report the best head direction and its score for each language in Table 5. This baseline finds the head of each token based on its closest possible head, or on its immediate left or right neighbor if there is no head rule for the POS at hand, which means that this system does not necessarily yield well-formed tress. Each token receives a head, and while the structures are single-rooted, they are not necessarily connected.
Note that we do not include results for the DMV model by Klein and Manning (2004), as it has been outperformed by a system similar to ours (Søgaard, 2012b). The usual adjacency baseline for unsupervised dependency parsing, where all words depend on their left or right neighbor, fares much worse than our baseline (20% UAS below on average) even when we make an oracle pick for the best per-language direction. Therefore we do not report those scores.The performance of UDP depends on PageRank to score content words, and on two-step decoding to ensure the leaf status of function words. In this section we isolate the constribution of both parts. We do so by comparing the performance of BL, UDP, and UDPNoPR, a version of UDP where we disable PR and rank content words according to their reading order, i.e. the first word in the ranking is the first word to be read, regardless of the specific language’s script direction
The baseline BL described in 5.1 already ensures function words are leaf nodes, because they have no listed dependent POS in the head rules. The task of the decoding steps is mainly to ensure the resulting structures are well-formed dependency trees.
However, if we measure the difference between UDPNoPR and BL, we observe that UDPNoPR contributes with 4 UAS points on average over the baseline. Nevertheless, the baseline is oracleinformed about the language’s best branching direction, a property that UDP does not have. Instead, the decoding step determines head direction as described in Section 3.2.
Complementary, we can measure the contribution of PR by observing the difference between regular UDP and UDPNoPR. The latter scores on average 9 UAS points lower than UDP. These 9 points are strictly determined by the better attachment of content words.UD is a constantly-improving effort, and not all v1.2 treebanks have the same level of formalism compliance. Thus, the interpretation of, e.g., the AUX-VERB or DET-PRON distinctions might differ. However, we do not incorporate these differences in our analysis and consider all treebanks equally compliant.
The root accuracy scores oscillate around an average of 69%, with Arabic and Tamil (26%) and Estonian (93%) as outliers. Given the PR personalization (Sec. 3.1), UDP has a strong bias for chosing the first verb as main predicate. However, without personalization, performance drops 2% on average. This improvement is consistent even for verb-final languages like Hindi. Moreover, our personalization strategy makes PR converge a whole order of magnitude faster.
The bigram heuristic to determine adposition
7
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
Language BLG MSDG UDPG MSDP UDPP
Bulgarian 50.1±2.4 73.5 ±3.5 69.7±1.8 71.3±3.3 66.9±3.2 Croatian+Serbian 42.1±0.7 66.0±3.0 57.8±1.4 62.1±3.0 54.4 ±2.0
English 42.2±2.8 60.1±6.2 53.9±2.5 57.3±4.3 52.0 ±3.3 Italian 50.3±1.2 70.0±5.4 70.1±3.3 68.1±6.0 68.7±3.9
Average Std. 1.8 4.5 2.5 4.2 3.1
Table 6: Average language-wise domain evaluation. We report average UAS and standard deviation per language. The bottom row provides the average standard deviation for each system.
direction succeeds at identifying the predominant pre- or postposition preference for all languages (average ADP UAS of 75%). The fixed direction for the other functional POS is largely effective, with few exceptions, e.g., DET is consistently right-attaching on all treebanks except Basque (average overall DET UAS of 84%, 32% for Basque). These alternations could also be estimated from the data in a manner similar to ADP. Our rules do not make nouns eligible heads for verbs. As a result, the system cannot infer relative clauses. We have excluded the NOUN → VERB head rule during development because it makes the hierarchical relation between verbs and nouns less conclusive.
We have not excluded punctuation from the evaluation. Indeed, the UAS for the PUNCT is low (an average of 21%, standard deviation of 9.6), even lower than the otherwise problematic CONJ. Even though conjunctions are pervasive and identifying their scope is one of the usual challenges for parsers, the average UAS for CONJ is much larger (an average of 38%, standard deviation of 13.5) than for PUNCT. Both POS show large standard deviations, which indicates great variability. This variability can be caused by linguistic properties of the languages or evaluation datasets, but also by differences in annotation convention.Models with fewer parameters are less likely to be overfit for a certain dataset. In our case, a system with few, general rules is less likely to make attachment decisions that are very particular of a certain language or dataset. Plank and van Noord (2010) have shown that rule-based parsers can be more stable to domain shift. We explore if their finding holds for UDP as well, by testing on i) the UD development data as a readily available proxy for domain shift, and ii) manually curated domain splits of select UD test sets.
Language Domain BLG MSDG UDPG MSDP UDPP Bulgarian bulletin 48.3 67.5 67.4 65.4 61.5 legal 47.9 76.9 69.2 73.0 68.6 literature 53.6 74.2 69.0 72.8 66.6 news 49.3 74.6 70.2 73.0 68.2 various 51.4 74.2 72.5 72.6 69.5
Croatian news 41.2 62.4 57.9 61.8 52.2 wiki 41.9 64.8 55.8 58.2 56.3
English answers 44.1 61.6 55.9 59.5 53.7 email 42.8 58.8 52.1 57.1 56.3 newsgroup 41.7 55.5 49.7 52.9 51.1 reviews 47.4 66.8 54.9 63.9 52.2 weblog 43.3 51.6 50.9 49.8 53.8 magazine† 41.4 60.9 55.6 58.4 53.3 bible† 38.4 56.2 56.2 56.8 48.6 questions† 38.7 69.7 55.6 60.5 47.2
Italian europarl 50.8 64.1 70.6 62.7 69.7 legal 51.1 67.9 69.0 64.4 67.2 news 49.4 68.9 67.5 67.0 65.3 questions 48.7 80.0 77.0 79.1 76.1 various 49.7 67.8 69.0 65.3 67.6 wiki 51.8 71.2 68.1 70.3 66.6
Serbian news 42.8 68.0 58.8 65.6 53.3 wiki 42.4 68.9 58.8 62.8 55.8
Table 7: Evaluation across domains. UAS for baseline with gold POS (BLG), UDP with gold POS (UDPG) and predicted POS (UDPP ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP ). English datasets marked with † are in-house annotated. Lowest results per language underlined. Bold: UDP outperforms MSD.
Development sets We have used the English development data to choose which relations would be included as head rules in the final system (Cf. Table 1). It would be possible that some of the rules are indeed more befitting for the English data or for that particular section.
However, if we regard the results for UDPG in Table 5, we can see that there are 24 languages (out of 32) for which the parser performs better than for English. This result indicates that the head rules are general enough to provide reasonable parses for languages other than the one chosen for development.
If we run UDPG on the development sections for the other languages, we find the results are very consistent. Any language scores on average ±1 UAS with regards to the test section. There is no clear tendency for either section being easier to parse with our system.
Cross-domain test sets To further assess the cross-domain robustness, we retrieved the domain (genre) splits6 from the test sections of the UD
6The data splits are freely available at http://ANONYMIZED
8
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
treebanks where the domain information is available as sentence metadata: from Bulgarian, Croatian, and Italian. We also include a UD-compliant Serbian dataset which is not included in the UD release but which is based on the same parallel corpus as Croatian and has the same domain splits (Agić and Ljubešić, 2015). When averaging we pool Croatian and Serbian together as they come from the same dataset. Also, we use a tagger trained on the Croatian UD training data for tagging Serbian.
For English, we have obtained the data splits in the test section matching the sentences from the original distribution of the English Web Treebank. In addition to these already available data sets, we have annotated three different datasets to asses domain variation more extensively, namely the first 50 verses of the King James Bible, 50 sentences from a magazine, and 75 sentences from the test split in QuestionBank (Judge et al., 2006). We include the third dataset to evaluate strictly on questions, which we could do already in Italian. While the answers domain in English is made up of text from the Yahoo! Answers forum, only one fourth of the sentences are questions. Note these three small datasets are not included in the results on the canonical test sections in Table 5.7
Table 6 summarizes the per-language average score and standard deviation, as well as the macroaveraged standard deviation across languages. UDP has a much lower standard deviation across domains compared to MSD. This holds across languages. We attribute this higher stability to UDP being developed to satisfy a set of general properties of the UD syntactic formalism, instead of being a data-driven method more sensitive to sampling bias. This holds for both the gold-POS and predicted-POS setup. The differences in standard deviation are unsurprisingly smaller in the predicted POS setup. In general, the rule-based UPD is less sensitive to domain shifts than the datadriven MSD counterpart, confirming earlier findings (Plank and van Noord, 2010).
Table 7 gives the detailed scores per language and domain. From the scores we can see that presidental bulletin, legal and weblogs are amongst the hardest domains to parse. However, the systems often do not agree on which domain is hardest, with the exception of Bulgarian
7The three in-house annotated datasets are freely available at http://ANONYMIZED
bulletin. More importantly, this might even change between gold and predicted POS, highlighting the importance of evaluating systems beyond gold POS. Interestingly, for the Italian data and some of the hardest domains UDP outperforms MSD, confirming that it is a robust baseline.","Our system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word’s frequency. Words that belong to the 100 most frequent word types of the input test section receive the FUNCTION tag.
Finally, we compare our system to a supervised cross-lingual system (MSD). It is a multi-
6
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
source delexicalized transfer parser, referred to as multi-dir in the original paper by McDonald et al. (2011). For this baseline we train TurboParser (Martins et al., 2013) on a delexicalized training set of 20k sentences, sampled uniformly from the UD training data excluding the target language. MSD is a competitive baseline in crosslingual transfer parsing work. This gives us an indication how our system compares to standard cross-lingual parsers.Table 5 shows that UDP is a competitive system; because UDPG is remarkably close to the supervised MSDG system, with an average difference of 6.4%, even outperforming MSDG on one language (Hindi).
More interestingly, on the evaluation scenario with predicted POS we observe that our system drops only marginally (2.2%) compared to MSD (2.7%). In the least robust rule-based setup, the error propagation rate from POS to dependency would be doubled, as either a wrongly tagged head or dependent would break the dependency rules. However, with an average POS accuracy by TnT of 94.1%, the error propagation is 0.37, i.e each POS error causes 0.37 additional dependency errors. In contrast, for the MSD system this error propagation is 0.46, thus higher.5
For the extreme POS scenario, content vs. function POS (CF), the drop in performance for UDP is however very large. But this might be a too crude evaluation setup. Nevertheless, UDP, the simple unsupervised system with PageRank, outperforms the adjacency baselines (BL) by 4% on average on the two type-based naive POS tag scenario. This difference indicates that even with very deficient POS tags, UDP can provide better structures.","Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns. Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences.
The Universal Dependencies (UD) project (Nivre et al., 2015) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for cross-lingual dependency parsing research (McDonald et al., 2013). Furthermore, we expect that such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach.
Our system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In par-
ticular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014).
Contributions Our method goes beyond the existing work on rule-aided unsupervised dependency parsing by a) adapting dependency head rules to UD-compliant POS relations, b) incorporating the UD restriction of function words being leaves, c) using personalized PageRank to improve main predicate identification, and d) making it completely free of language-specific parameters by estimating adposition attachment direction directly on test data.
We evaluate our system on 32 languages1 in three setups, depending on the reliability of available POS tags, and compare to a multi-source delexicalized transfer system. In addition, we evaluate the systems’ sensitivity to domain change for a subset of UD languages for which domain information was retrievable. The results expose a solid and competitive system for all UD languages. Our unsupervised parser compares favorably to delexicalized parsing, while being more robust to domain change across languages.","Over the recent years, cross-lingual linguistic structure prediction based on model transfer or projection of POS tags and dependency trees has become a relevant line of work (Das and Petrov, 2011; McDonald et al., 2011). These works mostly use supervised learning and different target language adaptation techniques.
1Out of 33 languages in UD v1.2. We exclude Japanese because the treebank is distributed without word forms and hence we can not provide results on predicted POS.
2
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015).
The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extend delexicalization to involve multiple sourceside parsers. This line of work depends on applying uniform POS and dependency representations (McDonald et al., 2013).
Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich Indo-European languages. Agić et al. (2015) expose some of the biases in their proposal for realistic cross-lingual POS tagging, as they emphasize the lack of perfect sentence and word splitting for truly low-resource languages. Johannsen et al. (2016) introduce joint projection of POS and dependencies from multiple sources while sharing the outlook on bias removal in real-world multilingual processing.
Cross-lingual learning, realistic or not, depends entirely on the availability of data: for the sources, for the targets, or most often for both sets of languages. Moreover, it typically does not exploit the constraints placed on the linguistic structures through the formalism, and it does so by design. With the emergence of UD as the practical standard for cross-language annotation of POS and syntactic dependencies, we argue for an approach that takes a fresh angle on both aspects. Namely, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploit-
ing the UD constraints on building POS and dependency annotations.
These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of unsupervised parsers falls far behind the approaches involving any sort of supervision.
Our work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b). In particular, we make use of Søgaard’s (2012b) PageRank method to rank words before decoding. Our system, however, has two key differences: i) the usage of PageRank personalization, and of ii) two-step decoding to treat content and function words differently according to the UD formalism. Through these differences, even without any training data, we parse nearly as well as a delexicalized transfer parser, and with increased stability to domain change.","In this section we provide a further error analysis of the UDP parser. We examine the contribution to the overal results of using PageRank to score content words, the behavior of the system across different parts of speech, and we assess the robustness of UDP when parsing text from different domains.
5Err. prop. = (E(ParseP )−E(ParseG))/E(POSP ), where E(x) = 1−Accuracy(x).","We have presented UDP, an unsupervised dependency parser for Universal Dependencies that makes use of personalized PageRank and a small set of head-dependent rules. The parser requires no training data and estimates adpositon direction directly from test data. We achieve competitive performance on all but two UD languages, and even beat a multi-source delexicalized parser (MSD) on Hindi. We evaluated the parser on three POS setups and across domains. Our results show that UDP is less affected by deteriorating POS tags than MSD, and is more resilient to domain changes. Both the parser and the in-domain annotated test sets are freely available.8
Further work includes extending the parser to handle multiword expressions, coordination, and proper names. Moreover, our usage of PR could be expanded to directly score the potential dependency edges—e.g., by means of edge reification— instead of words. Finally, we only considered unlabeled attachment, however, our system could easily be augmented with partial edge labeling."
